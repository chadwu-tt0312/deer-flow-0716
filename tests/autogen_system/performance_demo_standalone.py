#!/usr/bin/env python3
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
AutoGenæ€§èƒ½å„ªåŒ–ç¨ç«‹æ¼”ç¤º

ä¸ä¾è³´autogen_coreçš„æ€§èƒ½ç›£æ§å’Œå„ªåŒ–æ¼”ç¤ºã€‚
"""

import asyncio
import time
import random
import statistics
import threading
import psutil
import gc
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from enum import Enum


class MetricType(Enum):
    """æŒ‡æ¨™é¡å‹"""

    LATENCY = "latency"
    THROUGHPUT = "throughput"
    MEMORY = "memory"
    CPU = "cpu"


@dataclass
class MetricPoint:
    """æŒ‡æ¨™æ•¸æ“šé»"""

    timestamp: float
    value: float
    metadata: Dict[str, Any] = field(default_factory=dict)


class SimpleMetricsCollector:
    """ç°¡åŒ–çš„æŒ‡æ¨™æ”¶é›†å™¨"""

    def __init__(self):
        self.metrics = {}
        self.is_collecting = False
        self.collection_thread = None
        self.process = psutil.Process()

    def start_collection(self):
        """é–‹å§‹æ”¶é›†æŒ‡æ¨™"""
        if self.is_collecting:
            return

        self.is_collecting = True
        self.collection_thread = threading.Thread(target=self._collect_metrics)
        self.collection_thread.start()
        print("âœ“ æ€§èƒ½æŒ‡æ¨™æ”¶é›†å·²é–‹å§‹")

    def stop_collection(self):
        """åœæ­¢æ”¶é›†æŒ‡æ¨™"""
        if not self.is_collecting:
            return

        self.is_collecting = False
        if self.collection_thread:
            self.collection_thread.join()
        print("âœ“ æ€§èƒ½æŒ‡æ¨™æ”¶é›†å·²åœæ­¢")

    def _collect_metrics(self):
        """æ”¶é›†ç³»çµ±æŒ‡æ¨™"""
        while self.is_collecting:
            try:
                # CPUä½¿ç”¨ç‡
                cpu_percent = self.process.cpu_percent()
                self.add_metric("cpu_usage", cpu_percent)

                # å…§å­˜ä½¿ç”¨
                memory_info = self.process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024
                self.add_metric("memory_usage_mb", memory_mb)

                # ç·šç¨‹æ•¸
                thread_count = self.process.num_threads()
                self.add_metric("thread_count", thread_count)

            except Exception as e:
                print(f"æ”¶é›†æŒ‡æ¨™æ™‚å‡ºéŒ¯: {e}")

            time.sleep(0.5)

    def add_metric(self, name: str, value: float, metadata: Dict[str, Any] = None):
        """æ·»åŠ æŒ‡æ¨™"""
        if name not in self.metrics:
            self.metrics[name] = []

        point = MetricPoint(timestamp=time.time(), value=value, metadata=metadata or {})
        self.metrics[name].append(point)

        # é™åˆ¶æ•¸æ“šé»æ•¸é‡
        if len(self.metrics[name]) > 100:
            self.metrics[name] = self.metrics[name][-100:]

    def get_average(self, name: str) -> Optional[float]:
        """ç²å–å¹³å‡å€¼"""
        if name not in self.metrics or not self.metrics[name]:
            return None
        values = [point.value for point in self.metrics[name]]
        return statistics.mean(values)

    def get_latest(self, name: str) -> Optional[float]:
        """ç²å–æœ€æ–°å€¼"""
        if name not in self.metrics or not self.metrics[name]:
            return None
        return self.metrics[name][-1].value

    def measure_latency(self, operation_name: str):
        """æ¸¬é‡å»¶é²çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""

        class LatencyMeasurer:
            def __init__(self, collector, name):
                self.collector = collector
                self.name = name
                self.start_time = None

            def __enter__(self):
                self.start_time = time.time()
                return self

            def __exit__(self, exc_type, exc_val, exc_tb):
                if self.start_time:
                    duration = time.time() - self.start_time
                    self.collector.add_metric(f"latency_{self.name}", duration)

        return LatencyMeasurer(self, operation_name)


class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""

    def __init__(self):
        self.bottlenecks = []
        self.recommendations = []

    def analyze(self, metrics_collector: SimpleMetricsCollector) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½"""
        analysis = {
            "overall_score": 100.0,
            "bottlenecks": [],
            "recommendations": [],
            "insights": [],
        }

        # åˆ†æå…§å­˜ä½¿ç”¨
        memory_avg = metrics_collector.get_average("memory_usage_mb")
        if memory_avg and memory_avg > 500:
            analysis["bottlenecks"].append(
                {
                    "type": "memory",
                    "severity": min(1.0, memory_avg / 1000),
                    "description": f"å…§å­˜ä½¿ç”¨è¼ƒé«˜: {memory_avg:.1f}MB",
                }
            )
            analysis["recommendations"].append("å„ªåŒ–å…§å­˜ä½¿ç”¨ï¼Œæª¢æŸ¥å…§å­˜æ´©æ¼")
            analysis["overall_score"] -= 15

        # åˆ†æCPUä½¿ç”¨
        cpu_avg = metrics_collector.get_average("cpu_usage")
        if cpu_avg and cpu_avg > 70:
            analysis["bottlenecks"].append(
                {
                    "type": "cpu",
                    "severity": min(1.0, cpu_avg / 100),
                    "description": f"CPUä½¿ç”¨ç‡è¼ƒé«˜: {cpu_avg:.1f}%",
                }
            )
            analysis["recommendations"].append("å„ªåŒ–CPUå¯†é›†å‹æ“ä½œ")
            analysis["overall_score"] -= 10

        # åˆ†æå»¶é²
        latency_metrics = [
            name for name in metrics_collector.metrics.keys() if name.startswith("latency_")
        ]

        high_latency_count = 0
        for metric_name in latency_metrics:
            avg_latency = metrics_collector.get_average(metric_name)
            if avg_latency and avg_latency > 1.0:  # å¤§æ–¼1ç§’
                operation = metric_name.replace("latency_", "")
                analysis["bottlenecks"].append(
                    {
                        "type": "latency",
                        "severity": min(1.0, avg_latency / 5.0),
                        "description": f"æ“ä½œ {operation} å»¶é²è¼ƒé«˜: {avg_latency:.2f}s",
                    }
                )
                high_latency_count += 1

        if high_latency_count > 0:
            analysis["recommendations"].append("å„ªåŒ–é«˜å»¶é²æ“ä½œï¼Œè€ƒæ…®ç•°æ­¥è™•ç†")
            analysis["overall_score"] -= high_latency_count * 5

        # ç”Ÿæˆæ´å¯Ÿ
        if analysis["overall_score"] >= 90:
            analysis["insights"].append("ç³»çµ±æ€§èƒ½å„ªç§€ï¼Œé‹è¡Œè‰¯å¥½")
        elif analysis["overall_score"] >= 75:
            analysis["insights"].append("ç³»çµ±æ€§èƒ½è‰¯å¥½ï¼Œæœ‰å°å¹…å„ªåŒ–ç©ºé–“")
        elif analysis["overall_score"] >= 60:
            analysis["insights"].append("ç³»çµ±æ€§èƒ½ä¸€èˆ¬ï¼Œéœ€è¦æ³¨æ„å„ªåŒ–")
        else:
            analysis["insights"].append("ç³»çµ±æ€§èƒ½éœ€è¦æ”¹é€²ï¼Œå»ºè­°ç«‹å³å„ªåŒ–")

        if len(analysis["bottlenecks"]) > 2:
            analysis["insights"].append("ç™¼ç¾å¤šå€‹æ€§èƒ½ç“¶é ¸ï¼Œå»ºè­°ç³»çµ±æ€§å„ªåŒ–")

        return analysis


class PerformanceOptimizer:
    """æ€§èƒ½å„ªåŒ–å™¨"""

    def __init__(self):
        self.optimizations_applied = []

    async def apply_optimizations(self) -> List[str]:
        """æ‡‰ç”¨è‡ªå‹•å„ªåŒ–"""
        optimizations = []

        try:
            # 1. åƒåœ¾å›æ”¶å„ªåŒ–
            before_objects = len(gc.get_objects())
            collected = gc.collect()
            after_objects = len(gc.get_objects())

            if collected > 0:
                optimizations.append(f"åƒåœ¾å›æ”¶: é‡‹æ”¾äº† {collected} å€‹å°è±¡")
                optimizations.append(f"å°è±¡æ•¸é‡: {before_objects} -> {after_objects}")

            # 2. å…§å­˜å„ªåŒ–
            import sys

            before_refs = sys.gettotalrefcount() if hasattr(sys, "gettotalrefcount") else 0

            # æ¸…ç†å¯èƒ½çš„å¾ªç’°å¼•ç”¨
            gc.set_threshold(700, 10, 10)  # æ›´æ¿€é€²çš„åƒåœ¾å›æ”¶
            optimizations.append("èª¿æ•´åƒåœ¾å›æ”¶é–¾å€¼ä»¥æ›´é »ç¹æ¸…ç†")

            # 3. ä½µç™¼å„ªåŒ–å»ºè­°
            import os

            cpu_count = os.cpu_count()
            optimizations.append(f"å»ºè­°ç·šç¨‹æ± å¤§å°: {cpu_count * 2} (åŸºæ–¼ {cpu_count} æ ¸CPU)")

        except Exception as e:
            print(f"æ‡‰ç”¨å„ªåŒ–æ™‚å‡ºéŒ¯: {e}")

        self.optimizations_applied.extend(optimizations)
        return optimizations


class PerformanceDemo:
    """æ€§èƒ½æ¼”ç¤º"""

    def __init__(self):
        self.metrics_collector = SimpleMetricsCollector()
        self.analyzer = PerformanceAnalyzer()
        self.optimizer = PerformanceOptimizer()

    async def run_demo(self):
        """é‹è¡Œæ¼”ç¤º"""
        print("ğŸš€ AutoGenæ€§èƒ½å„ªåŒ–æ¼”ç¤ºé–‹å§‹")
        print("=" * 60)

        try:
            # 1. é–‹å§‹ç›£æ§
            await self._start_monitoring()

            # 2. æ¨¡æ“¬å·¥ä½œè² è¼‰
            await self._simulate_workloads()

            # 3. æ€§èƒ½åˆ†æ
            await self._analyze_performance()

            # 4. æ‡‰ç”¨å„ªåŒ–
            await self._apply_optimizations()

            # 5. ç”Ÿæˆå ±å‘Š
            await self._generate_reports()

            print("\nâœ… æ¼”ç¤ºå®Œæˆ!")

        except Exception as e:
            print(f"âŒ æ¼”ç¤ºå¤±æ•—: {e}")
            import traceback

            traceback.print_exc()

    async def _start_monitoring(self):
        """é–‹å§‹ç›£æ§"""
        print("\nğŸ“Š æ­¥é©Ÿ1: é–‹å§‹æ€§èƒ½ç›£æ§")
        print("-" * 30)

        self.metrics_collector.start_collection()
        await asyncio.sleep(1)  # æ”¶é›†ä¸€äº›åŸºæº–æ•¸æ“š

        # é¡¯ç¤ºåˆå§‹ç‹€æ…‹
        memory = self.metrics_collector.get_latest("memory_usage_mb")
        cpu = self.metrics_collector.get_latest("cpu_usage")

        if memory:
            print(f"ğŸ“ˆ åˆå§‹å…§å­˜ä½¿ç”¨: {memory:.1f}MB")
        if cpu:
            print(f"ğŸ“ˆ åˆå§‹CPUä½¿ç”¨: {cpu:.1f}%")

    async def _simulate_workloads(self):
        """æ¨¡æ“¬å·¥ä½œè² è¼‰"""
        print("\nâš¡ æ­¥é©Ÿ2: æ¨¡æ“¬å·¥ä½œè² è¼‰")
        print("-" * 30)

        # CPUå¯†é›†å‹ä»»å‹™
        print("ğŸ”¥ åŸ·è¡ŒCPUå¯†é›†å‹ä»»å‹™...")
        with self.metrics_collector.measure_latency("cpu_intensive"):
            await self._cpu_intensive_task()

        # å…§å­˜å¯†é›†å‹ä»»å‹™
        print("ğŸ’¾ åŸ·è¡Œå…§å­˜å¯†é›†å‹ä»»å‹™...")
        with self.metrics_collector.measure_latency("memory_intensive"):
            await self._memory_intensive_task()

        # IOå¯†é›†å‹ä»»å‹™
        print("ğŸ’¿ åŸ·è¡ŒIOå¯†é›†å‹ä»»å‹™...")
        with self.metrics_collector.measure_latency("io_intensive"):
            await self._io_intensive_task()

        # æ··åˆä»»å‹™
        print("ğŸ”„ åŸ·è¡Œæ··åˆå·¥ä½œè² è¼‰...")
        await self._mixed_workload()

    async def _cpu_intensive_task(self):
        """CPUå¯†é›†å‹ä»»å‹™"""
        start_time = time.time()
        total = 0

        # é‹è¡Œ1ç§’çš„è¨ˆç®—
        while time.time() - start_time < 1.0:
            total += sum(range(1000))
            await asyncio.sleep(0.001)  # è®“å‡ºæ§åˆ¶æ¬Š

        self.metrics_collector.add_metric("cpu_task_operations", total)

    async def _memory_intensive_task(self):
        """å…§å­˜å¯†é›†å‹ä»»å‹™"""
        large_data = []

        # åˆ†é…å¤§é‡å…§å­˜
        for i in range(50):
            chunk = [random.random() for _ in range(10000)]
            large_data.append(chunk)

            if i % 10 == 0:
                await asyncio.sleep(0.1)
                # è¨˜éŒ„å…§å­˜ä½¿ç”¨
                current_memory = self.metrics_collector.get_latest("memory_usage_mb")
                if current_memory:
                    self.metrics_collector.add_metric("peak_memory", current_memory)

        # ä¼°ç®—åˆ†é…çš„å…§å­˜
        estimated_mb = len(large_data) * len(large_data[0]) * 8 / 1024 / 1024
        self.metrics_collector.add_metric("allocated_memory_mb", estimated_mb)

        # æ¸…ç†
        del large_data
        gc.collect()

    async def _io_intensive_task(self):
        """IOå¯†é›†å‹ä»»å‹™"""
        tasks = []

        # æ¨¡æ“¬å¤šå€‹ä¸¦ç™¼IOæ“ä½œ
        for i in range(5):
            task = self._simulate_io_operation(f"operation_{i}")
            tasks.append(task)

        await asyncio.gather(*tasks)

    async def _simulate_io_operation(self, name: str):
        """æ¨¡æ“¬å–®å€‹IOæ“ä½œ"""
        with self.metrics_collector.measure_latency(f"io_{name}"):
            # æ¨¡æ“¬IOå»¶é²
            delay = random.uniform(0.1, 0.3)
            await asyncio.sleep(delay)

            # è¨˜éŒ„æ“ä½œ
            self.metrics_collector.add_metric("io_operations", 1)

    async def _mixed_workload(self):
        """æ··åˆå·¥ä½œè² è¼‰"""
        tasks = []

        # ä½µç™¼åŸ·è¡Œä¸åŒé¡å‹çš„ä»»å‹™
        for i in range(3):
            # CPUä»»å‹™
            tasks.append(self._light_cpu_task(f"cpu_{i}"))
            # IOä»»å‹™
            tasks.append(self._light_io_task(f"io_{i}"))

        await asyncio.gather(*tasks)

    async def _light_cpu_task(self, name: str):
        """è¼•é‡CPUä»»å‹™"""
        with self.metrics_collector.measure_latency(f"light_cpu_{name}"):
            total = sum(range(5000))
            await asyncio.sleep(0.1)
            self.metrics_collector.add_metric("light_cpu_result", total)

    async def _light_io_task(self, name: str):
        """è¼•é‡IOä»»å‹™"""
        with self.metrics_collector.measure_latency(f"light_io_{name}"):
            await asyncio.sleep(random.uniform(0.05, 0.15))
            self.metrics_collector.add_metric("light_io_operations", 1)

    async def _analyze_performance(self):
        """åˆ†ææ€§èƒ½"""
        print("\nğŸ” æ­¥é©Ÿ3: æ€§èƒ½åˆ†æ")
        print("-" * 30)

        # åœæ­¢æ”¶é›†æ•¸æ“šé€²è¡Œåˆ†æ
        self.metrics_collector.stop_collection()

        # åŸ·è¡Œåˆ†æ
        analysis = self.analyzer.analyze(self.metrics_collector)

        # é¡¯ç¤ºåˆ†æçµæœ
        print(f"ğŸ“Š ç¸½é«”æ€§èƒ½åˆ†æ•¸: {analysis['overall_score']:.1f}/100")

        if analysis["overall_score"] >= 90:
            grade = "å„ªç§€ ğŸŸ¢"
        elif analysis["overall_score"] >= 75:
            grade = "è‰¯å¥½ ğŸŸ¡"
        elif analysis["overall_score"] >= 60:
            grade = "ä¸€èˆ¬ ğŸŸ "
        else:
            grade = "éœ€è¦æ”¹é€² ğŸ”´"

        print(f"ğŸ¯ æ€§èƒ½ç­‰ç´š: {grade}")

        # é¡¯ç¤ºç“¶é ¸
        if analysis["bottlenecks"]:
            print(f"\nğŸ” ç™¼ç¾ {len(analysis['bottlenecks'])} å€‹æ€§èƒ½ç“¶é ¸:")
            for bottleneck in analysis["bottlenecks"]:
                severity = bottleneck["severity"]
                icon = "ğŸ”´" if severity > 0.7 else "ğŸŸ¡" if severity > 0.4 else "ğŸŸ¢"
                print(f"  {icon} {bottleneck['description']}")
        else:
            print("\nâœ… æœªç™¼ç¾æ˜é¡¯æ€§èƒ½ç“¶é ¸")

        # é¡¯ç¤ºæ´å¯Ÿ
        if analysis["insights"]:
            print(f"\nğŸ’¡ é—œéµæ´å¯Ÿ:")
            for insight in analysis["insights"]:
                print(f"  - {insight}")

        return analysis

    async def _apply_optimizations(self):
        """æ‡‰ç”¨å„ªåŒ–"""
        print("\nğŸ¯ æ­¥é©Ÿ4: æ‡‰ç”¨å„ªåŒ–")
        print("-" * 30)

        optimizations = await self.optimizer.apply_optimizations()

        if optimizations:
            print("âœ… å·²æ‡‰ç”¨ä»¥ä¸‹å„ªåŒ–:")
            for opt in optimizations:
                print(f"  - {opt}")
        else:
            print("ğŸ“ ç•¶å‰ç„¡å¯æ‡‰ç”¨çš„è‡ªå‹•å„ªåŒ–")

    async def _generate_reports(self):
        """ç”Ÿæˆå ±å‘Š"""
        print("\nğŸ“„ æ­¥é©Ÿ5: ç”Ÿæˆå ±å‘Š")
        print("-" * 30)

        # æ”¶é›†å ±å‘Šæ•¸æ“š
        report_data = {
            "performance_summary": {
                "memory_avg": self.metrics_collector.get_average("memory_usage_mb"),
                "cpu_avg": self.metrics_collector.get_average("cpu_usage"),
                "thread_avg": self.metrics_collector.get_average("thread_count"),
            },
            "latency_summary": {},
            "optimizations": self.optimizer.optimizations_applied,
        }

        # æ”¶é›†å»¶é²æ•¸æ“š
        for metric_name in self.metrics_collector.metrics:
            if metric_name.startswith("latency_"):
                avg_latency = self.metrics_collector.get_average(metric_name)
                if avg_latency:
                    operation = metric_name.replace("latency_", "")
                    report_data["latency_summary"][operation] = f"{avg_latency:.3f}s"

        # ç”Ÿæˆç°¡å–®å ±å‘Š
        report_lines = []
        report_lines.append("# AutoGenæ€§èƒ½æ¸¬è©¦å ±å‘Š")
        report_lines.append(f"\n**æ¸¬è©¦æ™‚é–“**: {time.strftime('%Y-%m-%d %H:%M:%S')}")

        # æ€§èƒ½æ‘˜è¦
        report_lines.append("\n## ğŸ“Š æ€§èƒ½æ‘˜è¦")
        summary = report_data["performance_summary"]
        if summary["memory_avg"]:
            report_lines.append(f"- **å¹³å‡å…§å­˜ä½¿ç”¨**: {summary['memory_avg']:.1f}MB")
        if summary["cpu_avg"]:
            report_lines.append(f"- **å¹³å‡CPUä½¿ç”¨**: {summary['cpu_avg']:.1f}%")
        if summary["thread_avg"]:
            report_lines.append(f"- **å¹³å‡ç·šç¨‹æ•¸**: {summary['thread_avg']:.0f}")

        # å»¶é²æ‘˜è¦
        if report_data["latency_summary"]:
            report_lines.append("\n## â±ï¸ å»¶é²æ‘˜è¦")
            for operation, latency in report_data["latency_summary"].items():
                report_lines.append(f"- **{operation}**: {latency}")

        # å„ªåŒ–è¨˜éŒ„
        if report_data["optimizations"]:
            report_lines.append("\n## ğŸ”§ å·²æ‡‰ç”¨å„ªåŒ–")
            for opt in report_data["optimizations"]:
                report_lines.append(f"- {opt}")

        # ä¿å­˜å ±å‘Š
        report_content = "\n".join(report_lines)
        report_file = Path("autogen_performance_report.md")
        report_file.write_text(report_content, encoding="utf-8")

        print(f"ğŸ“„ æ€§èƒ½å ±å‘Šå·²ä¿å­˜åˆ°: {report_file}")

        # é¡¯ç¤ºæ‘˜è¦
        print("\nğŸ“Š æ€§èƒ½æ¸¬è©¦æ‘˜è¦:")
        if summary["memory_avg"]:
            print(f"  ğŸ’¾ å¹³å‡å…§å­˜: {summary['memory_avg']:.1f}MB")
        if summary["cpu_avg"]:
            print(f"  ğŸ”¥ å¹³å‡CPU: {summary['cpu_avg']:.1f}%")

        if report_data["latency_summary"]:
            print("  â±ï¸ ä¸»è¦å»¶é²:")
            for operation, latency in list(report_data["latency_summary"].items())[:3]:
                print(f"    - {operation}: {latency}")


async def main():
    """ä¸»å‡½æ•¸"""
    demo = PerformanceDemo()
    await demo.run_demo()


if __name__ == "__main__":
    asyncio.run(main())
