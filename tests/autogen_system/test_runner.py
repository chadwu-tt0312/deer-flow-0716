#!/usr/bin/env python3
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
AutoGenç³»çµ±æ¸¬è©¦é‹è¡Œå™¨

æä¾›çµ±ä¸€çš„æ¸¬è©¦åŸ·è¡Œå’Œå ±å‘Šç”Ÿæˆã€‚
"""

import os
import sys
import asyncio
import time
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from enum import Enum

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import pytest
import coverage


class TestSuite(Enum):
    """æ¸¬è©¦å¥—ä»¶é¡å‹"""

    UNIT = "unit"
    INTEGRATION = "integration"
    PERFORMANCE = "performance"
    ALL = "all"


class TestStatus(Enum):
    """æ¸¬è©¦ç‹€æ…‹"""

    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    ERROR = "error"


@dataclass
class TestResult:
    """æ¸¬è©¦çµæœ"""

    name: str
    status: TestStatus
    duration: float
    message: Optional[str] = None
    traceback: Optional[str] = None


@dataclass
class TestSuiteResult:
    """æ¸¬è©¦å¥—ä»¶çµæœ"""

    suite_name: str
    total_tests: int
    passed: int
    failed: int
    skipped: int
    errors: int
    duration: float
    coverage_percentage: Optional[float] = None
    results: List[TestResult] = None

    def __post_init__(self):
        if self.results is None:
            self.results = []

    @property
    def success_rate(self) -> float:
        """è¨ˆç®—æˆåŠŸç‡"""
        if self.total_tests == 0:
            return 0.0
        return (self.passed / self.total_tests) * 100


class AutoGenTestRunner:
    """AutoGenæ¸¬è©¦é‹è¡Œå™¨"""

    def __init__(self, project_root: Path = None):
        """
        åˆå§‹åŒ–æ¸¬è©¦é‹è¡Œå™¨

        Args:
            project_root: é …ç›®æ ¹ç›®éŒ„è·¯å¾‘
        """
        self.project_root = project_root or Path(__file__).parent.parent.parent
        self.test_dir = self.project_root / "tests" / "autogen_system"
        self.coverage = None
        self.results: Dict[str, TestSuiteResult] = {}

    def setup_coverage(self, source_dir: str = "src/autogen_system"):
        """è¨­ç½®ä»£ç¢¼è¦†è“‹ç‡ç›£æ§"""

        self.coverage = coverage.Coverage(
            source=[str(self.project_root / source_dir)],
            config_file=False,
            auto_data=True,
        )
        self.coverage.start()

    def stop_coverage(self) -> float:
        """åœæ­¢è¦†è“‹ç‡ç›£æ§ä¸¦è¿”å›è¦†è“‹ç‡ç™¾åˆ†æ¯”"""

        if not self.coverage:
            return 0.0

        self.coverage.stop()
        self.coverage.save()

        # è¨ˆç®—è¦†è“‹ç‡
        coverage_data = self.coverage.get_data()
        total_lines = 0
        covered_lines = 0

        for filename in coverage_data.measured_files():
            lines = coverage_data.lines(filename) or []
            total_lines += len(lines)
            covered_lines += len(lines)

        if total_lines == 0:
            return 0.0

        return (covered_lines / total_lines) * 100

    async def run_test_suite(
        self, suite: TestSuite, verbose: bool = False, capture_output: bool = True
    ) -> TestSuiteResult:
        """
        é‹è¡ŒæŒ‡å®šçš„æ¸¬è©¦å¥—ä»¶

        Args:
            suite: æ¸¬è©¦å¥—ä»¶é¡å‹
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°è¼¸å‡º
            capture_output: æ˜¯å¦æ•ç²è¼¸å‡º

        Returns:
            TestSuiteResult: æ¸¬è©¦çµæœ
        """

        print(f"\n{'=' * 60}")
        print(f"é‹è¡Œæ¸¬è©¦å¥—ä»¶: {suite.value.upper()}")
        print(f"{'=' * 60}")

        # ç¢ºå®šæ¸¬è©¦è·¯å¾‘
        if suite == TestSuite.UNIT:
            test_path = self.test_dir / "unit"
        elif suite == TestSuite.INTEGRATION:
            test_path = self.test_dir / "integration"
        elif suite == TestSuite.PERFORMANCE:
            test_path = self.test_dir / "integration" / "test_performance.py"
        elif suite == TestSuite.ALL:
            test_path = self.test_dir
        else:
            raise ValueError(f"æœªçŸ¥çš„æ¸¬è©¦å¥—ä»¶: {suite}")

        # è¨­ç½®pyteståƒæ•¸
        pytest_args = [
            str(test_path),
            "-v" if verbose else "-q",
            "--tb=short",
            "--durations=10",
        ]

        if not capture_output:
            pytest_args.append("-s")

        # æ ¹æ“šå¥—ä»¶é¡å‹æ·»åŠ æ¨™è¨˜
        if suite == TestSuite.PERFORMANCE:
            pytest_args.extend(["-m", "performance or benchmark"])
        elif suite == TestSuite.UNIT:
            pytest_args.extend(["-m", "not performance and not benchmark"])

        # å•Ÿå‹•è¦†è“‹ç‡ç›£æ§
        if suite in [TestSuite.UNIT, TestSuite.ALL]:
            self.setup_coverage()

        start_time = time.time()

        try:
            # é‹è¡Œpytest
            exit_code = pytest.main(pytest_args)

            end_time = time.time()
            duration = end_time - start_time

            # åœæ­¢è¦†è“‹ç‡ç›£æ§
            coverage_percentage = None
            if self.coverage:
                coverage_percentage = self.stop_coverage()

            # è§£æçµæœï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
            result = TestSuiteResult(
                suite_name=suite.value,
                total_tests=0,  # éœ€è¦å¾pytestçµæœä¸­è§£æ
                passed=0,
                failed=0,
                skipped=0,
                errors=0,
                duration=duration,
                coverage_percentage=coverage_percentage,
            )

            # æ ¹æ“šé€€å‡ºç¢¼è¨­ç½®åŸºæœ¬çµæœ
            if exit_code == 0:
                print(f"âœ… æ¸¬è©¦å¥—ä»¶ {suite.value} åŸ·è¡ŒæˆåŠŸ")
                result.passed = 1  # ç°¡åŒ–
                result.total_tests = 1
            else:
                print(f"âŒ æ¸¬è©¦å¥—ä»¶ {suite.value} åŸ·è¡Œå¤±æ•— (é€€å‡ºç¢¼: {exit_code})")
                result.failed = 1  # ç°¡åŒ–
                result.total_tests = 1

            return result

        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time

            print(f"âŒ æ¸¬è©¦å¥—ä»¶åŸ·è¡Œç•°å¸¸: {e}")

            return TestSuiteResult(
                suite_name=suite.value,
                total_tests=1,
                passed=0,
                failed=0,
                skipped=0,
                errors=1,
                duration=duration,
            )

    async def run_all_tests(
        self, include_performance: bool = False, verbose: bool = False
    ) -> Dict[str, TestSuiteResult]:
        """
        é‹è¡Œæ‰€æœ‰æ¸¬è©¦å¥—ä»¶

        Args:
            include_performance: æ˜¯å¦åŒ…å«æ€§èƒ½æ¸¬è©¦
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°è¼¸å‡º

        Returns:
            Dict[str, TestSuiteResult]: æ‰€æœ‰æ¸¬è©¦çµæœ
        """

        suites_to_run = [TestSuite.UNIT, TestSuite.INTEGRATION]

        if include_performance:
            suites_to_run.append(TestSuite.PERFORMANCE)

        results = {}

        for suite in suites_to_run:
            try:
                result = await self.run_test_suite(suite, verbose=verbose)
                results[suite.value] = result
                self.results[suite.value] = result
            except Exception as e:
                print(f"é‹è¡Œæ¸¬è©¦å¥—ä»¶ {suite.value} æ™‚å‡ºéŒ¯: {e}")
                results[suite.value] = TestSuiteResult(
                    suite_name=suite.value,
                    total_tests=0,
                    passed=0,
                    failed=0,
                    skipped=0,
                    errors=1,
                    duration=0.0,
                )

        return results

    def generate_report(
        self, results: Dict[str, TestSuiteResult], output_file: Optional[Path] = None
    ) -> str:
        """
        ç”Ÿæˆæ¸¬è©¦å ±å‘Š

        Args:
            results: æ¸¬è©¦çµæœ
            output_file: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘

        Returns:
            str: å ±å‘Šå…§å®¹
        """

        report_lines = []
        report_lines.append("# AutoGenç³»çµ±æ¸¬è©¦å ±å‘Š")
        report_lines.append(f"\n**ç”Ÿæˆæ™‚é–“**: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"**é …ç›®è·¯å¾‘**: {self.project_root}")

        # ç¸½é«”æ¦‚æ³
        total_tests = sum(r.total_tests for r in results.values())
        total_passed = sum(r.passed for r in results.values())
        total_failed = sum(r.failed for r in results.values())
        total_skipped = sum(r.skipped for r in results.values())
        total_errors = sum(r.errors for r in results.values())
        total_duration = sum(r.duration for r in results.values())

        overall_success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0

        report_lines.append("\n## ğŸ“Š ç¸½é«”æ¦‚æ³")
        report_lines.append(f"- **ç¸½æ¸¬è©¦æ•¸**: {total_tests}")
        report_lines.append(f"- **é€šé**: {total_passed} âœ…")
        report_lines.append(f"- **å¤±æ•—**: {total_failed} âŒ")
        report_lines.append(f"- **è·³é**: {total_skipped} â­ï¸")
        report_lines.append(f"- **éŒ¯èª¤**: {total_errors} ğŸ’¥")
        report_lines.append(f"- **æˆåŠŸç‡**: {overall_success_rate:.1f}%")
        report_lines.append(f"- **ç¸½è€—æ™‚**: {total_duration:.3f}s")

        # å„å¥—ä»¶è©³æƒ…
        report_lines.append("\n## ğŸ“‹ æ¸¬è©¦å¥—ä»¶è©³æƒ…")

        for suite_name, result in results.items():
            status_emoji = "âœ…" if result.failed == 0 and result.errors == 0 else "âŒ"

            report_lines.append(f"\n### {status_emoji} {suite_name.upper()}")
            report_lines.append(f"- **æ¸¬è©¦æ•¸**: {result.total_tests}")
            report_lines.append(f"- **é€šé**: {result.passed}")
            report_lines.append(f"- **å¤±æ•—**: {result.failed}")
            report_lines.append(f"- **è·³é**: {result.skipped}")
            report_lines.append(f"- **éŒ¯èª¤**: {result.errors}")
            report_lines.append(f"- **æˆåŠŸç‡**: {result.success_rate:.1f}%")
            report_lines.append(f"- **è€—æ™‚**: {result.duration:.3f}s")

            if result.coverage_percentage is not None:
                report_lines.append(f"- **ä»£ç¢¼è¦†è“‹ç‡**: {result.coverage_percentage:.1f}%")

        # æ€§èƒ½æŒ‡æ¨™ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        if "performance" in results:
            perf_result = results["performance"]
            report_lines.append("\n## ğŸš€ æ€§èƒ½æŒ‡æ¨™")
            report_lines.append(f"- **æ€§èƒ½æ¸¬è©¦è€—æ™‚**: {perf_result.duration:.3f}s")
            report_lines.append(
                f"- **æ€§èƒ½æ¸¬è©¦ç‹€æ…‹**: {'é€šé' if perf_result.failed == 0 else 'å¤±æ•—'}"
            )

        # å»ºè­°å’Œç¸½çµ
        report_lines.append("\n## ğŸ’¡ å»ºè­°å’Œç¸½çµ")

        if overall_success_rate >= 95:
            report_lines.append("- âœ… æ¸¬è©¦è¦†è“‹è‰¯å¥½ï¼Œç³»çµ±ç©©å®šæ€§é«˜")
        elif overall_success_rate >= 80:
            report_lines.append("- âš ï¸ æ¸¬è©¦åŸºæœ¬é€šéï¼Œå»ºè­°é—œæ³¨å¤±æ•—çš„æ¸¬è©¦æ¡ˆä¾‹")
        else:
            report_lines.append("- âŒ æ¸¬è©¦å¤±æ•—ç‡è¼ƒé«˜ï¼Œéœ€è¦é‡é»æ”¹é€²")

        if total_errors > 0:
            report_lines.append("- ğŸ”§ ç™¼ç¾ç³»çµ±éŒ¯èª¤ï¼Œå»ºè­°å„ªå…ˆä¿®å¾©")

        if any(r.coverage_percentage and r.coverage_percentage < 80 for r in results.values()):
            report_lines.append("- ğŸ“ˆ å»ºè­°æé«˜ä»£ç¢¼è¦†è“‹ç‡è‡³80%ä»¥ä¸Š")

        report_content = "\n".join(report_lines)

        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_file:
            output_file.write_text(report_content, encoding="utf-8")
            print(f"\nğŸ“„ æ¸¬è©¦å ±å‘Šå·²ä¿å­˜åˆ°: {output_file}")

        return report_content

    def generate_json_report(
        self, results: Dict[str, TestSuiteResult], output_file: Optional[Path] = None
    ) -> Dict[str, Any]:
        """
        ç”ŸæˆJSONæ ¼å¼çš„æ¸¬è©¦å ±å‘Š

        Args:
            results: æ¸¬è©¦çµæœ
            output_file: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘

        Returns:
            Dict[str, Any]: JSONå ±å‘Šæ•¸æ“š
        """

        report_data = {
            "timestamp": time.time(),
            "date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "project_root": str(self.project_root),
            "summary": {
                "total_tests": sum(r.total_tests for r in results.values()),
                "total_passed": sum(r.passed for r in results.values()),
                "total_failed": sum(r.failed for r in results.values()),
                "total_skipped": sum(r.skipped for r in results.values()),
                "total_errors": sum(r.errors for r in results.values()),
                "total_duration": sum(r.duration for r in results.values()),
                "overall_success_rate": 0.0,
            },
            "suites": {},
        }

        # è¨ˆç®—ç¸½é«”æˆåŠŸç‡
        total_tests = report_data["summary"]["total_tests"]
        if total_tests > 0:
            success_rate = (report_data["summary"]["total_passed"] / total_tests) * 100
            report_data["summary"]["overall_success_rate"] = success_rate

        # æ·»åŠ å„å¥—ä»¶çµæœ
        for suite_name, result in results.items():
            report_data["suites"][suite_name] = asdict(result)

        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_file:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“Š JSONå ±å‘Šå·²ä¿å­˜åˆ°: {output_file}")

        return report_data


async def main():
    """ä¸»å‡½æ•¸"""

    parser = argparse.ArgumentParser(description="AutoGenç³»çµ±æ¸¬è©¦é‹è¡Œå™¨")
    parser.add_argument(
        "--suite",
        choices=["unit", "integration", "performance", "all"],
        default="all",
        help="è¦é‹è¡Œçš„æ¸¬è©¦å¥—ä»¶",
    )
    parser.add_argument("--verbose", "-v", action="store_true", help="é¡¯ç¤ºè©³ç´°è¼¸å‡º")
    parser.add_argument("--no-performance", action="store_true", help="è·³éæ€§èƒ½æ¸¬è©¦")
    parser.add_argument(
        "--output-dir", type=Path, default=Path("test_reports"), help="å ±å‘Šè¼¸å‡ºç›®éŒ„"
    )
    parser.add_argument("--no-capture", action="store_true", help="ä¸æ•ç²æ¸¬è©¦è¼¸å‡º")

    args = parser.parse_args()

    # å‰µå»ºæ¸¬è©¦é‹è¡Œå™¨
    runner = AutoGenTestRunner()

    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    args.output_dir.mkdir(exist_ok=True)

    print("ğŸš€ AutoGenç³»çµ±æ¸¬è©¦é‹è¡Œå™¨")
    print(f"ğŸ“ é …ç›®è·¯å¾‘: {runner.project_root}")
    print(f"ğŸ“‹ æ¸¬è©¦å¥—ä»¶: {args.suite}")

    try:
        if args.suite == "all":
            # é‹è¡Œæ‰€æœ‰æ¸¬è©¦
            results = await runner.run_all_tests(
                include_performance=not args.no_performance, verbose=args.verbose
            )
        else:
            # é‹è¡ŒæŒ‡å®šæ¸¬è©¦å¥—ä»¶
            suite = TestSuite(args.suite)
            result = await runner.run_test_suite(
                suite, verbose=args.verbose, capture_output=not args.no_capture
            )
            results = {suite.value: result}

        # ç”Ÿæˆå ±å‘Š
        timestamp = time.strftime("%Y%m%d_%H%M%S")

        # Markdownå ±å‘Š
        md_report_file = args.output_dir / f"test_report_{timestamp}.md"
        runner.generate_report(results, md_report_file)

        # JSONå ±å‘Š
        json_report_file = args.output_dir / f"test_report_{timestamp}.json"
        runner.generate_json_report(results, json_report_file)

        # è¼¸å‡ºæ‘˜è¦
        total_tests = sum(r.total_tests for r in results.values())
        total_failed = sum(r.failed for r in results.values())
        total_errors = sum(r.errors for r in results.values())

        print(f"\n{'=' * 60}")
        print("ğŸ“‹ æ¸¬è©¦å®Œæˆç¸½çµ")
        print(f"{'=' * 60}")

        if total_failed == 0 and total_errors == 0:
            print("âœ… æ‰€æœ‰æ¸¬è©¦é€šéï¼")
            exit_code = 0
        else:
            print(f"âŒ æ¸¬è©¦å¤±æ•—: {total_failed} å¤±æ•—, {total_errors} éŒ¯èª¤")
            exit_code = 1

        print(f"ğŸ“Š è©³ç´°å ±å‘Š: {md_report_file}")
        print(f"ğŸ“ˆ JSONæ•¸æ“š: {json_report_file}")

        sys.exit(exit_code)

    except KeyboardInterrupt:
        print("\nâš ï¸ æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(130)
    except Exception as e:
        print(f"\nğŸ’¥ æ¸¬è©¦é‹è¡Œå™¨éŒ¯èª¤: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
