# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
AutoGen çˆ¬èŸ²å·¥å…·

æä¾›ç¶²é å…§å®¹çˆ¬å–å’Œè™•ç†åŠŸèƒ½ã€‚
"""

import asyncio
import json
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from datetime import datetime
from urllib.parse import urlparse, urljoin

from src.crawler import Crawler
from src.logging import get_logger

logger = get_logger(__name__)


@dataclass
class CrawlResult:
    """çˆ¬èŸ²çµæœ"""

    url: str
    title: str
    content: str
    markdown_content: str
    success: bool
    error: Optional[str]
    crawl_time: float
    timestamp: datetime
    content_length: int


class AutoGenCrawlTool:
    """
    AutoGen çˆ¬èŸ²å·¥å…·

    æä¾›ç¶²é å…§å®¹çˆ¬å–åŠŸèƒ½ï¼Œæ”¯æ´å¤šç¨®æ ¼å¼è¼¸å‡ºã€‚
    """

    def __init__(self, timeout_seconds: int = 30, max_content_length: int = 10000):
        """
        åˆå§‹åŒ–çˆ¬èŸ²å·¥å…·

        Args:
            timeout_seconds: çˆ¬èŸ²è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
            max_content_length: æœ€å¤§å…§å®¹é•·åº¦
        """
        self.timeout_seconds = timeout_seconds
        self.max_content_length = max_content_length
        self.crawler = Crawler()
        self.crawl_history: List[CrawlResult] = []

        logger.info(
            f"AutoGen çˆ¬èŸ²å·¥å…·åˆå§‹åŒ–å®Œæˆï¼Œè¶…æ™‚: {timeout_seconds}ç§’ï¼Œæœ€å¤§é•·åº¦: {max_content_length}"
        )

    async def crawl_url(self, url: str, format_type: str = "markdown") -> str:
        """
        çˆ¬å–ç¶²é å…§å®¹

        Args:
            url: è¦çˆ¬å–çš„ç¶²å€
            format_type: è¼¸å‡ºæ ¼å¼ï¼ˆmarkdown, text, jsonï¼‰

        Returns:
            str: çˆ¬å–çµæœ
        """
        if not self._is_valid_url(url):
            error_msg = f"ç„¡æ•ˆçš„ç¶²å€: {url}"
            logger.error(error_msg)
            return self._format_error_result(url, error_msg)

        logger.info(f"é–‹å§‹çˆ¬å–ç¶²å€: {url}")
        start_time = asyncio.get_event_loop().time()

        try:
            # åœ¨äº‹ä»¶å¾ªç’°ä¸­åŸ·è¡Œçˆ¬èŸ²
            result = await asyncio.wait_for(self._crawl_async(url), timeout=self.timeout_seconds)

            crawl_time = asyncio.get_event_loop().time() - start_time

            # è™•ç†å…§å®¹é•·åº¦é™åˆ¶
            if len(result.content) > self.max_content_length:
                result.content = result.content[: self.max_content_length] + "...[å…§å®¹éé•·ï¼Œå·²æˆªæ–·]"
                result.markdown_content = (
                    result.markdown_content[: self.max_content_length] + "...[å…§å®¹éé•·ï¼Œå·²æˆªæ–·]"
                )

            # è¨˜éŒ„çˆ¬èŸ²æ­·å²
            crawl_result = CrawlResult(
                url=url,
                title=result.title or "ç„¡æ¨™é¡Œ",
                content=result.content,
                markdown_content=result.markdown_content,
                success=True,
                error=None,
                crawl_time=crawl_time,
                timestamp=datetime.now(),
                content_length=len(result.content),
            )
            self.crawl_history.append(crawl_result)

            logger.info(f"ç¶²é çˆ¬å–æˆåŠŸ: {url}ï¼Œè€—æ™‚: {crawl_time:.2f}ç§’")
            return self._format_success_result(crawl_result, format_type)

        except asyncio.TimeoutError:
            crawl_time = asyncio.get_event_loop().time() - start_time
            error_msg = f"çˆ¬èŸ²è¶…æ™‚ï¼ˆ{self.timeout_seconds}ç§’ï¼‰"
            logger.error(error_msg)
            return self._format_timeout_result(url, crawl_time)

        except Exception as e:
            crawl_time = asyncio.get_event_loop().time() - start_time
            error_msg = f"çˆ¬èŸ²å¤±æ•—: {str(e)}"
            logger.error(error_msg)

            # è¨˜éŒ„å¤±æ•—æ­·å²
            crawl_result = CrawlResult(
                url=url,
                title="",
                content="",
                markdown_content="",
                success=False,
                error=str(e),
                crawl_time=crawl_time,
                timestamp=datetime.now(),
                content_length=0,
            )
            self.crawl_history.append(crawl_result)

            return self._format_error_result(url, error_msg, crawl_time)

    async def _crawl_async(self, url: str) -> Any:
        """éåŒæ­¥çˆ¬å–"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.crawler.crawl, url)

    def _is_valid_url(self, url: str) -> bool:
        """é©—è­‰ç¶²å€æ ¼å¼"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except Exception:
            return False

    def _format_success_result(self, result: CrawlResult, format_type: str) -> str:
        """æ ¼å¼åŒ–æˆåŠŸçµæœ"""
        if format_type == "json":
            return json.dumps(
                {
                    "url": result.url,
                    "title": result.title,
                    "content": result.content,
                    "markdown_content": result.markdown_content,
                    "success": True,
                    "crawl_time": result.crawl_time,
                    "timestamp": result.timestamp.isoformat(),
                    "content_length": result.content_length,
                },
                ensure_ascii=False,
                indent=2,
            )

        elif format_type == "text":
            return f"""ç¶²é çˆ¬å–æˆåŠŸ

ç¶²å€: {result.url}
æ¨™é¡Œ: {result.title}
å…§å®¹é•·åº¦: {result.content_length} å­—å…ƒ
çˆ¬å–æ™‚é–“: {result.crawl_time:.2f} ç§’

å…§å®¹:
{result.content}
"""

        else:  # markdown (é è¨­)
            return f"""# ğŸ•·ï¸ ç¶²é çˆ¬å–çµæœ

**ç¶²å€:** {result.url}
**æ¨™é¡Œ:** {result.title}
**çˆ¬å–æ™‚é–“:** {result.crawl_time:.2f} ç§’
**å…§å®¹é•·åº¦:** {result.content_length} å­—å…ƒ
**æ™‚é–“æˆ³:** {result.timestamp.strftime("%Y-%m-%d %H:%M:%S")}

## ğŸ“„ ç¶²é å…§å®¹

{result.markdown_content}
"""

    def _format_error_result(self, url: str, error_msg: str, crawl_time: float = 0) -> str:
        """æ ¼å¼åŒ–éŒ¯èª¤çµæœ"""
        return f"""âŒ ç¶²é çˆ¬å–å¤±æ•—

**ç¶²å€:** {url}
**éŒ¯èª¤è¨Šæ¯:** {error_msg}
**çˆ¬å–æ™‚é–“:** {crawl_time:.2f} ç§’
**æ™‚é–“æˆ³:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

**å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:**
1. æª¢æŸ¥ç¶²å€æ˜¯å¦æ­£ç¢º
2. ç¢ºèªç¶²ç«™æ˜¯å¦å¯ä»¥è¨ªå•
3. æª¢æŸ¥ç¶²è·¯é€£æ¥æ˜¯å¦æ­£å¸¸
4. å˜—è©¦ä½¿ç”¨å…¶ä»–çˆ¬èŸ²å·¥å…·
"""

    def _format_timeout_result(self, url: str, crawl_time: float) -> str:
        """æ ¼å¼åŒ–è¶…æ™‚çµæœ"""
        return f"""â° ç¶²é çˆ¬å–è¶…æ™‚

**ç¶²å€:** {url}
**è¨­å®šè¶…æ™‚:** {self.timeout_seconds} ç§’
**å¯¦éš›è€—æ™‚:** {crawl_time:.2f} ç§’
**æ™‚é–“æˆ³:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

**å»ºè­°:**
1. å¢åŠ è¶…æ™‚æ™‚é–“è¨­å®š
2. æª¢æŸ¥ç¶²ç«™è¼‰å…¥é€Ÿåº¦
3. å˜—è©¦ç›´æ¥è¨ªå•ç¶²ç«™
4. ä½¿ç”¨å…¶ä»–çˆ¬èŸ²ç­–ç•¥
"""

    async def crawl_multiple_urls(self, urls: List[str], format_type: str = "markdown") -> str:
        """
        æ‰¹é‡çˆ¬å–å¤šå€‹ç¶²å€

        Args:
            urls: ç¶²å€åˆ—è¡¨
            format_type: è¼¸å‡ºæ ¼å¼

        Returns:
            str: æ‰¹é‡çˆ¬å–çµæœ
        """
        logger.info(f"é–‹å§‹æ‰¹é‡çˆ¬å– {len(urls)} å€‹ç¶²å€")

        results = []
        for i, url in enumerate(urls, 1):
            logger.info(f"çˆ¬å–é€²åº¦: {i}/{len(urls)} - {url}")
            result = await self.crawl_url(url, format_type)
            results.append(f"## ç¶²å€ {i}: {url}\n\n{result}\n\n---\n")

        combined_result = f"""# ğŸ•·ï¸ æ‰¹é‡ç¶²é çˆ¬å–çµæœ

**ç¸½è¨ˆç¶²å€æ•¸:** {len(urls)}
**å®Œæˆæ™‚é–“:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

{"".join(results)}

## ğŸ“Š æ‰¹é‡çˆ¬å–çµ±è¨ˆ

{self._get_batch_stats(urls)}
"""

        logger.info(f"æ‰¹é‡çˆ¬å–å®Œæˆï¼Œå…±è™•ç† {len(urls)} å€‹ç¶²å€")
        return combined_result

    def _get_batch_stats(self, urls: List[str]) -> str:
        """ç²å–æ‰¹é‡çˆ¬å–çµ±è¨ˆ"""
        recent_results = (
            self.crawl_history[-len(urls) :]
            if len(self.crawl_history) >= len(urls)
            else self.crawl_history
        )

        successful = sum(1 for r in recent_results if r.success)
        total_time = sum(r.crawl_time for r in recent_results)
        total_content = sum(r.content_length for r in recent_results)

        return f"""- æˆåŠŸçˆ¬å–: {successful}/{len(urls)} ({successful / len(urls) * 100:.1f}%)
- ç¸½è€—æ™‚: {total_time:.2f} ç§’
- å¹³å‡è€—æ™‚: {total_time / len(urls):.2f} ç§’/ç¶²å€
- ç¸½å…§å®¹é•·åº¦: {total_content:,} å­—å…ƒ
- å¹³å‡å…§å®¹é•·åº¦: {total_content // len(urls):,} å­—å…ƒ/ç¶²å€"""

    def get_crawl_history(self) -> List[Dict[str, Any]]:
        """ç²å–çˆ¬èŸ²æ­·å²"""
        return [
            {
                "url": result.url,
                "title": result.title,
                "success": result.success,
                "crawl_time": result.crawl_time,
                "content_length": result.content_length,
                "timestamp": result.timestamp.isoformat(),
                "has_error": result.error is not None,
            }
            for result in self.crawl_history
        ]

    def clear_history(self):
        """æ¸…é™¤çˆ¬èŸ²æ­·å²"""
        self.crawl_history.clear()
        logger.info("çˆ¬èŸ²æ­·å²å·²æ¸…é™¤")

    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        if not self.crawl_history:
            return {
                "total_crawls": 0,
                "successful_crawls": 0,
                "failed_crawls": 0,
                "average_crawl_time": 0,
                "total_content_length": 0,
                "average_content_length": 0,
            }

        successful = sum(1 for r in self.crawl_history if r.success)
        total_time = sum(r.crawl_time for r in self.crawl_history)
        total_content = sum(r.content_length for r in self.crawl_history)

        return {
            "total_crawls": len(self.crawl_history),
            "successful_crawls": successful,
            "failed_crawls": len(self.crawl_history) - successful,
            "success_rate": successful / len(self.crawl_history) * 100,
            "average_crawl_time": total_time / len(self.crawl_history),
            "total_crawl_time": total_time,
            "total_content_length": total_content,
            "average_content_length": total_content / len(self.crawl_history)
            if self.crawl_history
            else 0,
        }


# ä¾¿åˆ©å‡½æ•¸
async def crawl_url(url: str, format_type: str = "markdown") -> str:
    """çˆ¬å–å–®ä¸€ç¶²å€"""
    tool = AutoGenCrawlTool()
    return await tool.crawl_url(url, format_type)


async def crawl_multiple_urls(urls: List[str], format_type: str = "markdown") -> str:
    """æ‰¹é‡çˆ¬å–å¤šå€‹ç¶²å€"""
    tool = AutoGenCrawlTool()
    return await tool.crawl_multiple_urls(urls, format_type)
