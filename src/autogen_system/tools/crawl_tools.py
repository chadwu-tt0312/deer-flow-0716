# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
AutoGen Áà¨Ëü≤Â∑•ÂÖ∑

Êèê‰æõÁ∂≤È†ÅÂÖßÂÆπÁà¨ÂèñÂíåËôïÁêÜÂäüËÉΩ„ÄÇ
"""

import asyncio
import json
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from datetime import datetime
from urllib.parse import urlparse, urljoin

from src.crawler import Crawler
from src.logging import get_logger

logger = get_logger(__name__)


@dataclass
class CrawlResult:
    """Áà¨Ëü≤ÁµêÊûú"""

    url: str
    title: str
    content: str
    markdown_content: str
    success: bool
    error: Optional[str]
    crawl_time: float
    timestamp: datetime
    content_length: int


class AutoGenCrawlTool:
    """
    AutoGen Áà¨Ëü≤Â∑•ÂÖ∑

    Êèê‰æõÁ∂≤È†ÅÂÖßÂÆπÁà¨ÂèñÂäüËÉΩÔºåÊîØÊè¥Â§öÁ®ÆÊ†ºÂºèËº∏Âá∫„ÄÇ
    """

    def __init__(self, timeout_seconds: int = 30, max_content_length: int = 10000):
        """
        ÂàùÂßãÂåñÁà¨Ëü≤Â∑•ÂÖ∑

        Args:
            timeout_seconds: Áà¨Ëü≤Ë∂ÖÊôÇÊôÇÈñìÔºàÁßíÔºâ
            max_content_length: ÊúÄÂ§ßÂÖßÂÆπÈï∑Â∫¶
        """
        self.timeout_seconds = timeout_seconds
        self.max_content_length = max_content_length
        self.crawler = Crawler()
        self.crawl_history: List[CrawlResult] = []

        logger.info(
            f"AutoGen Áà¨Ëü≤Â∑•ÂÖ∑ÂàùÂßãÂåñÂÆåÊàêÔºåË∂ÖÊôÇ: {timeout_seconds}ÁßíÔºåÊúÄÂ§ßÈï∑Â∫¶: {max_content_length}"
        )

    async def crawl_url(self, url: str, format_type: str = "markdown") -> str:
        """
        Áà¨ÂèñÁ∂≤È†ÅÂÖßÂÆπ

        Args:
            url: Ë¶ÅÁà¨ÂèñÁöÑÁ∂≤ÂùÄ
            format_type: Ëº∏Âá∫Ê†ºÂºèÔºàmarkdown, text, jsonÔºâ

        Returns:
            str: Áà¨ÂèñÁµêÊûú
        """
        if not self._is_valid_url(url):
            error_msg = f"ÁÑ°ÊïàÁöÑÁ∂≤ÂùÄ: {url}"
            logger.error(error_msg)
            return self._format_error_result(url, error_msg)

        logger.info(f"ÈñãÂßãÁà¨ÂèñÁ∂≤ÂùÄ: {url}")
        start_time = asyncio.get_event_loop().time()

        try:
            # Âú®‰∫ã‰ª∂Âæ™Áí∞‰∏≠Âü∑Ë°åÁà¨Ëü≤
            result = await asyncio.wait_for(self._crawl_async(url), timeout=self.timeout_seconds)

            crawl_time = asyncio.get_event_loop().time() - start_time

            # ËôïÁêÜÂÖßÂÆπÈï∑Â∫¶ÈôêÂà∂
            if len(result.content) > self.max_content_length:
                result.content = result.content[: self.max_content_length] + "...[ÂÖßÂÆπÈÅéÈï∑ÔºåÂ∑≤Êà™Êñ∑]"
                result.markdown_content = (
                    result.markdown_content[: self.max_content_length] + "...[ÂÖßÂÆπÈÅéÈï∑ÔºåÂ∑≤Êà™Êñ∑]"
                )

            # Ë®òÈåÑÁà¨Ëü≤Ê≠∑Âè≤
            crawl_result = CrawlResult(
                url=url,
                title=result.title or "ÁÑ°Ê®ôÈ°å",
                content=result.content,
                markdown_content=result.markdown_content,
                success=True,
                error=None,
                crawl_time=crawl_time,
                timestamp=datetime.now(),
                content_length=len(result.content),
            )
            self.crawl_history.append(crawl_result)

            logger.info(f"Á∂≤È†ÅÁà¨ÂèñÊàêÂäü: {url}ÔºåËÄóÊôÇ: {crawl_time:.2f}Áßí")
            return self._format_success_result(crawl_result, format_type)

        except asyncio.TimeoutError:
            crawl_time = asyncio.get_event_loop().time() - start_time
            error_msg = f"Áà¨Ëü≤Ë∂ÖÊôÇÔºà{self.timeout_seconds}ÁßíÔºâ"
            logger.error(error_msg)
            return self._format_timeout_result(url, crawl_time)

        except Exception as e:
            crawl_time = asyncio.get_event_loop().time() - start_time
            error_msg = f"Áà¨Ëü≤Â§±Êïó: {str(e)}"
            logger.error(error_msg)

            # Ë®òÈåÑÂ§±ÊïóÊ≠∑Âè≤
            crawl_result = CrawlResult(
                url=url,
                title="",
                content="",
                markdown_content="",
                success=False,
                error=str(e),
                crawl_time=crawl_time,
                timestamp=datetime.now(),
                content_length=0,
            )
            self.crawl_history.append(crawl_result)

            return self._format_error_result(url, error_msg, crawl_time)

    async def _crawl_async(self, url: str) -> Any:
        """ÈùûÂêåÊ≠•Áà¨Âèñ"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.crawler.crawl, url)

    def _is_valid_url(self, url: str) -> bool:
        """È©óË≠âÁ∂≤ÂùÄÊ†ºÂºè"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except Exception:
            return False

    def _format_success_result(self, result: CrawlResult, format_type: str) -> str:
        """Ê†ºÂºèÂåñÊàêÂäüÁµêÊûú"""
        if format_type == "json":
            return json.dumps(
                {
                    "url": result.url,
                    "title": result.title,
                    "content": result.content,
                    "markdown_content": result.markdown_content,
                    "success": True,
                    "crawl_time": result.crawl_time,
                    "timestamp": result.timestamp.isoformat(),
                    "content_length": result.content_length,
                },
                ensure_ascii=False,
                indent=2,
            )

        elif format_type == "text":
            return f"""Á∂≤È†ÅÁà¨ÂèñÊàêÂäü

Á∂≤ÂùÄ: {result.url}
Ê®ôÈ°å: {result.title}
ÂÖßÂÆπÈï∑Â∫¶: {result.content_length} Â≠óÂÖÉ
Áà¨ÂèñÊôÇÈñì: {result.crawl_time:.2f} Áßí

ÂÖßÂÆπ:
{result.content}
"""

        else:  # markdown (È†êË®≠)
            return f"""# üï∑Ô∏è Á∂≤È†ÅÁà¨ÂèñÁµêÊûú

**Á∂≤ÂùÄ:** {result.url}
**Ê®ôÈ°å:** {result.title}
**Áà¨ÂèñÊôÇÈñì:** {result.crawl_time:.2f} Áßí
**ÂÖßÂÆπÈï∑Â∫¶:** {result.content_length} Â≠óÂÖÉ
**ÊôÇÈñìÊà≥:** {result.timestamp.strftime("%Y-%m-%d %H:%M:%S")}

## üìÑ Á∂≤È†ÅÂÖßÂÆπ

{result.markdown_content}
"""

    def _format_error_result(self, url: str, error_msg: str, crawl_time: float = 0) -> str:
        """Ê†ºÂºèÂåñÈåØË™§ÁµêÊûú"""
        return f"""‚ùå Á∂≤È†ÅÁà¨ÂèñÂ§±Êïó

**Á∂≤ÂùÄ:** {url}
**ÈåØË™§Ë®äÊÅØ:** {error_msg}
**Áà¨ÂèñÊôÇÈñì:** {crawl_time:.2f} Áßí
**ÊôÇÈñìÊà≥:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

**ÂèØËÉΩÁöÑËß£Ê±∫ÊñπÊ°à:**
1. Ê™¢Êü•Á∂≤ÂùÄÊòØÂê¶Ê≠£Á¢∫
2. Á¢∫Ë™çÁ∂≤Á´ôÊòØÂê¶ÂèØ‰ª•Ë®™Âïè
3. Ê™¢Êü•Á∂≤Ë∑ØÈÄ£Êé•ÊòØÂê¶Ê≠£Â∏∏
4. ÂòóË©¶‰ΩøÁî®ÂÖ∂‰ªñÁà¨Ëü≤Â∑•ÂÖ∑
"""

    def _format_timeout_result(self, url: str, crawl_time: float) -> str:
        """Ê†ºÂºèÂåñË∂ÖÊôÇÁµêÊûú"""
        return f"""‚è∞ Á∂≤È†ÅÁà¨ÂèñË∂ÖÊôÇ

**Á∂≤ÂùÄ:** {url}
**Ë®≠ÂÆöË∂ÖÊôÇ:** {self.timeout_seconds} Áßí
**ÂØ¶ÈöõËÄóÊôÇ:** {crawl_time:.2f} Áßí
**ÊôÇÈñìÊà≥:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

**Âª∫Ë≠∞:**
1. Â¢ûÂä†Ë∂ÖÊôÇÊôÇÈñìË®≠ÂÆö
2. Ê™¢Êü•Á∂≤Á´ôËºâÂÖ•ÈÄüÂ∫¶
3. ÂòóË©¶Áõ¥Êé•Ë®™ÂïèÁ∂≤Á´ô
4. ‰ΩøÁî®ÂÖ∂‰ªñÁà¨Ëü≤Á≠ñÁï•
"""

    async def crawl_multiple_urls(self, urls: List[str], format_type: str = "markdown") -> str:
        """
        ÊâπÈáèÁà¨ÂèñÂ§öÂÄãÁ∂≤ÂùÄ

        Args:
            urls: Á∂≤ÂùÄÂàóË°®
            format_type: Ëº∏Âá∫Ê†ºÂºè

        Returns:
            str: ÊâπÈáèÁà¨ÂèñÁµêÊûú
        """
        logger.info(f"ÈñãÂßãÊâπÈáèÁà¨Âèñ {len(urls)} ÂÄãÁ∂≤ÂùÄ")

        results = []
        for i, url in enumerate(urls, 1):
            logger.info(f"Áà¨ÂèñÈÄ≤Â∫¶: {i}/{len(urls)} - {url}")
            result = await self.crawl_url(url, format_type)
            results.append(f"## Á∂≤ÂùÄ {i}: {url}\n\n{result}\n\n---\n")

        combined_result = f"""# üï∑Ô∏è ÊâπÈáèÁ∂≤È†ÅÁà¨ÂèñÁµêÊûú

**Á∏ΩË®àÁ∂≤ÂùÄÊï∏:** {len(urls)}
**ÂÆåÊàêÊôÇÈñì:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

{"".join(results)}

## üìä ÊâπÈáèÁà¨ÂèñÁµ±Ë®à

{self._get_batch_stats(urls)}
"""

        logger.info(f"ÊâπÈáèÁà¨ÂèñÂÆåÊàêÔºåÂÖ±ËôïÁêÜ {len(urls)} ÂÄãÁ∂≤ÂùÄ")
        return combined_result

    def _get_batch_stats(self, urls: List[str]) -> str:
        """Áç≤ÂèñÊâπÈáèÁà¨ÂèñÁµ±Ë®à"""
        recent_results = (
            self.crawl_history[-len(urls) :]
            if len(self.crawl_history) >= len(urls)
            else self.crawl_history
        )

        successful = sum(1 for r in recent_results if r.success)
        total_time = sum(r.crawl_time for r in recent_results)
        total_content = sum(r.content_length for r in recent_results)

        return f"""- ÊàêÂäüÁà¨Âèñ: {successful}/{len(urls)} ({successful / len(urls) * 100:.1f}%)
- Á∏ΩËÄóÊôÇ: {total_time:.2f} Áßí
- Âπ≥ÂùáËÄóÊôÇ: {total_time / len(urls):.2f} Áßí/Á∂≤ÂùÄ
- Á∏ΩÂÖßÂÆπÈï∑Â∫¶: {total_content:,} Â≠óÂÖÉ
- Âπ≥ÂùáÂÖßÂÆπÈï∑Â∫¶: {total_content // len(urls):,} Â≠óÂÖÉ/Á∂≤ÂùÄ"""

    def get_crawl_history(self) -> List[Dict[str, Any]]:
        """Áç≤ÂèñÁà¨Ëü≤Ê≠∑Âè≤"""
        return [
            {
                "url": result.url,
                "title": result.title,
                "success": result.success,
                "crawl_time": result.crawl_time,
                "content_length": result.content_length,
                "timestamp": result.timestamp.isoformat(),
                "has_error": result.error is not None,
            }
            for result in self.crawl_history
        ]

    def clear_history(self):
        """Ê∏ÖÈô§Áà¨Ëü≤Ê≠∑Âè≤"""
        self.crawl_history.clear()
        logger.info("Áà¨Ëü≤Ê≠∑Âè≤Â∑≤Ê∏ÖÈô§")

    def get_stats(self) -> Dict[str, Any]:
        """Áç≤ÂèñÁµ±Ë®àË≥áË®ä"""
        if not self.crawl_history:
            return {
                "total_crawls": 0,
                "successful_crawls": 0,
                "failed_crawls": 0,
                "average_crawl_time": 0,
                "total_content_length": 0,
                "average_content_length": 0,
            }

        successful = sum(1 for r in self.crawl_history if r.success)
        total_time = sum(r.crawl_time for r in self.crawl_history)
        total_content = sum(r.content_length for r in self.crawl_history)

        return {
            "total_crawls": len(self.crawl_history),
            "successful_crawls": successful,
            "failed_crawls": len(self.crawl_history) - successful,
            "success_rate": successful / len(self.crawl_history) * 100,
            "average_crawl_time": total_time / len(self.crawl_history),
            "total_crawl_time": total_time,
            "total_content_length": total_content,
            "average_content_length": total_content / len(self.crawl_history)
            if self.crawl_history
            else 0,
        }


# ‰æøÂà©ÂáΩÊï∏
async def crawl_url(url: str, format_type: str = "markdown") -> str:
    """Áà¨ÂèñÂñÆ‰∏ÄÁ∂≤ÂùÄ"""
    tool = AutoGenCrawlTool()
    return await tool.crawl_url(url, format_type)


async def crawl_multiple_urls(urls: List[str], format_type: str = "markdown") -> str:
    """ÊâπÈáèÁà¨ÂèñÂ§öÂÄãÁ∂≤ÂùÄ"""
    tool = AutoGenCrawlTool()
    return await tool.crawl_multiple_urls(urls, format_type)
