# èªªæ˜ python æª”æ¡ˆç”¨é€”èˆ‡æ•´ç†å»ºè­°
_Exported on 2025/9/2 at 17:14:35 [GMT+8] from Cursor (1.5.9)_

---

**User**

@autogen_system/  ç›®éŒ„ä¸‹çš„ python æª”æ¡ˆï¼Œè«‹èªªæ˜ç”¨é€”èˆ‡æ˜¯å¦å¯ç§»é™¤? æˆ–è€…éœ€è¦é‡æ§‹/æ•´ç†

---

**Cursor**

æˆ‘å°‡åˆ†æ `autogen_system/` ç›®éŒ„ä¸‹çš„ Python æª”æ¡ˆï¼Œè©•ä¼°å…¶ç”¨é€”ä¸¦æå‡ºæ•´ç†å»ºè­°ã€‚

åŸºæ–¼æˆ‘çš„åˆ†æï¼Œä»¥ä¸‹æ˜¯ `autogen_system/` ç›®éŒ„ä¸‹å„ Python æª”æ¡ˆçš„ç”¨é€”è©•ä¼°å’Œæ•´ç†å»ºè­°ï¼š

## ğŸ“ ç›®éŒ„çµæ§‹åˆ†æ

### ğŸ”§ **æ ¸å¿ƒåŠŸèƒ½æ¨¡çµ„** (ä¿ç•™)

#### `adapters/`
- **`llm_adapter.py`** âœ… **ä¿ç•™** - æ ¸å¿ƒé©é…å™¨ï¼Œå°‡ç¾æœ‰ LLM ç³»çµ±é©é…ç‚º AutoGen çš„ ChatCompletionClient

#### `agents/`
- **`agents_v3.py`** âœ… **ä¿ç•™** - V3 ç‰ˆæ™ºèƒ½é«”å¯¦ç¾ï¼ŒåŸºæ–¼ AutoGen AssistantAgent
- **`message_framework.py`** âœ… **ä¿ç•™** - è¨Šæ¯æ¡†æ¶ï¼Œå®šç¾©æ™ºèƒ½é«”é–“çš„é€šè¨Šæ ¼å¼

#### `config/`
- **`agent_config.py`** âœ… **ä¿ç•™** - æ™ºèƒ½é«”é…ç½®è³‡æ–™é¡åˆ¥
- **`config_loader.py`** âœ… **ä¿ç•™** - é…ç½®è¼‰å…¥å™¨ï¼Œæ”¯æ´ YAML å’Œç’°å¢ƒè®Šæ•¸

#### `tools/`
- **`tools_integration.py`** âœ… **ä¿ç•™** - å·¥å…·æ•´åˆæ¨¡çµ„ï¼Œç‚º AutoGen åŒ…è£ç¾æœ‰å·¥å…·

#### `workflow/`
- **`agent_selector.py`** âœ… **ä¿ç•™** - æ™ºèƒ½é«”é¸æ“‡å™¨ï¼Œå¯¦ç¾å·¥ä½œæµç¨‹é‚è¼¯

### ğŸ”„ **ç›¸å®¹æ€§å±¤** (éœ€é‡æ§‹)

#### `compatibility/`
- **`api_adapter.py`** âš ï¸ **é‡æ§‹** - API é©é…å™¨ï¼Œæœ‰ Mock é¡åˆ¥éœ€æ¸…ç†
- **`autogen_api_server.py`** âš ï¸ **é‡æ§‹** - API æœå‹™å™¨ï¼ŒMock é¡åˆ¥éå¤š
- **`langgraph_compatibility.py`** âš ï¸ **é‡æ§‹** - LangGraph ç›¸å®¹æ€§å±¤ï¼Œéœ€æ¸…ç† Mock
- **`response_mapper.py`** âœ… **ä¿ç•™** - éŸ¿æ‡‰æ˜ å°„å™¨ï¼ŒåŠŸèƒ½å®Œæ•´
- **`system_switcher.py`** âš ï¸ **é‡æ§‹** - ç³»çµ±åˆ‡æ›å™¨ï¼Œæœ‰ Mock é¡åˆ¥

### ğŸ“ **ç¯„ä¾‹å’Œæ¸¬è©¦** (å¯ç§»é™¤æˆ–æ•´åˆ)

#### æ ¹ç›®éŒ„
- **`selector_group_chat_example.py`** ğŸ”„ **ç§»å‹•** â†’ æ‡‰ç§»è‡³ `examples/` ç›®éŒ„
- **`test_selector_setup.py`** ğŸ”„ **ç§»å‹•** â†’ æ‡‰ç§»è‡³ `tests/` ç›®éŒ„

#### `compatibility/`
- **`example_usage.py`** ğŸ”„ **ç§»å‹•** â†’ ç§»è‡³ `examples/compatibility/`
- **`test_compatibility.py`** ğŸ”„ **ç§»å‹•** â†’ ç§»è‡³ `tests/compatibility/`

#### `workflow/examples/`
- **`mermaid_flow_test.py`** âœ… **ä¿ç•™** - Mermaid æµç¨‹æ¸¬è©¦
- **`selector_usage_example.py`** âœ… **ä¿ç•™** - é¸æ“‡å™¨ä½¿ç”¨ç¯„ä¾‹
- **`simple_test.py`** âŒ **å¯ç§»é™¤** - ç°¡å–®æ¸¬è©¦ï¼ŒåŠŸèƒ½é‡è¤‡
- **`standalone_selector_test.py`** âŒ **å¯ç§»é™¤** - ç¨ç«‹æ¸¬è©¦ï¼Œèˆ‡ä¸»æ¸¬è©¦é‡è¤‡

## ğŸš¨ **ä¸»è¦å•é¡Œ**

### 1. **Mock é¡åˆ¥éå¤š**
å¤šå€‹æª”æ¡ˆåŒ…å« `MockChatCompletionClient` ç­‰ Mock é¡åˆ¥ï¼š
- `api_adapter.py`
- `autogen_api_server.py` 
- `langgraph_compatibility.py`
- `system_switcher.py`
- `example_usage.py`
- `test_compatibility.py`

### 2. **æª”æ¡ˆä½ç½®ä¸ç•¶**
- ç¯„ä¾‹æª”æ¡ˆæ•£ä½ˆåœ¨æ ¹ç›®éŒ„
- æ¸¬è©¦æª”æ¡ˆæœªçµ±ä¸€ç®¡ç†

### 3. **åŠŸèƒ½é‡è¤‡**
- å¤šå€‹æ¸¬è©¦æª”æ¡ˆæ¸¬è©¦ç›¸ä¼¼åŠŸèƒ½
- é¸æ“‡å™¨æœ‰å¤šå€‹ç‰ˆæœ¬

## ğŸ’¡ **é‡æ§‹å»ºè­°**

### **éšæ®µ 1: æ¸…ç† Mock é¡åˆ¥**
```python
# å»ºè­°å‰µå»ºçµ±ä¸€çš„ Mock æ¨¡çµ„
# src/autogen_system/testing/mocks.py
class MockChatCompletionClient:
    """çµ±ä¸€çš„ Mock å¯¦ç¾"""
    pass
```

### **éšæ®µ 2: é‡çµ„ç›®éŒ„çµæ§‹**
```
src/autogen_system/
â”œâ”€â”€ core/                    # æ ¸å¿ƒåŠŸèƒ½
â”‚   â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ tools/
â”‚   â””â”€â”€ workflow/
â”œâ”€â”€ compatibility/           # ç›¸å®¹æ€§å±¤ï¼ˆæ¸…ç†å¾Œï¼‰
â”œâ”€â”€ examples/               # æ‰€æœ‰ç¯„ä¾‹
â”‚   â”œâ”€â”€ basic/
â”‚   â”œâ”€â”€ compatibility/
â”‚   â””â”€â”€ workflow/
â”œâ”€â”€ testing/                # æ¸¬è©¦å·¥å…·
â”‚   â”œâ”€â”€ mocks.py
â”‚   â””â”€â”€ fixtures.py
â””â”€â”€ tests/                  # å–®å…ƒæ¸¬è©¦
```

### **éšæ®µ 3: ä¾è³´æ¸…ç†**
- ç§»é™¤æœªä½¿ç”¨çš„å°å…¥
- çµ±ä¸€ logging ä½¿ç”¨
- æ¸…ç†å¾ªç’°ä¾è³´

```python
# å»ºè­°å‰µå»ºçµ±ä¸€çš„ Mock æ¨¡çµ„
# src/autogen_system/testing/mocks.py
class MockChatCompletionClient:
    """çµ±ä¸€çš„ Mock å¯¦ç¾"""
    pass
```

```plaintext
src/autogen_system/
â”œâ”€â”€ core/                    # æ ¸å¿ƒåŠŸèƒ½
â”‚   â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ tools/
â”‚   â””â”€â”€ workflow/
â”œâ”€â”€ compatibility/           # ç›¸å®¹æ€§å±¤ï¼ˆæ¸…ç†å¾Œï¼‰
â”œâ”€â”€ examples/               # æ‰€æœ‰ç¯„ä¾‹
â”‚   â”œâ”€â”€ basic/
â”‚   â”œâ”€â”€ compatibility/
â”‚   â””â”€â”€ workflow/
â”œâ”€â”€ testing/                # æ¸¬è©¦å·¥å…·
â”‚   â”œâ”€â”€ mocks.py
â”‚   â””â”€â”€ fixtures.py
â””â”€â”€ tests/                  # å–®å…ƒæ¸¬è©¦
```

## ğŸ“‹ **æ¸…ç†è¨ˆåŠƒ**

### **ğŸ”´ ç«‹å³å¯ç§»é™¤çš„æª”æ¡ˆ**
```bash
# é‡è¤‡çš„æ¸¬è©¦æª”æ¡ˆ
rm src/autogen_system/workflow/examples/simple_test.py
rm src/autogen_system/workflow/examples/standalone_selector_test.py
```

### **ğŸŸ¡ éœ€è¦ç§»å‹•çš„æª”æ¡ˆ**
```bash
# å‰µå»ºæ–°ç›®éŒ„çµæ§‹
mkdir -p src/autogen_system/examples/basic
mkdir -p src/autogen_system/examples/compatibility
mkdir -p src/autogen_system/testing
mkdir -p tests/autogen_system

# ç§»å‹•ç¯„ä¾‹æª”æ¡ˆ
mv src/autogen_system/selector_group_chat_example.py src/autogen_system/examples/basic/
mv src/autogen_system/compatibility/example_usage.py src/autogen_system/examples/compatibility/

# ç§»å‹•æ¸¬è©¦æª”æ¡ˆ
mv src/autogen_system/test_selector_setup.py tests/autogen_system/
mv src/autogen_system/compatibility/test_compatibility.py tests/autogen_system/
```

### **ğŸŸ  éœ€è¦é‡æ§‹çš„æª”æ¡ˆ**
1. **å‰µå»ºçµ±ä¸€ Mock æ¨¡çµ„**
2. **æ¸…ç† compatibility å±¤çš„ Mock é¡åˆ¥**
3. **æ›´æ–°å°å…¥è·¯å¾‘**

### **âœ… ä¿ç•™ä¸¦ç¶­è­·çš„æ ¸å¿ƒæª”æ¡ˆ**
- `adapters/llm_adapter.py` - æ ¸å¿ƒé©é…å™¨
- `agents/agents_v3.py` - V3 æ™ºèƒ½é«”
- `agents/message_framework.py` - è¨Šæ¯æ¡†æ¶
- `config/agent_config.py` - é…ç½®é¡åˆ¥
- `config/config_loader.py` - é…ç½®è¼‰å…¥å™¨
- `tools/tools_integration.py` - å·¥å…·æ•´åˆ
- `workflow/agent_selector.py` - æ™ºèƒ½é«”é¸æ“‡å™¨
- `compatibility/response_mapper.py` - éŸ¿æ‡‰æ˜ å°„å™¨

```shellscript
# é‡è¤‡çš„æ¸¬è©¦æª”æ¡ˆ
rm src/autogen_system/workflow/examples/simple_test.py
rm src/autogen_system/workflow/examples/standalone_selector_test.py
```

```shellscript
# å‰µå»ºæ–°ç›®éŒ„çµæ§‹
mkdir -p src/autogen_system/examples/basic
mkdir -p src/autogen_system/examples/compatibility
mkdir -p src/autogen_system/testing
mkdir -p tests/autogen_system

# ç§»å‹•ç¯„ä¾‹æª”æ¡ˆ
mv src/autogen_system/selector_group_chat_example.py src/autogen_system/examples/basic/
mv src/autogen_system/compatibility/example_usage.py src/autogen_system/examples/compatibility/

# ç§»å‹•æ¸¬è©¦æª”æ¡ˆ
mv src/autogen_system/test_selector_setup.py tests/autogen_system/
mv src/autogen_system/compatibility/test_compatibility.py tests/autogen_system/
```

```shellscript
# é‡è¤‡çš„æ¸¬è©¦æª”æ¡ˆ
rm src/autogen_system/workflow/examples/simple_test.py
rm src/autogen_system/workflow/examples/standalone_selector_test.py
```

```shellscript
# å‰µå»ºæ–°ç›®éŒ„çµæ§‹
mkdir -p src/autogen_system/examples/basic
mkdir -p src/autogen_system/examples/compatibility
mkdir -p src/autogen_system/testing
mkdir -p tests/autogen_system

# ç§»å‹•ç¯„ä¾‹æª”æ¡ˆ
mv src/autogen_system/selector_group_chat_example.py src/autogen_system/examples/basic/
mv src/autogen_system/compatibility/example_usage.py src/autogen_system/examples/compatibility/

# ç§»å‹•æ¸¬è©¦æª”æ¡ˆ
mv src/autogen_system/test_selector_setup.py tests/autogen_system/
mv src/autogen_system/compatibility/test_compatibility.py tests/autogen_system/
```

## ğŸ“Š **ç¸½çµ**

### **çµ±è¨ˆè³‡è¨Š**
- **ç¸½æª”æ¡ˆæ•¸**: 19 å€‹ Python æª”æ¡ˆ
- **å¯ç«‹å³ç§»é™¤**: 2 å€‹æª”æ¡ˆ (10.5%)
- **éœ€è¦ç§»å‹•**: 4 å€‹æª”æ¡ˆ (21.1%)
- **éœ€è¦é‡æ§‹**: 5 å€‹æª”æ¡ˆ (26.3%)
- **æ ¸å¿ƒä¿ç•™**: 8 å€‹æª”æ¡ˆ (42.1%)

### **å„ªå…ˆç´šå»ºè­°**

**ğŸ”´ é«˜å„ªå…ˆç´š** (ç«‹å³åŸ·è¡Œ)
1. ç§»é™¤é‡è¤‡æ¸¬è©¦æª”æ¡ˆ
2. å‰µå»ºçµ±ä¸€ Mock æ¨¡çµ„
3. ç§»å‹•ç¯„ä¾‹å’Œæ¸¬è©¦æª”æ¡ˆåˆ°æ­£ç¢ºä½ç½®

**ğŸŸ¡ ä¸­å„ªå…ˆç´š** (1-2é€±å…§)
1. é‡æ§‹ compatibility å±¤
2. æ¸…ç† Mock é¡åˆ¥
3. çµ±ä¸€å°å…¥è·¯å¾‘

**ğŸŸ¢ ä½å„ªå…ˆç´š** (æœ‰æ™‚é–“æ™‚)
1. å®Œå–„æ–‡æª”
2. æ·»åŠ é¡å‹è¨»è§£
3. å„ªåŒ–æ•ˆèƒ½

é€™æ¨£çš„æ•´ç†å°‡ä½¿ `autogen_system/` ç›®éŒ„æ›´åŠ æ¸…æ™°ã€ç¶­è­·æ€§æ›´å¥½ï¼Œä¸¦æ¸›å°‘ä»£ç¢¼é‡è¤‡ã€‚å»ºè­°æŒ‰å„ªå…ˆç´šé€æ­¥åŸ·è¡Œï¼Œç¢ºä¿ä¸å½±éŸ¿ç¾æœ‰åŠŸèƒ½ã€‚

---

**User**

1. ç§»é™¤é‡è¤‡æ¸¬è©¦æª”æ¡ˆ
2. ç§»å‹•ç¯„ä¾‹å’Œæ¸¬è©¦æª”æ¡ˆåˆ°æ­£ç¢ºä½ç½®
3. é‡æ§‹ compatibility å±¤
4. çµ±ä¸€å°å…¥è·¯å¾‘
5. ç§»é™¤ Mock æ¨¡çµ„

---

**Cursor**

æˆ‘å°‡æŒ‰ç…§æ‚¨çš„è¦æ±‚åŸ·è¡Œé€™äº›é‡æ§‹ä»»å‹™ã€‚è®“æˆ‘é€æ­¥é€²è¡Œæ¸…ç†å’Œé‡çµ„ã€‚

## 1. ç§»é™¤é‡è¤‡æ¸¬è©¦æª”æ¡ˆ

## 2. ç§»å‹•ç¯„ä¾‹å’Œæ¸¬è©¦æª”æ¡ˆåˆ°æ­£ç¢ºä½ç½®

é¦–å…ˆå‰µå»ºæ–°çš„ç›®éŒ„çµæ§‹ï¼š

ç§»å‹•ç¯„ä¾‹æª”æ¡ˆï¼š

```python
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
AutoGen SelectorGroupChat ç¯„ä¾‹ç¨‹å¼

åŸºæ–¼ AutoGen çš„ SelectorGroupChat å¯¦ç¾å¤šæ™ºèƒ½é«”å”ä½œå·¥ä½œæµç¨‹ï¼Œ
å–ä»£åŸæœ‰çš„ LangGraph æ¶æ§‹ï¼Œä½¿ç”¨ AutoGen åŸç”Ÿçš„è¨Šæ¯å‚³éæ©Ÿåˆ¶ã€‚
"""

import asyncio
import json
import os
import sys
from typing import Sequence, Dict, Any, Optional
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# AutoGen æ ¸å¿ƒå°å…¥
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, TextMessage
from autogen_agentchat.ui import Console

# å°ˆæ¡ˆå…§éƒ¨å°å…¥
from src.deerflow_logging import (
    init_thread_logging,
    get_thread_logger,
    set_thread_context,
)
from src.config import load_yaml_config

# å°å…¥é‡æ–°çµ„ç¹”å¾Œçš„æ¨¡çµ„
from src.autogen_system.agents.agents_v3 import (
    CoordinatorAgentV3,
    PlannerAgentV3,
    ResearcherAgentV3,
    CoderAgentV3,
    ReporterAgentV3,
    BackgroundInvestigatorAgentV3,
    HumanFeedbackerAgentV3,
)
from src.autogen_system.agents.message_framework import (
    ResearchWorkflowMessage,
    PlanMessage,
    ResearchResultMessage,
    CodeExecutionMessage,
    ReportMessage,
    parse_workflow_message,
)
from src.autogen_system.tools.tools_integration import initialize_all_tools
from src.autogen_system.workflow import create_selector_function, AgentSelector

# åˆå§‹åŒ– thread-safe æ—¥èªŒ
init_thread_logging()
# è¨­å®š thread contextï¼ˆé€™è£¡ä½¿ç”¨å›ºå®šçš„ thread_idï¼Œå¯¦éš›ä½¿ç”¨æ™‚æœƒå¾è«‹æ±‚ä¸­ç²å–ï¼‰
thread_id = "selector_group_chat_example"
set_thread_context(thread_id)
logger = get_thread_logger()  # ä½¿ç”¨ç•¶å‰ thread context

# è¨­å®š AutoGen å’Œå…¶ä»–ç¬¬ä¸‰æ–¹åº«çš„æ—¥èªŒç´šåˆ¥å’Œè™•ç†å™¨
import logging

autogen_logger = logging.getLogger("autogen_agentchat")
autogen_core_logger = logging.getLogger("autogen_core")

# å°‡ AutoGen çš„æ—¥èªŒä¹Ÿé‡å®šå‘åˆ°æˆ‘å€‘çš„æª”æ¡ˆ
from src.deerflow_logging.thread_logger import _manager

thread_logger_instance = _manager.get_logger(thread_id)
for handler in thread_logger_instance.handlers:
    if hasattr(handler, "baseFilename"):  # æª”æ¡ˆè™•ç†å™¨
        autogen_logger.addHandler(handler)
        autogen_core_logger.addHandler(handler)
        break

autogen_logger.setLevel(logging.INFO)
autogen_core_logger.setLevel(logging.INFO)


class WorkflowState:
    """å·¥ä½œæµç¨‹ç‹€æ…‹ç®¡ç†"""

    def __init__(self):
        self.current_plan: Optional[Dict[str, Any]] = None
        self.research_results: Dict[str, Any] = {}
        self.code_results: Dict[str, Any] = {}
        self.completed_steps: set = set()
        self.workflow_complete: bool = False
        self.error_messages: list = []

    def update_plan(self, plan: Dict[str, Any]):
        """æ›´æ–°åŸ·è¡Œè¨ˆåŠƒ"""
        self.current_plan = plan
        logger.info(f"å·¥ä½œæµç¨‹è¨ˆåŠƒå·²æ›´æ–°: {len(plan.get('steps', []))} å€‹æ­¥é©Ÿ")

    def mark_step_complete(self, step_id: str, result: Any):
        """æ¨™è¨˜æ­¥é©Ÿå®Œæˆ"""
        self.completed_steps.add(step_id)
        logger.info(f"æ­¥é©Ÿ {step_id} å·²å®Œæˆ")

    def is_workflow_complete(self) -> bool:
        """æª¢æŸ¥å·¥ä½œæµç¨‹æ˜¯å¦å®Œæˆ"""
        if not self.current_plan:
            return False

        total_steps = len(self.current_plan.get("steps", []))
        completed_count = len(self.completed_steps)

        return completed_count >= total_steps or self.workflow_complete

    def get_next_step(self) -> Optional[Dict[str, Any]]:
        """ç²å–ä¸‹ä¸€å€‹å¾…åŸ·è¡Œçš„æ­¥é©Ÿ"""
        if not self.current_plan:
            return None

        for step in self.current_plan.get("steps", []):
            step_id = step.get("id", str(step.get("step_type", "")))
            if step_id not in self.completed_steps:
                return step

        return None


# å‰µå»ºå…¨å±€é¸æ“‡å™¨å¯¦ä¾‹
_global_selector = None


def get_selector_func(selector_type: str = "basic", **kwargs):
    """
    ç²å–é¸æ“‡å™¨å‡½æ•¸

    Args:
        selector_type: é¸æ“‡å™¨é¡å‹ ("basic" æˆ– "advanced")
        **kwargs: é¸æ“‡å™¨åˆå§‹åŒ–åƒæ•¸

    Returns:
        callable: é¸æ“‡å™¨å‡½æ•¸
    """
    global _global_selector

    if _global_selector is None:
        _global_selector = create_selector_function(
            selector_type=selector_type, enable_debug=True, **kwargs
        )

    return _global_selector


def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:
    """
    æ™ºèƒ½é«”é¸æ“‡å‡½æ•¸ï¼ˆé‡æ§‹ç‰ˆæœ¬ï¼‰

    ä½¿ç”¨æ–°çš„ AgentSelector é¡ä¾†æ±ºå®šä¸‹ä¸€å€‹æ‡‰è©²ç™¼è¨€çš„æ™ºèƒ½é«”ã€‚
    ä¿æŒèˆ‡åŸå§‹å‡½æ•¸ç›¸åŒçš„ä»‹é¢ä»¥ç¢ºä¿å‘å¾Œå…¼å®¹æ€§ã€‚

    Args:
        messages: å°è©±æ­·å²è¨Šæ¯

    Returns:
        str | None: ä¸‹ä¸€å€‹æ™ºèƒ½é«”çš„åç¨±ï¼Œæˆ– None è®“æ¨¡å‹è‡ªå‹•é¸æ“‡
    """
    try:
        # ç²å–é¸æ“‡å™¨å‡½æ•¸
        selector = get_selector_func()
        return selector(messages)
    except Exception as e:
        logger.error(f"Selector å‡½æ•¸åŸ·è¡ŒéŒ¯èª¤: {e}")
        return None


async def create_agents(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    å‰µå»ºæ‰€æœ‰æ™ºèƒ½é«”

    Args:
        config: é…ç½®å­—å…¸

    Returns:
        Dict[str, Any]: æ™ºèƒ½é«”å­—å…¸
    """
    logger.info("é–‹å§‹å‰µå»ºæ™ºèƒ½é«”...")

    # å…ˆåˆå§‹åŒ–å·¥å…·
    logger.info("åˆå§‹åŒ–å·¥å…·ç³»çµ±...")
    await initialize_all_tools()

    # å‰µå»ºæ™ºèƒ½é«”å¯¦ä¾‹
    coordinator = await CoordinatorAgentV3.create(config)
    planner = await PlannerAgentV3.create(config)
    researcher = await ResearcherAgentV3.create(config)
    coder = await CoderAgentV3.create(config)
    reporter = await ReporterAgentV3.create(config)
    background_investigator = await BackgroundInvestigatorAgentV3.create(config)
    human_feedbacker = await HumanFeedbackerAgentV3.create(config)

    agents = {
        "coordinator": coordinator,
        "planner": planner,
        "researcher": researcher,
        "coder": coder,
        "reporter": reporter,
        "background_investigator": background_investigator,
        "human_feedbacker": human_feedbacker,
    }

    logger.info(f"æ™ºèƒ½é«”å‰µå»ºå®Œæˆï¼Œå…± {len(agents)} å€‹")
    return agents


async def run_workflow_example(task: str, config_path: str = "conf_autogen.yaml"):
    """
    åŸ·è¡Œå·¥ä½œæµç¨‹ç¯„ä¾‹

    Args:
        task: è¦åŸ·è¡Œçš„ä»»å‹™æè¿°
        config_path: é…ç½®æª”æ¡ˆè·¯å¾‘
    """
    logger.info(f"ğŸš€ é–‹å§‹åŸ·è¡Œ AutoGen SelectorGroupChat å·¥ä½œæµç¨‹")
    logger.info(f"ğŸ“‹ ä»»å‹™: {task}")

    try:
        # è¼‰å…¥é…ç½®
        config = load_yaml_config(config_path)
        logger.info("âœ… é…ç½®è¼‰å…¥æˆåŠŸ")

        # å‰µå»ºæ™ºèƒ½é«”
        agents = await create_agents(config)

        # å‰µå»ºæ™ºèƒ½é«”åˆ—è¡¨ï¼ˆä½¿ç”¨åº•å±¤çš„ AssistantAgentï¼‰
        agent_list = [
            agents["coordinator"].get_agent(),  # ç²å–åº•å±¤çš„ AssistantAgent
            agents["planner"].get_agent(),
            agents["researcher"].get_agent(),
            agents["coder"].get_agent(),
            agents["reporter"].get_agent(),
            agents["background_investigator"].get_agent(),
            agents["human_feedbacker"].get_agent(),
        ]

        # å‰µå»ºçµ‚æ­¢æ¢ä»¶
        termination = TextMentionTermination("WORKFLOW_COMPLETE")

        # ç²å–æ¨¡å‹å®¢æˆ¶ç«¯ï¼ˆä½¿ç”¨å”èª¿è€…çš„æ¨¡å‹ï¼‰
        model_client = agents["coordinator"]._model_client

        # ç²å–é¸æ“‡å™¨å‡½æ•¸ï¼ˆå¯ä»¥é¸æ“‡ "basic" æˆ– "advanced"ï¼‰
        selector_function = get_selector_func(selector_type="basic", max_turns=50)

        # å‰µå»º SelectorGroupChat
        # æ³¨æ„ï¼šåƒæ•¸åç¨±å¯èƒ½å› ç‰ˆæœ¬è€Œç•°ï¼Œå˜—è©¦ä¸åŒçš„åƒæ•¸åç¨±
        try:
            team = SelectorGroupChat(
                participants=agent_list,  # å˜—è©¦ participants åƒæ•¸
                model_client=model_client,
                termination_condition=termination,
                selector_func=selector_function,
                max_turns=50,
            )
        except TypeError:
            # å¦‚æœ participants ä¸å°ï¼Œå˜—è©¦å…¶ä»–åƒæ•¸åç¨±
            try:
                team = SelectorGroupChat(
                    agent_list,  # å˜—è©¦ä½ç½®åƒæ•¸
                    model_client=model_client,
                    termination_condition=termination,
                    selector_func=selector_function,
                    max_turns=50,
                )
            except TypeError:
                # æœ€å¾Œå˜—è©¦æœ€ç°¡åŒ–çš„åˆå§‹åŒ–
                team = SelectorGroupChat(
                    participants=agent_list,
                    selector_func=selector_function,
                )

        logger.info("âœ… SelectorGroupChat å‰µå»ºæˆåŠŸ")

        # åŸ·è¡Œå·¥ä½œæµç¨‹
        logger.info("ğŸ¯ é–‹å§‹åŸ·è¡Œä»»å‹™...")
        await Console(team.run_stream(task=task))

        logger.info("ğŸ‰ å·¥ä½œæµç¨‹åŸ·è¡Œå®Œæˆ")

    except Exception as e:
        logger.error(f"âŒ å·¥ä½œæµç¨‹åŸ·è¡Œå¤±æ•—: {e}")
        raise


async def main():
    """ä¸»å‡½æ•¸"""
    # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
    if not os.getenv("AZURE_OPENAI_ENDPOINT"):
        logger.error("âŒ è«‹è¨­å®š AZURE_OPENAI_ENDPOINT ç’°å¢ƒè®Šæ•¸")
        return

    # ç¯„ä¾‹ä»»å‹™
    # task = """
    # è«‹ç ”ç©¶äººå·¥æ™ºæ…§åœ¨æ•™è‚²é ˜åŸŸçš„æœ€æ–°æ‡‰ç”¨ï¼ŒåŒ…æ‹¬ï¼š
    # 1. æœå°‹ç›¸é—œçš„æœ€æ–°ç ”ç©¶è«–æ–‡å’ŒæŠ€è¡“å ±å‘Š
    # 2. åˆ†æä¸»è¦çš„æ‡‰ç”¨å ´æ™¯å’ŒæŠ€è¡“ç‰¹é»
    # 3. æ•´ç†ç›¸é—œæ•¸æ“šä¸¦é€²è¡Œç°¡å–®çš„çµ±è¨ˆåˆ†æ
    # 4. ç”Ÿæˆä¸€ä»½è©³ç´°çš„ç ”ç©¶å ±å‘Š
    # """
    task = "è«‹ç ”ç©¶äººå·¥æ™ºæ…§åœ¨æ•™è‚²é ˜åŸŸçš„æœ€æ–°æ‡‰ç”¨"

    # åŸ·è¡Œå·¥ä½œæµç¨‹
    await run_workflow_example(task)


if __name__ == "__main__":
    # ç¢ºä¿æ—¥èªŒç›®éŒ„å­˜åœ¨
    os.makedirs("logs", exist_ok=True)

    # åŸ·è¡Œä¸»å‡½æ•¸
    asyncio.run(main())
```

ç§»å‹•æ¸¬è©¦æª”æ¡ˆï¼š

```python
#!/usr/bin/env python3
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
æ¸¬è©¦ AutoGen SelectorGroupChat è¨­ç½®

å¿«é€Ÿé©—è­‰ç’°å¢ƒè¨­ç½®å’Œä¾è³´æ˜¯å¦æ­£ç¢ºã€‚
"""

import sys
import os
from pathlib import Path

# ç¢ºä¿å°ˆæ¡ˆæ ¹ç›®éŒ„åœ¨è·¯å¾‘ä¸­
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


def test_imports():
    """æ¸¬è©¦å°å…¥"""
    print("ğŸ§ª æ¸¬è©¦å°å…¥...")

    try:
        # æ¸¬è©¦åŸºç¤å°å…¥
        from src.deerflow_logging import get_simple_logger as get_logger

        print("âœ… åŸºç¤æ—¥èªŒæ¨¡çµ„å°å…¥æˆåŠŸ")

        from src.config import load_yaml_config

        print("âœ… é…ç½®æ¨¡çµ„å°å…¥æˆåŠŸ")

        from src.config.agents import AGENT_LLM_MAP

        print("âœ… Agent é…ç½®å°å…¥æˆåŠŸ")

        # æ¸¬è©¦ AutoGen å°å…¥
        from autogen_agentchat.teams import SelectorGroupChat
        from autogen_agentchat.agents import AssistantAgent

        print("âœ… AutoGen æ¨¡çµ„å°å…¥æˆåŠŸ")

        # æ¸¬è©¦ç¯„ä¾‹æ¨¡çµ„å°å…¥
        from src.autogen_system.agents.message_framework import MessageType

        print("âœ… è¨Šæ¯æ¡†æ¶å°å…¥æˆåŠŸ")

        from src.autogen_system.tools.tools_integration import initialize_all_tools

        print("âœ… å·¥å…·æ•´åˆæ¨¡çµ„å°å…¥æˆåŠŸ")

        from src.autogen_system.agents.agents_v3 import CoordinatorAgentV3

        print("âœ… V3 æ™ºèƒ½é«”æ¨¡çµ„å°å…¥æˆåŠŸ")

        return True

    except ImportError as e:
        print(f"âŒ å°å…¥å¤±æ•—: {e}")
        return False


def test_environment():
    """æ¸¬è©¦ç’°å¢ƒè®Šæ•¸"""
    print("\nğŸ”§ æ¸¬è©¦ç’°å¢ƒè®Šæ•¸...")

    required_vars = [
        "AZURE_OPENAI_ENDPOINT",
        "BASIC_MODEL__API_KEY",
        "REASONING_MODEL__API_KEY",
    ]

    missing = []
    for var in required_vars:
        if os.getenv(var):
            print(f"âœ… {var}: {'*' * 20}")  # éš±è—å¯¦éš›å€¼
        else:
            print(f"âŒ {var}: æœªè¨­å®š")
            missing.append(var)

    optional_vars = [
        "AZURE_DEPLOYMENT_NAME_4_1_MINI",
        "AZURE_DEPLOYMENT_NAME_4_1",
        "BASIC_MODEL__API_VERSION",
    ]

    print("\nğŸ“‹ å¯é¸ç’°å¢ƒè®Šæ•¸:")
    for var in optional_vars:
        value = os.getenv(var, "æœªè¨­å®š")
        if value != "æœªè¨­å®š":
            print(f"âœ… {var}: {value}")
        else:
            print(f"âš ï¸ {var}: æœªè¨­å®šï¼ˆå°‡ä½¿ç”¨é è¨­å€¼ï¼‰")

    return len(missing) == 0


def test_config_file():
    """æ¸¬è©¦é…ç½®æª”æ¡ˆ"""
    print("\nğŸ“„ æ¸¬è©¦é…ç½®æª”æ¡ˆ...")

    config_path = project_root / "conf_autogen.yaml"
    if not config_path.exists():
        print(f"âŒ é…ç½®æª”æ¡ˆä¸å­˜åœ¨: {config_path}")
        return False

    print(f"âœ… é…ç½®æª”æ¡ˆå­˜åœ¨: {config_path}")

    try:
        from src.config import load_yaml_config

        config = load_yaml_config("conf_autogen.yaml")

        # æª¢æŸ¥ V3 æ™ºèƒ½é«”é…ç½®
        agents = config.get("agents", {})
        v3_agents = ["coordinator_v3", "planner_v3", "researcher_v3", "coder_v3", "reporter_v3"]

        for agent in v3_agents:
            if agent in agents:
                print(f"âœ… {agent} é…ç½®å­˜åœ¨")
            else:
                print(f"âŒ {agent} é…ç½®ç¼ºå¤±")
                return False

        # æª¢æŸ¥å·¥ä½œæµç¨‹é…ç½®
        workflows = config.get("workflows", {})
        if "research_v3" in workflows:
            print("âœ… research_v3 å·¥ä½œæµç¨‹é…ç½®å­˜åœ¨")
        else:
            print("âŒ research_v3 å·¥ä½œæµç¨‹é…ç½®ç¼ºå¤±")
            return False

        return True

    except Exception as e:
        print(f"âŒ é…ç½®æª”æ¡ˆè®€å–å¤±æ•—: {e}")
        return False


def test_llm_mapping():
    """æ¸¬è©¦ LLM æ˜ å°„"""
    print("\nğŸ§  æ¸¬è©¦ LLM æ˜ å°„...")

    try:
        from src.config.agents import AGENT_LLM_MAP

        v3_agents = [
            "CoordinatorAgentV3",
            "PlannerAgentV3",
            "ResearcherAgentV3",
            "CoderAgentV3",
            "ReporterAgentV3",
        ]

        for agent in v3_agents:
            if agent in AGENT_LLM_MAP:
                llm_type = AGENT_LLM_MAP[agent]
                print(f"âœ… {agent}: {llm_type}")
            else:
                print(f"âŒ {agent}: æ˜ å°„ç¼ºå¤±")
                return False

        return True

    except Exception as e:
        print(f"âŒ LLM æ˜ å°„æ¸¬è©¦å¤±æ•—: {e}")
        return False


async def test_tools():
    """æ¸¬è©¦å·¥å…·åˆå§‹åŒ–"""
    print("\nğŸ› ï¸ æ¸¬è©¦å·¥å…·åˆå§‹åŒ–...")

    try:
        from src.autogen_system.tools.tools_integration import initialize_all_tools

        tools = await initialize_all_tools()
        print(f"âœ… å·¥å…·åˆå§‹åŒ–æˆåŠŸï¼Œå…± {len(tools)} å€‹å·¥å…·")

        # æª¢æŸ¥é‡è¦å·¥å…·
        important_tools = ["web_search", "python_repl", "crawl_website"]
        for tool_name in important_tools:
            if tool_name in tools:
                print(f"âœ… {tool_name} å·¥å…·å¯ç”¨")
            else:
                print(f"âš ï¸ {tool_name} å·¥å…·ä¸å¯ç”¨")

        return True

    except Exception as e:
        print(f"âŒ å·¥å…·åˆå§‹åŒ–å¤±æ•—: {e}")
        return False


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ AutoGen SelectorGroupChat è¨­ç½®æ¸¬è©¦")
    print("=" * 50)

    tests = [
        ("å°å…¥æ¸¬è©¦", test_imports),
        ("ç’°å¢ƒè®Šæ•¸æ¸¬è©¦", test_environment),
        ("é…ç½®æª”æ¡ˆæ¸¬è©¦", test_config_file),
        ("LLM æ˜ å°„æ¸¬è©¦", test_llm_mapping),
    ]

    results = []

    # åŸ·è¡ŒåŒæ­¥æ¸¬è©¦
    for name, test_func in tests:
        try:
            result = test_func()
            results.append((name, result))
        except Exception as e:
            print(f"âŒ {name} åŸ·è¡Œå¤±æ•—: {e}")
            results.append((name, False))

    # åŸ·è¡Œç•°æ­¥æ¸¬è©¦
    try:
        print("\n")
        tools_result = await test_tools()
        results.append(("å·¥å…·åˆå§‹åŒ–æ¸¬è©¦", tools_result))
    except Exception as e:
        print(f"âŒ å·¥å…·åˆå§‹åŒ–æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        results.append(("å·¥å…·åˆå§‹åŒ–æ¸¬è©¦", False))

    # ç¸½çµçµæœ
    print("\n" + "=" * 50)
    print("ğŸ“Š æ¸¬è©¦çµæœç¸½çµ:")

    passed = 0
    total = len(results)

    for name, result in results:
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        print(f"{name}: {status}")
        if result:
            passed += 1

    print(f"\nç¸½è¨ˆ: {passed}/{total} æ¸¬è©¦é€šé")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼å¯ä»¥åŸ·è¡Œ SelectorGroupChat ç¯„ä¾‹ã€‚")
        print("\nåŸ·è¡ŒæŒ‡ä»¤:")
        print("python -m src.autogen_system.examples.basic.selector_group_chat_example")
    else:
        print("âš ï¸ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ä¸Šè¿°å•é¡Œå¾Œé‡è©¦ã€‚")


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())
```

ç§»å‹• compatibility å±¤çš„ç¯„ä¾‹æª”æ¡ˆï¼š

```python
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
API ç›¸å®¹æ€§å±¤ä½¿ç”¨ç¯„ä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨ AutoGen API ç›¸å®¹æ€§å±¤ã€‚
"""

import asyncio
from typing import Dict, Any

from src.deerflow_logging import get_simple_logger as get_logger
from src.config.report_style import ReportStyle
from src.autogen_system.compatibility import (
    AutoGenAPIAdapter,
    LangGraphCompatibilityLayer,
    ResponseMapper,
    StreamResponseMapper,
    run_compatibility_tests,
)

logger = get_logger(__name__)


async def example_api_adapter_usage():
    """API é©é…å™¨ä½¿ç”¨ç¯„ä¾‹"""
    print("=== API é©é…å™¨ä½¿ç”¨ç¯„ä¾‹ ===")

    # å‰µå»ºæ¨¡å‹å®¢æˆ¶ç«¯
    from src.autogen_system.adapters.llm_adapter import create_chat_client
    
    model_client = create_chat_client("coordinator")

    # å‰µå»º API é©é…å™¨
    adapter = AutoGenAPIAdapter(model_client)

    # æ¨¡æ“¬èŠå¤©è«‹æ±‚
    messages = [{"role": "user", "content": "æ¸¬è©¦ AutoGen API é©é…å™¨"}]

    print("é–‹å§‹è™•ç†èŠå¤©è«‹æ±‚...")

    try:
        # è™•ç†è«‹æ±‚ä¸¦æ”¶é›†äº‹ä»¶
        events = []
        async for event in adapter.process_chat_request(
            messages=messages,
            thread_id="example_thread",
            max_plan_iterations=1,
            max_step_num=2,
            auto_accepted_plan=True,
            enable_background_investigation=False,
            report_style=ReportStyle.ACADEMIC,
        ):
            events.append(event)
            print(
                f"äº‹ä»¶: {event.get('event')} - {event.get('data', {}).get('content', '')[:50]}..."
            )

            # é™åˆ¶äº‹ä»¶æ•¸é‡
            if len(events) >= 5:
                break

        print(f"âœ… æˆåŠŸè™•ç† {len(events)} å€‹äº‹ä»¶")

    except Exception as e:
        print(f"âŒ è™•ç†å¤±æ•—: {e}")


async def example_langgraph_compatibility():
    """LangGraph ç›¸å®¹æ€§ç¯„ä¾‹"""
    print("\n=== LangGraph ç›¸å®¹æ€§ç¯„ä¾‹ ===")

    # å‰µå»ºæ¨¡å‹å®¢æˆ¶ç«¯
    from src.autogen_system.adapters.llm_adapter import create_chat_client
    
    model_client = create_chat_client("coordinator")

    # å‰µå»ºç›¸å®¹æ€§å±¤
    compatibility_layer = LangGraphCompatibilityLayer(model_client)

    # æ¨¡æ“¬ LangGraph è¼¸å…¥æ ¼å¼
    input_data = {"messages": [{"role": "user", "content": "æ¸¬è©¦ LangGraph ç›¸å®¹æ€§"}]}

    config = {"thread_id": "langgraph_test", "max_plan_iterations": 1, "auto_accepted_plan": True}

    try:
        print("æ¸¬è©¦ ainvoke æ–¹æ³•...")
        result = await compatibility_layer.ainvoke(input_data, config)

        print(f"âœ… ainvoke æˆåŠŸ")
        print(f"   - è¨Šæ¯æ•¸é‡: {len(result.get('messages', []))}")
        print(f"   - æœ‰æœ€çµ‚å ±å‘Š: {'final_report' in result}")
        print(f"   - åŸ·è¡Œç‹€æ…‹: {result.get('execution_metadata', {}).get('success', 'unknown')}")

        print("\næ¸¬è©¦ astream æ–¹æ³•...")
        events = []
        async for event in compatibility_layer.astream(input_data, config):
            events.append(event)
            print(f"   äº‹ä»¶: {type(event).__name__}")

            # é™åˆ¶äº‹ä»¶æ•¸é‡
            if len(events) >= 3:
                break

        print(f"âœ… astream æˆåŠŸï¼Œæ”¶åˆ° {len(events)} å€‹äº‹ä»¶")

    except Exception as e:
        print(f"âŒ LangGraph ç›¸å®¹æ€§æ¸¬è©¦å¤±æ•—: {e}")


async def example_response_mapping():
    """éŸ¿æ‡‰æ˜ å°„ç¯„ä¾‹"""
    print("\n=== éŸ¿æ‡‰æ˜ å°„ç¯„ä¾‹ ===")

    # æ¨¡æ“¬ AutoGen åŸ·è¡Œçµæœ
    autogen_result = {
        "success": True,
        "research_topic": "AI åœ¨é†«ç™‚é ˜åŸŸçš„æ‡‰ç”¨",
        "final_report": "äººå·¥æ™ºæ…§åœ¨é†«ç™‚é ˜åŸŸå±•ç¾å‡ºå·¨å¤§æ½›åŠ›...",
        "execution_time": 15.7,
        "workflow_plan": {
            "id": "plan_123",
            "name": "é†«ç™‚ AI ç ”ç©¶è¨ˆåŠƒ",
            "description": "æ·±å…¥åˆ†æ AI é†«ç™‚æ‡‰ç”¨",
            "steps": [
                {
                    "step_id": "research_step",
                    "step_type": "research",
                    "description": "æ”¶é›† AI é†«ç™‚ç›¸é—œè³‡æ–™",
                    "timeout_seconds": 120,
                },
                {
                    "step_id": "analysis_step",
                    "step_type": "analysis",
                    "description": "åˆ†ææ”¶é›†åˆ°çš„è³‡æ–™",
                    "timeout_seconds": 180,
                },
            ],
        },
        "session_id": "session_456",
        "timestamp": "2025-01-08T16:00:00Z",
    }

    try:
        # æ˜ å°„åŸ·è¡Œçµæœ
        mapped_result = ResponseMapper.map_execution_result(autogen_result)

        print("âœ… åŸ·è¡Œçµæœæ˜ å°„æˆåŠŸ")
        print(f"   - æˆåŠŸç‹€æ…‹: {mapped_result.get('success')}")
        print(f"   - ç ”ç©¶ä¸»é¡Œ: {mapped_result.get('research_topic')}")
        print(f"   - åŸ·è¡Œæ™‚é–“: {mapped_result.get('execution_time')} ç§’")
        print(f"   - æœ‰å…ƒæ•¸æ“š: {'execution_metadata' in mapped_result}")

        # æ˜ å°„è¨ˆåŠƒæ•¸æ“š
        mapped_plan = ResponseMapper.map_plan_data(autogen_result["workflow_plan"])

        print(f"\nâœ… è¨ˆåŠƒæ˜ å°„æˆåŠŸ")
        print(f"   - è¨ˆåŠƒåç¨±: {mapped_plan.get('name')}")
        print(f"   - æ­¥é©Ÿæ•¸é‡: {len(mapped_plan.get('steps', []))}")
        print(f"   - é ä¼°æ™‚é–“: {mapped_plan.get('estimated_time')} åˆ†é˜")

        # æ¸¬è©¦æµå¼éŸ¿æ‡‰æ˜ å°„
        print(f"\næ¸¬è©¦æµå¼éŸ¿æ‡‰æ˜ å°„...")

        async def mock_stream():
            """æ¨¡æ“¬ AutoGen æµå¼äº‹ä»¶"""
            events = [
                {
                    "event": "message_chunk",
                    "data": {
                        "thread_id": "test",
                        "agent": "researcher",
                        "content": "é–‹å§‹ç ”ç©¶...",
                        "id": "msg_1",
                    },
                },
                {
                    "event": "message_chunk",
                    "data": {
                        "thread_id": "test",
                        "agent": "reporter",
                        "content": "ç ”ç©¶å®Œæˆ",
                        "finish_reason": "stop",
                        "id": "msg_2",
                    },
                },
            ]

            for event in events:
                yield event

        sse_events = []
        async for sse_event in StreamResponseMapper.map_stream_events(mock_stream()):
            sse_events.append(sse_event)

        print(f"âœ… æµå¼æ˜ å°„æˆåŠŸï¼Œç”Ÿæˆ {len(sse_events)} å€‹ SSE äº‹ä»¶")

    except Exception as e:
        print(f"âŒ éŸ¿æ‡‰æ˜ å°„æ¸¬è©¦å¤±æ•—: {e}")


async def example_compatibility_testing():
    """ç›¸å®¹æ€§æ¸¬è©¦ç¯„ä¾‹"""
    print("\n=== ç›¸å®¹æ€§æ¸¬è©¦ç¯„ä¾‹ ===")

    try:
        print("é‹è¡Œå®Œæ•´ç›¸å®¹æ€§æ¸¬è©¦...")
        results = await run_compatibility_tests()

        print(f"\næ¸¬è©¦çµæœ:")
        print(f"   - ç¸½æ¸¬è©¦: {results['total_tests']}")
        print(f"   - é€šé: {results['passed']}")
        print(f"   - å¤±æ•—: {results['failed']}")
        print(f"   - æˆåŠŸç‡: {results['success_rate']:.1f}%")

        if results["success_rate"] >= 80:
            print("âœ… ç›¸å®¹æ€§æ¸¬è©¦å¤§éƒ¨åˆ†é€šé")
        else:
            print("âŒ ç›¸å®¹æ€§æ¸¬è©¦å­˜åœ¨å•é¡Œ")

        # é¡¯ç¤ºè©³ç´°çµæœ
        print(f"\nè©³ç´°æ¸¬è©¦çµæœ:")
        for test_name, result in results["results"].items():
            status = "âœ…" if result.get("passed", False) else "âŒ"
            print(f"   {status} {test_name}")
            if not result.get("passed", False) and "error" in result:
                print(f"      éŒ¯èª¤: {result['error']}")

    except Exception as e:
        print(f"âŒ ç›¸å®¹æ€§æ¸¬è©¦å¤±æ•—: {e}")


async def main():
    """ä¸»å‡½æ•¸ - é‹è¡Œæ‰€æœ‰ç¯„ä¾‹"""
    print("AutoGen API ç›¸å®¹æ€§å±¤ä½¿ç”¨ç¯„ä¾‹")
    print("=" * 50)

    try:
        await example_api_adapter_usage()
        await example_langgraph_compatibility()
        await example_response_mapping()
        await example_compatibility_testing()

        print("\n" + "=" * 50)
        print("âœ… æ‰€æœ‰ç¯„ä¾‹åŸ·è¡Œå®Œæˆ")
        print("\nğŸ“š ä½¿ç”¨æŒ‡å—:")
        print("1. API é©é…å™¨: è™•ç†æ¨™æº–çš„èŠå¤©è«‹æ±‚")
        print("2. LangGraph ç›¸å®¹æ€§: èˆ‡ç¾æœ‰ LangGraph ä»£ç¢¼ç„¡ç¸«æ•´åˆ")
        print("3. éŸ¿æ‡‰æ˜ å°„: ç¢ºä¿å‰ç«¯æ¥æ”¶æ­£ç¢ºæ ¼å¼çš„æ•¸æ“š")
        print("4. ç›¸å®¹æ€§æ¸¬è©¦: é©—è­‰ç³»çµ±é›†æˆç‹€æ…‹")

    except Exception as e:
        print(f"\nâŒ ç¯„ä¾‹åŸ·è¡Œå¤±æ•—: {e}")


if __name__ == "__main__":
    # é‹è¡Œç¯„ä¾‹
    asyncio.run(main())
```

ç§»å‹•æ¸¬è©¦æª”æ¡ˆï¼š

```python
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
API ç›¸å®¹æ€§æ¸¬è©¦

é©—è­‰ AutoGen ç³»çµ±èˆ‡ç¾æœ‰ API çš„ç›¸å®¹æ€§ã€‚
"""

import asyncio
import json
from typing import Dict, Any, List
from datetime import datetime

from src.deerflow_logging import get_simple_logger as get_logger
from src.config.report_style import ReportStyle
from src.autogen_system.compatibility.api_adapter import AutoGenAPIAdapter
from src.autogen_system.compatibility.langgraph_compatibility import LangGraphCompatibilityLayer
from src.autogen_system.compatibility.response_mapper import ResponseMapper, StreamResponseMapper

logger = get_logger(__name__)


class CompatibilityTester:
    """
    ç›¸å®¹æ€§æ¸¬è©¦å™¨

    æ¸¬è©¦ AutoGen ç³»çµ±èˆ‡ç¾æœ‰ API çš„ç›¸å®¹æ€§ã€‚
    """

    def __init__(self):
        """åˆå§‹åŒ–æ¸¬è©¦å™¨"""
        # å‰µå»ºæ¨¡å‹å®¢æˆ¶ç«¯
        from src.autogen_system.adapters.llm_adapter import create_chat_client
        
        self.model_client = create_chat_client("coordinator")

        self.api_adapter = AutoGenAPIAdapter(self.model_client)
        self.compatibility_layer = LangGraphCompatibilityLayer(self.model_client)

        self.test_results = []

        logger.info("ç›¸å®¹æ€§æ¸¬è©¦å™¨åˆå§‹åŒ–å®Œæˆ")

    async def run_all_tests(self) -> Dict[str, Any]:
        """é‹è¡Œæ‰€æœ‰ç›¸å®¹æ€§æ¸¬è©¦"""
        logger.info("é–‹å§‹é‹è¡Œç›¸å®¹æ€§æ¸¬è©¦")

        test_suite = [
            ("API é©é…å™¨æ¸¬è©¦", self.test_api_adapter),
            ("LangGraph ç›¸å®¹æ€§æ¸¬è©¦", self.test_langgraph_compatibility),
            ("éŸ¿æ‡‰æ˜ å°„æ¸¬è©¦", self.test_response_mapping),
            ("æµå¼éŸ¿æ‡‰æ¸¬è©¦", self.test_stream_response),
            ("è«‹æ±‚æ ¼å¼ç›¸å®¹æ€§æ¸¬è©¦", self.test_request_format_compatibility),
            ("éŒ¯èª¤è™•ç†æ¸¬è©¦", self.test_error_handling),
        ]

        results = {}
        passed = 0
        total = len(test_suite)

        for test_name, test_func in test_suite:
            try:
                logger.info(f"é‹è¡Œæ¸¬è©¦: {test_name}")
                result = await test_func()
                results[test_name] = result

                if result.get("passed", False):
                    passed += 1
                    logger.info(f"âœ… {test_name} - é€šé")
                else:
                    logger.warning(f"âŒ {test_name} - å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")

            except Exception as e:
                logger.error(f"âŒ {test_name} - ç•°å¸¸: {e}")
                results[test_name] = {"passed": False, "error": str(e), "exception": True}

        summary = {
            "total_tests": total,
            "passed": passed,
            "failed": total - passed,
            "success_rate": (passed / total) * 100 if total > 0 else 0,
            "results": results,
            "timestamp": datetime.now().isoformat(),
        }

        logger.info(f"ç›¸å®¹æ€§æ¸¬è©¦å®Œæˆ - é€šéç‡: {summary['success_rate']:.1f}%")
        return summary

    async def test_api_adapter(self) -> Dict[str, Any]:
        """æ¸¬è©¦ API é©é…å™¨"""
        try:
            # æ¨¡æ“¬èŠå¤©è«‹æ±‚
            messages = [{"role": "user", "content": "æ¸¬è©¦ AutoGen API é©é…å™¨"}]

            # æ”¶é›†äº‹ä»¶
            events = []
            async for event in self.api_adapter.process_chat_request(
                messages=messages,
                thread_id="test_thread",
                max_plan_iterations=1,
                max_step_num=2,
                auto_accepted_plan=True,
                enable_background_investigation=False,
            ):
                events.append(event)
                # é™åˆ¶äº‹ä»¶æ•¸é‡ä»¥é¿å…ç„¡é™å¾ªç’°
                if len(events) >= 10:
                    break

            # é©—è­‰çµæœ
            has_status_event = any(
                "å·¥ä½œæµé–‹å§‹" in str(event.get("data", {}).get("content", "")) for event in events
            )

            return {
                "passed": len(events) > 0,
                "events_count": len(events),
                "has_status_event": has_status_event,
                "sample_event": events[0] if events else None,
            }

        except Exception as e:
            return {"passed": False, "error": str(e)}

    async def test_langgraph_compatibility(self) -> Dict[str, Any]:
        """æ¸¬è©¦ LangGraph ç›¸å®¹æ€§"""
        try:
            # æ¸¬è©¦ ainvoke
            input_data = {"messages": [{"role": "user", "content": "æ¸¬è©¦ LangGraph ç›¸å®¹æ€§"}]}

            result = await self.compatibility_layer.ainvoke(input_data)

            # é©—è­‰çµæœæ ¼å¼
            has_messages = "messages" in result
            has_final_report = "final_report" in result
            has_metadata = "execution_metadata" in result

            # æ¸¬è©¦ astream
            events = []
            async for event in self.compatibility_layer.astream(input_data):
                events.append(event)
                if len(events) >= 5:  # é™åˆ¶äº‹ä»¶æ•¸é‡
                    break

            return {
                "passed": has_messages and has_metadata and len(events) > 0,
                "ainvoke_result": {
                    "has_messages": has_messages,
                    "has_final_report": has_final_report,
                    "has_metadata": has_metadata,
                },
                "astream_events": len(events),
                "sample_event_format": type(events[0]).__name__ if events else None,
            }

        except Exception as e:
            return {"passed": False, "error": str(e)}

    async def test_response_mapping(self) -> Dict[str, Any]:
        """æ¸¬è©¦éŸ¿æ‡‰æ˜ å°„"""
        try:
            # æ¸¬è©¦æˆåŠŸçµæœæ˜ å°„
            autogen_result = {
                "success": True,
                "research_topic": "æ¸¬è©¦ä¸»é¡Œ",
                "final_report": "æ¸¬è©¦å ±å‘Š",
                "execution_time": 10.5,
                "workflow_plan": {
                    "id": "test_plan",
                    "name": "æ¸¬è©¦è¨ˆåŠƒ",
                    "steps": [
                        {"step_id": "step_1", "description": "æ¸¬è©¦æ­¥é©Ÿ", "step_type": "research"}
                    ],
                },
                "session_id": "test_session",
                "timestamp": datetime.now().isoformat(),
            }

            mapped_result = ResponseMapper.map_execution_result(autogen_result)

            # é©—è­‰æ˜ å°„çµæœ
            has_success = mapped_result.get("success") == True
            has_topic = "research_topic" in mapped_result
            has_report = "final_report" in mapped_result
            has_metadata = "execution_metadata" in mapped_result

            # æ¸¬è©¦å¤±æ•—çµæœæ˜ å°„
            error_result = {
                "success": False,
                "error": "æ¸¬è©¦éŒ¯èª¤",
                "timestamp": datetime.now().isoformat(),
            }

            mapped_error = ResponseMapper.map_execution_result(error_result)
            has_error = mapped_error.get("success") == False and "error" in mapped_error

            return {
                "passed": has_success and has_topic and has_report and has_metadata and has_error,
                "success_mapping": {
                    "has_success": has_success,
                    "has_topic": has_topic,
                    "has_report": has_report,
                    "has_metadata": has_metadata,
                },
                "error_mapping": has_error,
            }

        except Exception as e:
            return {"passed": False, "error": str(e)}

    async def test_stream_response(self) -> Dict[str, Any]:
        """æ¸¬è©¦æµå¼éŸ¿æ‡‰"""
        try:
            # å‰µå»ºæ¨¡æ“¬çš„ AutoGen æµ
            async def mock_autogen_stream():
                events = [
                    {
                        "event": "message_chunk",
                        "data": {
                            "thread_id": "test",
                            "agent": "system",
                            "content": "æ¸¬è©¦å…§å®¹",
                            "id": "test_1",
                        },
                    },
                    {
                        "event": "message_chunk",
                        "data": {
                            "thread_id": "test",
                            "agent": "assistant",
                            "content": "å®Œæˆ",
                            "finish_reason": "stop",
                            "id": "test_2",
                        },
                    },
                ]

                for event in events:
                    yield event

            # æ¸¬è©¦æµå¼æ˜ å°„
            sse_events = []
            async for sse_event in StreamResponseMapper.map_stream_events(mock_autogen_stream()):
                sse_events.append(sse_event)

            # é©—è­‰ SSE æ ¼å¼
            valid_sse = all(event.startswith("event:") and "data:" in event for event in sse_events)

            return {
                "passed": len(sse_events) == 2 and valid_sse,
                "sse_events_count": len(sse_events),
                "valid_sse_format": valid_sse,
                "sample_sse": sse_events[0] if sse_events else None,
            }

        except Exception as e:
            return {"passed": False, "error": str(e)}

    async def test_request_format_compatibility(self) -> Dict[str, Any]:
        """æ¸¬è©¦è«‹æ±‚æ ¼å¼ç›¸å®¹æ€§"""
        try:
            # æ¸¬è©¦ä¸åŒçš„è«‹æ±‚æ ¼å¼
            test_cases = [
                # æ¨™æº–æ ¼å¼
                {
                    "messages": [{"role": "user", "content": "æ¨™æº–æ ¼å¼æ¸¬è©¦"}],
                    "thread_id": "test1",
                    "max_plan_iterations": 1,
                },
                # ç©ºè¨Šæ¯
                {"messages": [], "thread_id": "test2"},
                # å¤šæ¨¡æ…‹å…§å®¹
                {
                    "messages": [
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": "å¤šæ¨¡æ…‹æ¸¬è©¦"},
                                {"type": "text", "text": "ç¬¬äºŒéƒ¨åˆ†"},
                            ],
                        }
                    ],
                    "thread_id": "test3",
                },
            ]

            results = []
            for i, test_case in enumerate(test_cases):
                try:
                    # æ¸¬è©¦åƒæ•¸è§£æ
                    messages = test_case.get("messages", [])
                    user_input = self.api_adapter._extract_user_input(messages)

                    config = self.api_adapter._create_config(
                        max_plan_iterations=test_case.get("max_plan_iterations", 1),
                        max_step_num=3,
                        max_search_results=3,
                        auto_accepted_plan=True,
                        enable_background_investigation=False,
                        enable_deep_thinking=False,
                    )

                    results.append(
                        {
                            "test_case": i + 1,
                            "success": True,
                            "user_input_extracted": bool(user_input) or len(messages) == 0,
                            "config_created": config is not None,
                        }
                    )

                except Exception as e:
                    results.append({"test_case": i + 1, "success": False, "error": str(e)})

            all_passed = all(result.get("success", False) for result in results)

            return {
                "passed": all_passed,
                "test_cases": len(test_cases),
                "passed_cases": sum(1 for r in results if r.get("success", False)),
                "results": results,
            }

        except Exception as e:
            return {"passed": False, "error": str(e)}

    async def test_error_handling(self) -> Dict[str, Any]:
        """æ¸¬è©¦éŒ¯èª¤è™•ç†"""
        try:
            # æ¸¬è©¦ç„¡æ•ˆè¼¸å…¥
            error_cases = []

            # æ¸¬è©¦ç©ºè¨Šæ¯çš„éŒ¯èª¤è™•ç†
            events = []
            async for event in self.api_adapter.process_chat_request(
                messages=[], thread_id="error_test", auto_accepted_plan=True
            ):
                events.append(event)
                if len(events) >= 3:  # é™åˆ¶äº‹ä»¶æ•¸é‡
                    break

            has_error_event = any(
                event.get("event") == "error"
                or "éŒ¯èª¤" in str(event.get("data", {}).get("content", ""))
                for event in events
            )

            error_cases.append(
                {
                    "case": "empty_messages",
                    "has_error_event": has_error_event,
                    "events_count": len(events),
                }
            )

            # æ¸¬è©¦ LangGraph ç›¸å®¹æ€§éŒ¯èª¤è™•ç†
            try:
                invalid_result = await self.compatibility_layer.ainvoke(None)
                langgraph_error_handled = "error" in str(invalid_result).lower()
            except:
                langgraph_error_handled = True  # ç•°å¸¸è¢«æ­£ç¢ºæ‹‹å‡º

            error_cases.append(
                {"case": "langgraph_invalid_input", "error_handled": langgraph_error_handled}
            )

            return {
                "passed": all(
                    case.get("has_error_event", case.get("error_handled", False))
                    for case in error_cases
                ),
                "error_cases": error_cases,
            }

        except Exception as e:
            return {"passed": False, "error": str(e)}


# ä¾¿åˆ©å‡½æ•¸
async def run_compatibility_tests() -> Dict[str, Any]:
    """é‹è¡Œç›¸å®¹æ€§æ¸¬è©¦"""
    tester = CompatibilityTester()
    return await tester.run_all_tests()


async def quick_compatibility_check() -> bool:
    """å¿«é€Ÿç›¸å®¹æ€§æª¢æŸ¥"""
    try:
        tester = CompatibilityTester()

        # ç°¡å–®çš„ API é©é…å™¨æ¸¬è©¦
        result = await tester.test_api_adapter()
        return result.get("passed", False)

    except Exception:
        return False


if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    async def main():
        print("é–‹å§‹ AutoGen API ç›¸å®¹æ€§æ¸¬è©¦...")
        results = await run_compatibility_tests()

        print(f"\næ¸¬è©¦çµæœæ‘˜è¦:")
        print(f"ç¸½æ¸¬è©¦æ•¸: {results['total_tests']}")
        print(f"é€šé: {results['passed']}")
        print(f"å¤±æ•—: {results['failed']}")
        print(f"æˆåŠŸç‡: {results['success_rate']:.1f}%")

        if results["success_rate"] >= 80:
            print("\nâœ… ç›¸å®¹æ€§æ¸¬è©¦å¤§éƒ¨åˆ†é€šéï¼Œç³»çµ±å¯ä»¥ä½¿ç”¨")
        else:
            print("\nâŒ ç›¸å®¹æ€§æ¸¬è©¦å¤±æ•—è¼ƒå¤šï¼Œå»ºè­°æª¢æŸ¥ç³»çµ±é…ç½®")

    asyncio.run(main())
```

## 3. é‡æ§‹ compatibility å±¤

ç¾åœ¨é–‹å§‹æ¸…ç† compatibility å±¤ä¸­çš„ Mock é¡åˆ¥å’Œé‡è¤‡ä»£ç¢¼ï¼š

```python
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
AutoGen API é©é…å™¨

æä¾›èˆ‡åŸæœ‰ LangGraph API å®Œå…¨ç›¸å®¹çš„æ¥å£å±¤ã€‚
"""

import asyncio
from typing import Dict, Any, Optional, List, AsyncGenerator, Union
from datetime import datetime


from autogen_core.models import ChatCompletionClient

from src.deerflow_logging import get_simple_logger as get_logger
from src.config.report_style import ReportStyle
from src.rag.retriever import Resource
# ç§»é™¤ä¸å­˜åœ¨çš„å°å…¥ï¼Œä½¿ç”¨å¯¦éš›çš„é…ç½®é¡åˆ¥
from src.config.report_style import ReportStyle

logger = get_logger(__name__)


class AutoGenAPIAdapter:
    """
    AutoGen API é©é…å™¨

    æä¾›çµ±ä¸€çš„ API æ¥å£ï¼Œå…§éƒ¨ä½¿ç”¨ AutoGen ç³»çµ±ã€‚
    """

    def __init__(self, model_client: ChatCompletionClient):
        """
        åˆå§‹åŒ– API é©é…å™¨

        Args:
            model_client: èŠå¤©å®Œæˆå®¢æˆ¶ç«¯
        """
        self.model_client = model_client
        self.active_workflows: Dict[str, ResearchWorkflowManager] = {}

        logger.info("AutoGen API é©é…å™¨åˆå§‹åŒ–å®Œæˆ")

    async def process_chat_request(
        self, messages: List[Dict[str, Any]], thread_id: str = "default", **kwargs
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        è™•ç†èŠå¤©è«‹æ±‚

        Args:
            messages: è¨Šæ¯åˆ—è¡¨
            thread_id: åŸ·è¡Œç·’ ID
            **kwargs: å…¶ä»–é…ç½®åƒæ•¸

        Yields:
            Dict[str, Any]: äº‹ä»¶æ•¸æ“š
        """
        logger.info(f"è™•ç†èŠå¤©è«‹æ±‚: thread_id={thread_id}")

        try:
            # æå–ç”¨æˆ¶è¼¸å…¥
            user_input = self._extract_user_input(messages)

            # å‰µå»ºé…ç½®
            config = self._create_config(**kwargs)

            # å‰µå»ºæˆ–ç²å–å·¥ä½œæµç®¡ç†å™¨
            workflow_manager = await self._get_workflow_manager(thread_id, config)

            # åŸ·è¡Œå·¥ä½œæµä¸¦ç”¢ç”Ÿäº‹ä»¶
            async for event in self._execute_workflow_with_events(
                workflow_manager, user_input, thread_id
            ):
                yield event

        except Exception as e:
            logger.error(f"èŠå¤©è«‹æ±‚è™•ç†å¤±æ•—: {e}")
            yield self._create_error_event(str(e), thread_id)

    def _extract_user_input(self, messages: List[Dict[str, Any]]) -> str:
        """æå–ç”¨æˆ¶è¼¸å…¥"""
        if not messages:
            return ""

        # æ‰¾åˆ°æœ€å¾Œä¸€å€‹ç”¨æˆ¶è¨Šæ¯
        for msg in reversed(messages):
            if msg.get("role") == "user":
                return msg.get("content", "")

        return ""

    def _create_config(self, **kwargs) -> Dict[str, Any]:
        """å‰µå»ºå°è©±é…ç½®"""
        return {
            "enable_background_investigation": kwargs.get("enable_background_investigation", True),
            "max_plan_iterations": kwargs.get("max_plan_iterations", 1),
            "max_step_iterations": kwargs.get("max_step_num", 3),
            "max_search_results": kwargs.get("max_search_results", 3),
            "auto_accept_plan": kwargs.get("auto_accept_plan", True),
            "human_feedback_enabled": not kwargs.get("auto_accepted_plan", True),
            "debug_mode": kwargs.get("debug", False),
            "report_style": kwargs.get("report_style", ReportStyle.ACADEMIC),
            "resources": kwargs.get("resources", []),
            "mcp_settings": kwargs.get("mcp_settings", {}),
        }

    async def _get_workflow_manager(
        self, thread_id: str, config: Dict[str, Any]
    ) -> Any:
        """ç²å–æˆ–å‰µå»ºå·¥ä½œæµç®¡ç†å™¨"""
        # ç°¡åŒ–å¯¦ç¾ï¼Œç›´æ¥ä½¿ç”¨ AutoGen ç³»çµ±
        if thread_id not in self.active_workflows:
            # é€™è£¡æ‡‰è©²å‰µå»º AutoGen å·¥ä½œæµç®¡ç†å™¨
            # æš«æ™‚ä½¿ç”¨ç°¡åŒ–å¯¦ç¾
            self.active_workflows[thread_id] = {"config": config, "client": self.model_client}

        return self.active_workflows[thread_id]

    async def _execute_workflow_with_events(
        self, workflow_manager: Any, user_input: str, thread_id: str
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """åŸ·è¡Œå·¥ä½œæµä¸¦ç”¢ç”Ÿäº‹ä»¶"""

        # é–‹å§‹äº‹ä»¶
        yield self._create_event(
            "workflow_start",
            {"message": "é–‹å§‹åŸ·è¡Œç ”ç©¶å·¥ä½œæµ", "user_input": user_input},
            "coordinator",
            thread_id,
        )

        try:
            # åŸ·è¡Œå·¥ä½œæµ - ç°¡åŒ–å¯¦ç¾
            # é€™è£¡æ‡‰è©²èª¿ç”¨å¯¦éš›çš„ AutoGen å·¥ä½œæµ
            result = {
                "success": True,
                "workflow_plan": {"steps": []},
                "execution_result": {"steps_by_status": {"completed": 1}},
                "final_report": f"è™•ç†å®Œæˆï¼š{user_input}",
                "execution_time": 1.0
            }

            # å·¥ä½œæµå„éšæ®µäº‹ä»¶
            if result.get("success"):
                # è¨ˆåŠƒç”Ÿæˆäº‹ä»¶
                yield self._create_event(
                    "plan_generated",
                    {"message": "ç ”ç©¶è¨ˆåŠƒå·²ç”Ÿæˆ", "plan": result.get("workflow_plan")},
                    "planner",
                    thread_id,
                )

                # åŸ·è¡Œäº‹ä»¶
                execution_result = result.get("execution_result", {})
                steps_completed = execution_result.get("steps_by_status", {}).get("completed", 0)

                yield self._create_event(
                    "execution_progress",
                    {"message": f"å·²å®Œæˆ {steps_completed} å€‹æ­¥é©Ÿ", "progress": execution_result},
                    "researcher",
                    thread_id,
                )

                # æœ€çµ‚å ±å‘Šäº‹ä»¶
                final_report = result.get("final_report", "")
                if final_report:
                    # åˆ†å¡Šç™¼é€å ±å‘Š
                    chunk_size = 500
                    for i in range(0, len(final_report), chunk_size):
                        chunk = final_report[i : i + chunk_size]
                        is_final = i + chunk_size >= len(final_report)

                        yield self._create_event(
                            "message_chunk",
                            {"content": chunk, "finish_reason": "stop" if is_final else None},
                            "reporter",
                            thread_id,
                        )

                # å®Œæˆäº‹ä»¶
                yield self._create_event(
                    "workflow_complete",
                    {
                        "message": "ç ”ç©¶å·¥ä½œæµåŸ·è¡Œå®Œæˆ",
                        "execution_time": result.get("execution_time", 0),
                        "success": True,
                    },
                    "coordinator",
                    thread_id,
                )
            else:
                # éŒ¯èª¤äº‹ä»¶
                error_msg = result.get("error", "æœªçŸ¥éŒ¯èª¤")
                yield self._create_error_event(error_msg, thread_id)

        except Exception as e:
            logger.error(f"å·¥ä½œæµåŸ·è¡Œç•°å¸¸: {e}")
            yield self._create_error_event(str(e), thread_id)

    def _create_event(
        self,
        event_type: str,
        data: Dict[str, Any],
        agent: str = "system",
        thread_id: str = "default",
    ) -> Dict[str, Any]:
        """å‰µå»ºäº‹ä»¶"""
        return {
            "event": event_type,
            "data": {
                **data,
                "agent": agent,
                "thread_id": thread_id,
                "timestamp": datetime.now().isoformat(),
            },
        }

    def _create_error_event(self, error_message: str, thread_id: str = "default") -> Dict[str, Any]:
        """å‰µå»ºéŒ¯èª¤äº‹ä»¶"""
        return self._create_event(
            "error",
            {"content": f"âŒ åŸ·è¡ŒéŒ¯èª¤: {error_message}", "error": True, "finish_reason": "error"},
            "error",
            thread_id,
        )

    async def cleanup_thread(self, thread_id: str):
        """æ¸…ç†åŸ·è¡Œç·’è³‡æº"""
        if thread_id in self.active_workflows:
            await self.active_workflows[thread_id].cleanup()
            del self.active_workflows[thread_id]
            logger.info(f"å·²æ¸…ç†åŸ·è¡Œç·’: {thread_id}")

    async def cleanup_all(self):
        """æ¸…ç†æ‰€æœ‰è³‡æº"""
        for thread_id in list(self.active_workflows.keys()):
            await self.cleanup_thread(thread_id)
        logger.info("å·²æ¸…ç†æ‰€æœ‰åŸ·è¡Œç·’")


# å…¨åŸŸ API ç›¸å®¹æ€§å‡½æ•¸
async def run_agent_workflow_async(
    user_input: str,
    debug: bool = False,
    max_plan_iterations: int = 1,
    max_step_num: int = 3,
    enable_background_investigation: bool = True,
    auto_accepted_plan: bool = True,
    resources: List[Resource] = None,
    report_style: ReportStyle = ReportStyle.ACADEMIC,
    mcp_settings: Dict[str, Any] = None,
    model_client: ChatCompletionClient = None,
    **kwargs,
) -> Dict[str, Any]:
    """
    ç•°æ­¥åŸ·è¡Œæ™ºèƒ½é«”å·¥ä½œæµ

    ä¿æŒèˆ‡åŸæœ‰ LangGraph API å®Œå…¨ç›¸å®¹çš„ä»‹é¢ã€‚

    Args:
        user_input: ç”¨æˆ¶è¼¸å…¥
        debug: åµéŒ¯æ¨¡å¼
        max_plan_iterations: æœ€å¤§è¨ˆåŠƒè¿­ä»£æ¬¡æ•¸
        max_step_num: æœ€å¤§æ­¥é©Ÿæ•¸
        enable_background_investigation: å•Ÿç”¨èƒŒæ™¯èª¿æŸ¥
        auto_accepted_plan: è‡ªå‹•æ¥å—è¨ˆåŠƒ
        resources: è³‡æºåˆ—è¡¨
        report_style: å ±å‘Šé¢¨æ ¼
        mcp_settings: MCP è¨­å®š
        model_client: æ¨¡å‹å®¢æˆ¶ç«¯
        **kwargs: å…¶ä»–åƒæ•¸

    Returns:
        Dict[str, Any]: åŸ·è¡Œçµæœ
    """
    logger.info(f"åŸ·è¡Œæ™ºèƒ½é«”å·¥ä½œæµ: {user_input}")

    if not model_client:
        # å¦‚æœæ²’æœ‰æä¾›æ¨¡å‹å®¢æˆ¶ç«¯ï¼Œéœ€è¦å¾å…¨åŸŸé…ç½®å–å¾—
        from src.llms.llm import get_default_model_client

        model_client = get_default_model_client()

    # å‰µå»º API é©é…å™¨
    adapter = AutoGenAPIAdapter(model_client)

    try:
        # æº–å‚™åƒæ•¸
        messages = [{"role": "user", "content": user_input}]

        config_params = {
            "debug": debug,
            "max_plan_iterations": max_plan_iterations,
            "max_step_num": max_step_num,
            "enable_background_investigation": enable_background_investigation,
            "auto_accepted_plan": auto_accepted_plan,
            "resources": resources or [],
            "report_style": report_style,
            "mcp_settings": mcp_settings or {},
            **kwargs,
        }

        # æ”¶é›†æ‰€æœ‰äº‹ä»¶
        events = []
        final_content = ""
        execution_metadata = {}

        async for event in adapter.process_chat_request(
            messages=messages, thread_id="api_workflow", **config_params
        ):
            events.append(event)

            # æå–æœ€çµ‚å…§å®¹
            data = event.get("data", {})
            if data.get("agent") == "reporter" and data.get("content"):
                final_content += data["content"]

            # æå–åŸ·è¡Œå…ƒæ•¸æ“š
            if event.get("event") == "workflow_complete":
                execution_metadata = data

        # è¿”å›ç›¸å®¹æ ¼å¼çš„çµæœ
        return {
            "success": True,
            "user_input": user_input,
            "final_report": final_content,
            "events": events,
            "execution_metadata": execution_metadata,
            "debug_info": {
                "total_events": len(events),
                "completed_at": datetime.now().isoformat(),
            }
            if debug
            else None,
        }

    except Exception as e:
        logger.error(f"å·¥ä½œæµåŸ·è¡Œå¤±æ•—: {e}")
        return {
            "success": False,
            "user_input": user_input,
            "error": str(e),
            "final_report": "",
            "events": [],
            "execution_metadata": {},
            "debug_info": {
                "error_details": str(e),
                "failed_at": datetime.now().isoformat(),
            }
            if debug
            else None,
        }

    finally:
        # æ¸…ç†è³‡æº
        await adapter.cleanup_all()


def run_agent_workflow(
    user_input: str,
    debug: bool = False,
    max_plan_iterations: int = 1,
    max_step_num: int = 3,
    enable_background_investigation: bool = True,
    auto_accepted_plan: bool = True,
    resources: List[Resource] = None,
    report_style: ReportStyle = ReportStyle.ACADEMIC,
    mcp_settings: Dict[str, Any] = None,
    model_client: ChatCompletionClient = None,
    **kwargs,
) -> Dict[str, Any]:
    """
    åŒæ­¥åŸ·è¡Œæ™ºèƒ½é«”å·¥ä½œæµ

    Args:
        åŒ run_agent_workflow_async

    Returns:
        Dict[str, Any]: åŸ·è¡Œçµæœ
    """
    # ä½¿ç”¨ asyncio é‹è¡Œç•°æ­¥ç‰ˆæœ¬
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    try:
        result = loop.run_until_complete(
            run_agent_workflow_async(
                user_input=user_input,
                debug=debug,
                max_plan_iterations=max_plan_iterations,
                max_step_num=max_step_num,
                enable_background_investigation=enable_background_investigation,
                auto_accepted_plan=auto_accepted_plan,
                resources=resources,
                report_style=report_style,
                mcp_settings=mcp_settings,
                model_client=model_client,
                **kwargs,
            )
        )
        return result
    finally:
        loop.close()


def create_autogen_api_adapter(
    model_client: ChatCompletionClient = None, **kwargs
) -> AutoGenAPIAdapter:
    """
    å‰µå»º AutoGen API é©é…å™¨å¯¦ä¾‹

    Args:
        model_client: èŠå¤©å®Œæˆå®¢æˆ¶ç«¯
        **kwargs: å…¶ä»–åƒæ•¸

    Returns:
        AutoGenAPIAdapter: API é©é…å™¨å¯¦ä¾‹
    """
    return AutoGenAPIAdapter(model_client=model_client, **kwargs)

```

ç¾åœ¨æ¸…ç†å…¶ä»– compatibility æª”æ¡ˆï¼š

```python
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
AutoGen API æœå‹™å™¨

æä¾›èˆ‡ç¾æœ‰ API å®Œå…¨ç›¸å®¹çš„ AutoGen æ¥å£ã€‚
"""

import asyncio
from typing import List, Dict, Any, Optional
from uuid import uuid4

from fastapi import HTTPException
from fastapi.responses import StreamingResponse

from src.config.report_style import ReportStyle
from src.llms.llm import get_configured_llm_models
from src.rag.retriever import Resource
from src.server.chat_request import ChatRequest
from src.deerflow_logging import get_simple_logger as get_logger, set_thread_context, clear_thread_context, setup_thread_logging

from .api_adapter import AutoGenAPIAdapter
from .langgraph_compatibility import LangGraphCompatibilityLayer
from .response_mapper import StreamResponseMapper


# ä½¿ç”¨å¯¦éš›çš„ AutoGen é¡åˆ¥
from autogen_core.models import ChatCompletionClient as OpenAIChatCompletionClient

logger = get_logger(__name__)


class AutoGenAPIServer:
    """
    AutoGen API æœå‹™å™¨

    æä¾›èˆ‡ç¾æœ‰ FastAPI ç«¯é»ç›¸å®¹çš„ AutoGen æ¥å£ã€‚
    """

    def __init__(self):
        """åˆå§‹åŒ– API æœå‹™å™¨"""
        self.model_clients = {}
        self.api_adapters = {}
        self.compatibility_layers = {}

        # åˆå§‹åŒ–æ¨¡å‹å®¢æˆ¶ç«¯
        self._initialize_model_clients()

        logger.info("AutoGen API æœå‹™å™¨åˆå§‹åŒ–å®Œæˆ")

    def _initialize_model_clients(self):
        """åˆå§‹åŒ–æ¨¡å‹å®¢æˆ¶ç«¯"""
        try:
            configured_models = get_configured_llm_models()

            # æª¢æŸ¥ configured_models çš„çµæ§‹
            if isinstance(configured_models, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œè½‰æ›ç‚ºæ¨¡å‹é…ç½®åˆ—è¡¨
                model_configs = []
                for llm_type, model_names in configured_models.items():
                    if isinstance(model_names, list):
                        for model_name in model_names:
                            model_configs.append(
                                {
                                    "name": f"{llm_type}_{model_name}",
                                    "model": model_name,
                                    "type": llm_type,
                                }
                            )
                    else:
                        # å¦‚æœ model_names ä¸æ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
                        model_configs.append(
                            {
                                "name": f"{llm_type}_{model_names}",
                                "model": model_names,
                                "type": llm_type,
                            }
                        )
            else:
                # å¦‚æœå·²ç¶“æ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
                model_configs = configured_models

            # ç‚ºæ¯å€‹é…ç½®çš„æ¨¡å‹å‰µå»ºå®¢æˆ¶ç«¯
            for model_config in model_configs:
                if isinstance(model_config, dict):
                    model_name = model_config.get("name", "default")
                    model_value = model_config.get("model", "gpt-4")
                else:
                    # å¦‚æœ model_config ä¸æ˜¯å­—å…¸ï¼Œè·³é
                    continue

                # é€™è£¡æ‡‰è©²æ ¹æ“šå¯¦éš›çš„æ¨¡å‹é…ç½®å‰µå»ºå®¢æˆ¶ç«¯
                # æš«æ™‚ä½¿ç”¨æ¨¡æ“¬çš„å®¢æˆ¶ç«¯
                try:
                    # ä½¿ç”¨ AutoGen é©é…å™¨å‰µå»ºå®¢æˆ¶ç«¯
                    from src.autogen_system.adapters.llm_adapter import create_autogen_model_client
                    
                    client = create_autogen_model_client(
                        llm_type="basic",
                        config={"model": model_value}
                    )
                    self.model_clients[model_name] = client

                    # å‰µå»ºå°æ‡‰çš„é©é…å™¨å’Œç›¸å®¹æ€§å±¤
                    try:
                        from .api_adapter import AutoGenAPIAdapter
                        from .langgraph_compatibility import LangGraphCompatibilityLayer

                        self.api_adapters[model_name] = AutoGenAPIAdapter(client)
                        self.compatibility_layers[model_name] = LangGraphCompatibilityLayer(client)
                        logger.info(f"æˆåŠŸå‰µå»ºæ¨¡å‹ {model_name} çš„é©é…å™¨å’Œç›¸å®¹æ€§å±¤")
                    except Exception as e:
                        logger.error(f"å‰µå»ºé©é…å™¨å¤±æ•— {model_name}: {e}")
                        # å³ä½¿é©é…å™¨å‰µå»ºå¤±æ•—ï¼Œä¹Ÿè¦ç¢ºä¿æœ‰é è¨­çš„é©é…å™¨

                except Exception as e:
                    logger.warning(f"ç„¡æ³•å‰µå»ºæ¨¡å‹å®¢æˆ¶ç«¯ {model_name}: {e}")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æ¨¡å‹å®¢æˆ¶ç«¯å¤±æ•—: {e}")

        # ç¢ºä¿è‡³å°‘æœ‰é è¨­å®¢æˆ¶ç«¯å’Œé©é…å™¨
        if "default" not in self.model_clients:
            try:
                from src.autogen_system.adapters.llm_adapter import create_autogen_model_client
                
                default_client = create_autogen_model_client(
                    llm_type="basic",
                    config={"model": "gpt-4o-mini"}
                )
                self.model_clients["default"] = default_client
                logger.info("å‰µå»ºé è¨­æ¨¡å‹å®¢æˆ¶ç«¯")
            except Exception as e:
                logger.error(f"å‰µå»ºé è¨­æ¨¡å‹å®¢æˆ¶ç«¯å¤±æ•—: {e}")
                # å‰µå»ºä¸€å€‹æœ€åŸºæœ¬çš„é©é…å™¨
                from src.autogen_system.adapters.llm_adapter import LLMChatCompletionAdapter
                self.model_clients["default"] = LLMChatCompletionAdapter("basic")

        if "default" not in self.api_adapters:
            try:
                from .api_adapter import AutoGenAPIAdapter

                self.api_adapters["default"] = AutoGenAPIAdapter(self.model_clients["default"])
                logger.info("å‰µå»ºé è¨­ API é©é…å™¨")
            except Exception as e:
                logger.error(f"å‰µå»ºé è¨­ API é©é…å™¨å¤±æ•—: {e}")
                # å‰µå»ºä¸€å€‹æœ€åŸºæœ¬çš„æ¨¡æ“¬é©é…å™¨
                self.api_adapters["default"] = self._create_mock_adapter()

        if "default" not in self.compatibility_layers:
            try:
                from .langgraph_compatibility import LangGraphCompatibilityLayer

                self.compatibility_layers["default"] = LangGraphCompatibilityLayer(
                    self.model_clients["default"]
                )
                logger.info("å‰µå»ºé è¨­ç›¸å®¹æ€§å±¤")
            except Exception as e:
                logger.error(f"å‰µå»ºé è¨­ç›¸å®¹æ€§å±¤å¤±æ•—: {e}")
                # å‰µå»ºä¸€å€‹æœ€åŸºæœ¬çš„æ¨¡æ“¬ç›¸å®¹æ€§å±¤
                self.compatibility_layers["default"] = self._create_mock_compatibility_layer()

        logger.info(
            f"åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹å®¢æˆ¶ç«¯: {len(self.model_clients)}, é©é…å™¨: {len(self.api_adapters)}, ç›¸å®¹æ€§å±¤: {len(self.compatibility_layers)}"
        )

    def _create_mock_adapter(self):
        """å‰µå»ºæ¨¡æ“¬é©é…å™¨ä½œç‚ºå‚™ç”¨"""

        class MockAdapter:
            async def process_chat_request(self, messages, thread_id="default", **kwargs):
                yield {
                    "type": "error",
                    "data": {
                        "message": "AutoGen ç³»çµ±æš«æ™‚ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥é…ç½®",
                        "thread_id": thread_id,
                        "timestamp": "2025-01-08T16:00:00Z",
                    },
                }

        return MockAdapter()

    def _create_mock_compatibility_layer(self):
        """å‰µå»ºæ¨¡æ“¬ç›¸å®¹æ€§å±¤ä½œç‚ºå‚™ç”¨"""

        class MockCompatibilityLayer:
            async def ainvoke(self, input_data, config=None):
                return {
                    "error": "AutoGen ç›¸å®¹æ€§å±¤æš«æ™‚ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥é…ç½®",
                    "timestamp": "2025-01-08T16:00:00Z",
                }

        return MockCompatibilityLayer()

    def get_model_client(self, model_name: str = "default"):
        """ç²å–æ¨¡å‹å®¢æˆ¶ç«¯"""
        return self.model_clients.get(model_name, self.model_clients.get("default"))

    def get_api_adapter(self, model_name: str = "default") -> AutoGenAPIAdapter:
        """ç²å– API é©é…å™¨"""
        return self.api_adapters.get(model_name, self.api_adapters.get("default"))

    def get_compatibility_layer(self, model_name: str = "default") -> LangGraphCompatibilityLayer:
        """ç²å–ç›¸å®¹æ€§å±¤"""
        return self.compatibility_layers.get(model_name, self.compatibility_layers.get("default"))

    async def handle_chat_stream(self, request: ChatRequest) -> StreamingResponse:
        """
        è™•ç†èŠå¤©æµå¼è«‹æ±‚

        Args:
            request: èŠå¤©è«‹æ±‚

        Returns:
            StreamingResponse: æµå¼éŸ¿æ‡‰
        """
        thread_id = request.thread_id
        if thread_id == "__default__":
            thread_id = str(uuid4())

        # è¨˜éŒ„ API å‘¼å«
        logger.info("AutoGen Chat stream started")

        # è¨­ç½®åŸ·è¡Œç·’ä¸Šä¸‹æ–‡
        clear_thread_context()
        logger.info(f"Thread [{thread_id}] started")

        return StreamingResponse(
            self._autogen_stream_generator(request, thread_id),
            media_type="text/event-stream",
        )

    async def _autogen_stream_generator(self, request: ChatRequest, thread_id: str):
        """AutoGen æµå¼ç”Ÿæˆå™¨"""
        try:
            # è¨­å®šåŸ·è¡Œç·’ä¸Šä¸‹æ–‡
            # ä½¿ç”¨æ–°çš„ Thread-specific æ—¥èªŒç³»çµ±
            thread_logger = setup_thread_logging(thread_id)
            set_thread_context(thread_id)

            # è¨˜éŒ„ thread é–‹å§‹
            thread_logger.info(f"é–‹å§‹è™•ç† AutoGen å°è©±: {thread_id}")

            # ç²å– API é©é…å™¨
            adapter = self.get_api_adapter()

            # è½‰æ›è«‹æ±‚åƒæ•¸
            messages = [msg.dict() for msg in request.messages] if request.messages else []

            # åŸ·è¡Œ AutoGen å·¥ä½œæµ
            autogen_stream = adapter.process_chat_request(
                messages=messages,
                thread_id=thread_id,
                resources=request.resources or [],
                max_plan_iterations=request.max_plan_iterations or 1,
                max_step_num=request.max_step_num or 3,
                max_search_results=request.max_search_results or 3,
                auto_accepted_plan=request.auto_accepted_plan or False,
                interrupt_feedback=request.interrupt_feedback,
                mcp_settings=request.mcp_settings or {},
                enable_background_investigation=request.enable_background_investigation or True,
                report_style=request.report_style or ReportStyle.ACADEMIC,
                enable_deep_thinking=request.enable_deep_thinking or False,
            )

            # å°‡ AutoGen æµè½‰æ›ç‚º SSE æ ¼å¼
            async for sse_event in StreamResponseMapper.map_stream_events(autogen_stream):
                yield sse_event

        except Exception as e:
            if thread_logger:
                thread_logger.error(f"AutoGen æµå¼ç”Ÿæˆå¤±æ•—: {e}")
            else:
                logger.error(f"AutoGen æµå¼ç”Ÿæˆå¤±æ•—: {e}")
            # ç™¼é€éŒ¯èª¤äº‹ä»¶
            error_sse = StreamResponseMapper._create_error_sse(str(e))
            yield error_sse

        finally:
            # è¨˜éŒ„ thread çµæŸ
            if thread_logger:
                thread_logger.info(f"AutoGen å°è©±è™•ç†å®Œæˆ: {thread_id}")
            # æ¸…ç†åŸ·è¡Œç·’ä¸Šä¸‹æ–‡
            clear_thread_context()

    async def handle_langgraph_compatibility(
        self, input_data: Dict[str, Any], config: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        è™•ç† LangGraph ç›¸å®¹æ€§è«‹æ±‚

        Args:
            input_data: è¼¸å…¥æ•¸æ“š
            config: é…ç½®åƒæ•¸

        Returns:
            Dict[str, Any]: åŸ·è¡Œçµæœ
        """
        try:
            compatibility_layer = self.get_compatibility_layer()
            result = await compatibility_layer.ainvoke(input_data, config)
            return result

        except Exception as e:
            logger.error(f"LangGraph ç›¸å®¹æ€§è™•ç†å¤±æ•—: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    async def handle_langgraph_astream(
        self,
        input_data: Dict[str, Any],
        config: Dict[str, Any] = None,
        stream_mode: List[str] = None,
        subgraphs: bool = True,
    ):
        """
        è™•ç† LangGraph astream ç›¸å®¹æ€§è«‹æ±‚

        Args:
            input_data: è¼¸å…¥æ•¸æ“š
            config: é…ç½®åƒæ•¸
            stream_mode: æµæ¨¡å¼
            subgraphs: æ˜¯å¦åŒ…å«å­åœ–

        Yields:
            tuple: LangGraph æ ¼å¼çš„äº‹ä»¶
        """
        try:
            compatibility_layer = self.get_compatibility_layer()

            async for event in compatibility_layer.astream(
                input_data=input_data, config=config, stream_mode=stream_mode, subgraphs=subgraphs
            ):
                yield event

        except Exception as e:
            logger.error(f"LangGraph astream ç›¸å®¹æ€§è™•ç†å¤±æ•—: {e}")
            # ç”¢ç”ŸéŒ¯èª¤äº‹ä»¶
            yield compatibility_layer._create_langgraph_error(str(e))

    def get_server_status(self) -> Dict[str, Any]:
        """ç²å–æœå‹™å™¨ç‹€æ…‹"""
        return {
            "status": "running",
            "system": "autogen",
            "available_models": list(self.model_clients.keys()),
            "adapters_count": len(self.api_adapters),
            "compatibility_layers_count": len(self.compatibility_layers),
            "features": {
                "chat_stream": True,
                "langgraph_compatibility": True,
                "interactive_workflow": True,
                "tool_integration": True,
                "human_feedback": True,
            },
        }


# å…¨åŸŸæœå‹™å™¨å¯¦ä¾‹
autogen_api_server = AutoGenAPIServer()


# ä¾¿åˆ©å‡½æ•¸
async def get_autogen_chat_stream(request: ChatRequest) -> StreamingResponse:
    """ç²å– AutoGen èŠå¤©æµ"""
    return await autogen_api_server.handle_chat_stream(request)


async def invoke_autogen_workflow(
    input_data: Dict[str, Any], config: Dict[str, Any] = None
) -> Dict[str, Any]:
    """èª¿ç”¨ AutoGen å·¥ä½œæµ"""
    return await autogen_api_server.handle_langgraph_compatibility(input_data, config)


async def stream_autogen_workflow(
    input_data: Dict[str, Any],
    config: Dict[str, Any] = None,
    stream_mode: List[str] = None,
    subgraphs: bool = True,
):
    """æµå¼èª¿ç”¨ AutoGen å·¥ä½œæµ"""
    async for event in autogen_api_server.handle_langgraph_astream(
        input_data, config, stream_mode, subgraphs
    ):
        yield event

```

```python
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
LangGraph ç›¸å®¹æ€§å±¤

æä¾›èˆ‡ç¾æœ‰ LangGraph æ¥å£çš„å®Œå…¨ç›¸å®¹æ€§ã€‚
"""

import asyncio
from typing import Dict, List, Any, Optional, AsyncGenerator, Union
from datetime import datetime


# ä½¿ç”¨å¯¦éš›çš„ AutoGen é¡åˆ¥
from autogen_core.models import ChatCompletionClient

from src.deerflow_logging import get_simple_logger as get_logger
from src.config.report_style import ReportStyle
from src.rag.retriever import Resource
# ç§»é™¤ä¸å­˜åœ¨çš„å°å…¥
from .api_adapter import AutoGenAPIAdapter

logger = get_logger(__name__)


class LangGraphCompatibilityLayer:
    """
    LangGraph ç›¸å®¹æ€§å±¤

    æ¨¡æ“¬ LangGraph çš„æ¥å£è¡Œç‚ºï¼Œå…§éƒ¨ä½¿ç”¨ AutoGen ç³»çµ±ã€‚
    """

    def __init__(self, model_client: ChatCompletionClient):
        """
        åˆå§‹åŒ–ç›¸å®¹æ€§å±¤

        Args:
            model_client: èŠå¤©å®Œæˆå®¢æˆ¶ç«¯
        """
        self.model_client = model_client
        self.api_adapter = AutoGenAPIAdapter(model_client)
        self._state_storage: Dict[str, Dict[str, Any]] = {}

        logger.info("LangGraph ç›¸å®¹æ€§å±¤åˆå§‹åŒ–å®Œæˆ")

    async def astream(
        self,
        input_data: Union[Dict[str, Any], Any],
        config: Dict[str, Any] = None,
        stream_mode: List[str] = None,
        subgraphs: bool = True,
    ) -> AsyncGenerator[tuple, None]:
        """
        æ¨¡æ“¬ LangGraph çš„ astream æ–¹æ³•

        Args:
            input_data: è¼¸å…¥æ•¸æ“š
            config: é…ç½®åƒæ•¸
            stream_mode: æµæ¨¡å¼
            subgraphs: æ˜¯å¦åŒ…å«å­åœ–

        Yields:
            tuple: (agent, metadata, event_data) æ ¼å¼çš„å…ƒçµ„
        """
        logger.info("LangGraph ç›¸å®¹æ€§å±¤ - astream é–‹å§‹")

        try:
            # è§£æè¼¸å…¥æ•¸æ“š
            messages, thread_id = self._parse_input_data(input_data)

            # è§£æé…ç½®
            adapter_config = self._parse_config(config or {})

            # å¾é…ç½®ä¸­ç§»é™¤ thread_idï¼Œé¿å…é‡è¤‡å‚³é
            if "thread_id" in adapter_config:
                del adapter_config["thread_id"]

            # åŸ·è¡Œ AutoGen å·¥ä½œæµ
            async for event in self.api_adapter.process_chat_request(
                messages=messages, thread_id=thread_id, **adapter_config
            ):
                # è½‰æ›ç‚º LangGraph æ ¼å¼
                langgraph_event = self._convert_to_langgraph_format(event)
                yield langgraph_event

        except Exception as e:
            logger.error(f"LangGraph ç›¸å®¹æ€§å±¤åŸ·è¡Œå¤±æ•—: {e}")
            # ç”¢ç”ŸéŒ¯èª¤äº‹ä»¶
            yield self._create_langgraph_error(str(e))

    def _parse_input_data(
        self, input_data: Union[Dict[str, Any], Any]
    ) -> tuple[List[Dict[str, Any]], str]:
        """è§£æè¼¸å…¥æ•¸æ“š"""
        if isinstance(input_data, dict):
            messages = input_data.get("messages", [])
            thread_id = "default_thread"

            # è™•ç† LangGraph æ ¼å¼çš„è¨Šæ¯
            if messages and isinstance(messages, list):
                # å°‡ LangGraph è¨Šæ¯æ ¼å¼è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
                converted_messages = []
                for msg in messages:
                    if isinstance(msg, dict):
                        converted_messages.append(msg)
                    else:
                        # è™•ç† LangChain è¨Šæ¯å°è±¡
                        converted_msg = {
                            "role": getattr(msg, "type", "user"),
                            "content": getattr(msg, "content", str(msg)),
                        }
                        converted_messages.append(converted_msg)

                return converted_messages, thread_id

            # è™•ç†å…¶ä»–æ ¼å¼çš„è¼¸å…¥
            elif "research_topic" in input_data:
                research_topic = input_data["research_topic"]
                messages = [{"role": "user", "content": research_topic}]
                return messages, thread_id

        # è™•ç†å­—ç¬¦ä¸²è¼¸å…¥
        elif isinstance(input_data, str):
            messages = [{"role": "user", "content": input_data}]
            return messages, "default_thread"

        # é è¨­è™•ç†
        return [], "default_thread"

    def _parse_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æé…ç½®åƒæ•¸"""
        return {
            "thread_id": config.get("thread_id", "default_thread"),
            "resources": config.get("resources", []),
            "max_plan_iterations": config.get("max_plan_iterations", 1),
            "max_step_num": config.get("max_step_num", 3),
            "max_search_results": config.get("max_search_results", 3),
            "auto_accepted_plan": config.get("auto_accepted_plan", True),
            "interrupt_feedback": config.get("interrupt_feedback"),
            "mcp_settings": config.get("mcp_settings", {}),
            "enable_background_investigation": config.get("enable_background_investigation", True),
            "report_style": self._parse_report_style(config.get("report_style", "academic")),
            "enable_deep_thinking": config.get("enable_deep_thinking", False),
        }

    def _parse_report_style(self, style: Union[str, ReportStyle]) -> ReportStyle:
        """è§£æå ±å‘Šé¢¨æ ¼"""
        if isinstance(style, ReportStyle):
            return style

        style_mapping = {
            "academic": ReportStyle.ACADEMIC,
            "popular_science": ReportStyle.POPULAR_SCIENCE,
            "news": ReportStyle.NEWS,
            "social_media": ReportStyle.SOCIAL_MEDIA,
        }

        return style_mapping.get(str(style).lower(), ReportStyle.ACADEMIC)

    def _convert_to_langgraph_format(self, autogen_event: Dict[str, Any]) -> tuple:
        """å°‡ AutoGen äº‹ä»¶è½‰æ›ç‚º LangGraph æ ¼å¼"""
        event_type = autogen_event.get("event", "message_chunk")
        data = autogen_event.get("data", {})

        # å‰µå»º agent æ¨™è­˜
        agent_name = data.get("agent", "autogen")
        agent = (f"{agent_name}:default",)

        # å‰µå»ºå…ƒæ•¸æ“š
        metadata = {
            "thread_id": data.get("thread_id", "default"),
            "event_type": event_type,
            "timestamp": datetime.now().isoformat(),
        }

        # æ ¹æ“šäº‹ä»¶é¡å‹å‰µå»ºä¸åŒçš„äº‹ä»¶æ•¸æ“š
        if event_type == "interrupt":
            # ä¸­æ–·äº‹ä»¶
            event_data = {
                "__interrupt__": [
                    (
                        type(
                            "Interrupt",
                            (),
                            {
                                "ns": [data.get("id", "interrupt")],
                                "value": data.get("content", "ä¸­æ–·è«‹æ±‚"),
                            },
                        )()
                    )
                ]
            }
        elif event_type == "error":
            # éŒ¯èª¤äº‹ä»¶
            event_data = self._create_message_chunk(data, is_error=True)
        else:
            # æ™®é€šè¨Šæ¯äº‹ä»¶
            event_data = self._create_message_chunk(data)

        return agent, metadata, event_data

    def _create_message_chunk(self, data: Dict[str, Any], is_error: bool = False) -> tuple:
        """å‰µå»ºè¨Šæ¯å¡Š"""
        from langchain_core.messages import AIMessageChunk

        content = data.get("content", "")
        message_id = data.get("id", f"msg_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}")

        # å‰µå»º AIMessageChunk
        message_chunk = AIMessageChunk(content=content, id=message_id, response_metadata={})

        # æ·»åŠ å®ŒæˆåŸå› 
        finish_reason = data.get("finish_reason")
        if finish_reason:
            message_chunk.response_metadata["finish_reason"] = finish_reason

        # æ·»åŠ éŒ¯èª¤æ¨™è¨˜
        if is_error:
            message_chunk.response_metadata["error"] = True

        # å‰µå»ºå…ƒæ•¸æ“š
        chunk_metadata = {
            "agent": data.get("agent", "autogen"),
            "thread_id": data.get("thread_id", "default"),
            "timestamp": datetime.now().isoformat(),
        }

        return message_chunk, chunk_metadata

    def _create_langgraph_error(self, error_message: str) -> tuple:
        """å‰µå»º LangGraph æ ¼å¼çš„éŒ¯èª¤äº‹ä»¶"""
        agent = ("error:default",)
        metadata = {
            "thread_id": "default",
            "event_type": "error",
            "timestamp": datetime.now().isoformat(),
        }

        error_data = {
            "content": f"âŒ {error_message}",
            "id": f"error_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}",
            "agent": "error",
            "finish_reason": "error",
        }

        event_data = self._create_message_chunk(error_data, is_error=True)

        return agent, metadata, event_data

    def invoke(self, input_data: Dict[str, Any], config: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        æ¨¡æ“¬ LangGraph çš„ invoke æ–¹æ³•ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰

        Args:
            input_data: è¼¸å…¥æ•¸æ“š
            config: é…ç½®åƒæ•¸

        Returns:
            Dict[str, Any]: åŸ·è¡Œçµæœ
        """
        logger.info("LangGraph ç›¸å®¹æ€§å±¤ - invoke é–‹å§‹")

        # ä½¿ç”¨ asyncio é‹è¡Œç•°æ­¥ç‰ˆæœ¬
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            result = loop.run_until_complete(self.ainvoke(input_data, config))
            return result
        finally:
            loop.close()

    async def ainvoke(
        self, input_data: Dict[str, Any], config: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        æ¨¡æ“¬ LangGraph çš„ ainvoke æ–¹æ³•ï¼ˆç•°æ­¥ç‰ˆæœ¬ï¼‰

        Args:
            input_data: è¼¸å…¥æ•¸æ“š
            config: é…ç½®åƒæ•¸

        Returns:
            Dict[str, Any]: åŸ·è¡Œçµæœ
        """
        logger.info("LangGraph ç›¸å®¹æ€§å±¤ - ainvoke é–‹å§‹")

        try:
            # è§£æè¼¸å…¥æ•¸æ“š
            messages, thread_id = self._parse_input_data(input_data)

            # è§£æé…ç½®
            adapter_config = self._parse_config(config or {})

            # å¾é…ç½®ä¸­ç§»é™¤ thread_idï¼Œé¿å…é‡è¤‡å‚³é
            if "thread_id" in adapter_config:
                del adapter_config["thread_id"]

            # æ”¶é›†æ‰€æœ‰äº‹ä»¶
            events = []
            final_content = ""

            async for event in self.api_adapter.process_chat_request(
                messages=messages, thread_id=thread_id, **adapter_config
            ):
                events.append(event)

                # æ”¶é›†æœ€çµ‚å…§å®¹
                data = event.get("data", {})
                if data.get("agent") == "reporter" and data.get("content"):
                    final_content += data["content"]

            # è¿”å› LangGraph é¢¨æ ¼çš„çµæœ
            return {
                "messages": messages + [{"role": "assistant", "content": final_content}],
                "final_report": final_content,
                "thread_id": thread_id,
                "events": events,
                "execution_metadata": {
                    "total_events": len(events),
                    "completed_at": datetime.now().isoformat(),
                    "success": True,
                },
            }

        except Exception as e:
            logger.error(f"LangGraph ç›¸å®¹æ€§å±¤ ainvoke å¤±æ•—: {e}")
            return {
                "messages": [],
                "final_report": "",
                "thread_id": "error",
                "events": [],
                "execution_metadata": {
                    "error": str(e),
                    "completed_at": datetime.now().isoformat(),
                    "success": False,
                },
            }

    def get_state(self, thread_id: str) -> Dict[str, Any]:
        """ç²å–åŸ·è¡Œç·’ç‹€æ…‹"""
        return self._state_storage.get(thread_id, {})

    def update_state(self, thread_id: str, state: Dict[str, Any]):
        """æ›´æ–°åŸ·è¡Œç·’ç‹€æ…‹"""
        if thread_id not in self._state_storage:
            self._state_storage[thread_id] = {}
        self._state_storage[thread_id].update(state)


# ä¾¿åˆ©å‡½æ•¸
def create_langgraph_compatible_graph(
    model_client: ChatCompletionClient,
) -> LangGraphCompatibilityLayer:
    """å‰µå»º LangGraph ç›¸å®¹çš„åœ–å°è±¡"""
    return LangGraphCompatibilityLayer(model_client)

```

```python
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
ç³»çµ±åˆ‡æ›å™¨

æä¾› LangGraph å’Œ AutoGen ç³»çµ±ä¹‹é–“çš„å‹•æ…‹åˆ‡æ›åŠŸèƒ½ã€‚
"""

import os
import asyncio
from typing import Dict, Any, Optional, List, Union
from enum import Enum
from datetime import datetime


# ä½¿ç”¨å¯¦éš›çš„ AutoGen é¡åˆ¥
from autogen_core.models import ChatCompletionClient

from src.deerflow_logging import get_simple_logger as get_logger
from src.config.report_style import ReportStyle
from src.rag.retriever import Resource

logger = get_logger(__name__)


class SystemType(Enum):
    """ç³»çµ±é¡å‹"""

    LANGGRAPH = "langgraph"
    AUTOGEN = "autogen"


class SystemSwitcher:
    """
    ç³»çµ±åˆ‡æ›å™¨

    å‹•æ…‹é¸æ“‡ä½¿ç”¨ LangGraph æˆ– AutoGen ç³»çµ±åŸ·è¡Œå·¥ä½œæµã€‚
    """

    def __init__(self, default_system: SystemType = SystemType.AUTOGEN):
        """
        åˆå§‹åŒ–ç³»çµ±åˆ‡æ›å™¨

        Args:
            default_system: é è¨­ç³»çµ±é¡å‹
        """
        self.default_system = default_system
        self.current_system = self._detect_system()
        self.performance_stats = {
            SystemType.LANGGRAPH: {"count": 0, "total_time": 0, "errors": 0},
            SystemType.AUTOGEN: {"count": 0, "total_time": 0, "errors": 0},
        }

        logger.info(f"ç³»çµ±åˆ‡æ›å™¨åˆå§‹åŒ–å®Œæˆï¼Œç•¶å‰ç³»çµ±: {self.current_system.value}")

    def _detect_system(self) -> SystemType:
        """æª¢æ¸¬æ‡‰ä½¿ç”¨çš„ç³»çµ±"""
        # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
        env_system = os.getenv("USE_AUTOGEN_SYSTEM", "true").lower()

        if env_system in ["true", "1", "yes", "on"]:
            return SystemType.AUTOGEN
        elif env_system in ["false", "0", "no", "off"]:
            return SystemType.LANGGRAPH
        else:
            return self.default_system

    def get_current_system(self) -> SystemType:
        """ç²å–ç•¶å‰ç³»çµ±é¡å‹"""
        return self.current_system

    def switch_system(self, system_type: SystemType):
        """åˆ‡æ›ç³»çµ±"""
        old_system = self.current_system
        self.current_system = system_type
        logger.info(f"ç³»çµ±å·²åˆ‡æ›: {old_system.value} -> {system_type.value}")

    async def run_workflow(
        self,
        user_input: str,
        workflow_type: str = "research",
        model_client: Optional[ChatCompletionClient] = None,
        force_system: Optional[SystemType] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œå·¥ä½œæµï¼ˆè‡ªå‹•é¸æ“‡ç³»çµ±ï¼‰

        Args:
            user_input: ç”¨æˆ¶è¼¸å…¥
            workflow_type: å·¥ä½œæµé¡å‹
            model_client: æ¨¡å‹å®¢æˆ¶ç«¯
            force_system: å¼·åˆ¶ä½¿ç”¨çš„ç³»çµ±é¡å‹
            **kwargs: å…¶ä»–åƒæ•¸

        Returns:
            Dict[str, Any]: åŸ·è¡Œçµæœ
        """
        # æ±ºå®šä½¿ç”¨çš„ç³»çµ±
        system_to_use = force_system or self.current_system

        start_time = datetime.now()

        try:
            if system_to_use == SystemType.AUTOGEN:
                result = await self._run_autogen_workflow(
                    user_input, workflow_type, model_client, **kwargs
                )
            else:
                result = await self._run_langgraph_workflow(
                    user_input, workflow_type, model_client, **kwargs
                )

            # è¨˜éŒ„æˆåŠŸçµ±è¨ˆ
            execution_time = (datetime.now() - start_time).total_seconds()
            self.performance_stats[system_to_use]["count"] += 1
            self.performance_stats[system_to_use]["total_time"] += execution_time

            # æ·»åŠ ç³»çµ±æ¨™è­˜
            result["system_used"] = system_to_use.value
            result["execution_time"] = execution_time

            return result

        except Exception as e:
            # è¨˜éŒ„éŒ¯èª¤çµ±è¨ˆ
            self.performance_stats[system_to_use]["errors"] += 1

            logger.error(f"{system_to_use.value} ç³»çµ±åŸ·è¡Œå¤±æ•—: {e}")

            # å¦‚æœä¸æ˜¯å¼·åˆ¶æŒ‡å®šç³»çµ±ï¼Œå˜—è©¦å›é€€åˆ°å¦ä¸€å€‹ç³»çµ±
            if not force_system and system_to_use != self.default_system:
                logger.info(f"å˜—è©¦å›é€€åˆ° {self.default_system.value} ç³»çµ±")
                return await self.run_workflow(
                    user_input,
                    workflow_type,
                    model_client,
                    force_system=self.default_system,
                    **kwargs,
                )

            # å›å‚³éŒ¯èª¤çµæœ
            return {
                "success": False,
                "error": str(e),
                "system_used": system_to_use.value,
                "execution_time": (datetime.now() - start_time).total_seconds(),
                "user_input": user_input,
                "timestamp": datetime.now().isoformat(),
            }

    async def _run_autogen_workflow(
        self,
        user_input: str,
        workflow_type: str,
        model_client: Optional[ChatCompletionClient],
        **kwargs,
    ) -> Dict[str, Any]:
        """åŸ·è¡Œ AutoGen å·¥ä½œæµ"""
        logger.info(f"ä½¿ç”¨ AutoGen ç³»çµ±åŸ·è¡Œ {workflow_type} å·¥ä½œæµ")

        if workflow_type == "research":
            from src.autogen_system.compatibility.api_adapter import run_agent_workflow_async

            return await run_agent_workflow_async(
                user_input=user_input, model_client=model_client, **kwargs
            )
        elif workflow_type == "podcast":
            from src.autogen_system.workflows.podcast_workflow import PodcastWorkflowManager

            manager = PodcastWorkflowManager(model_client)
            await manager.initialize()
            return await manager.run_podcast_workflow(user_input, **kwargs)
        elif workflow_type == "ppt":
            from src.autogen_system.workflows.ppt_workflow import PPTWorkflowManager

            manager = PPTWorkflowManager(model_client)
            await manager.initialize()
            return await manager.run_ppt_workflow(user_input, **kwargs)
        elif workflow_type == "prose":
            from src.autogen_system.workflows.prose_workflow import ProseWorkflowManager

            manager = ProseWorkflowManager(model_client)
            await manager.initialize()
            return await manager.run_prose_workflow(user_input, **kwargs)
        elif workflow_type == "prompt_enhancer":
            from src.autogen_system.workflows.prompt_enhancer_workflow import (
                PromptEnhancerWorkflowManager,
            )

            manager = PromptEnhancerWorkflowManager(model_client)
            await manager.initialize()
            return await manager.run_prompt_enhancer_workflow(user_input, **kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ AutoGen å·¥ä½œæµé¡å‹: {workflow_type}")

    async def _run_langgraph_workflow(
        self,
        user_input: str,
        workflow_type: str,
        model_client: Optional[ChatCompletionClient],
        **kwargs,
    ) -> Dict[str, Any]:
        """åŸ·è¡Œ LangGraph å·¥ä½œæµ"""
        logger.info(f"ä½¿ç”¨ LangGraph ç³»çµ±åŸ·è¡Œ {workflow_type} å·¥ä½œæµ")

        try:
            # å˜—è©¦åŒ¯å…¥ LangGraph ç³»çµ±
            if workflow_type == "research":
                from src.workflow import run_agent_workflow_async

                return await run_agent_workflow_async(user_input=user_input, **kwargs)
            else:
                # å…¶ä»–å·¥ä½œæµå¯èƒ½éœ€è¦ä¸åŒçš„åŒ¯å…¥è·¯å¾‘
                raise NotImplementedError(f"LangGraph {workflow_type} å·¥ä½œæµå°šæœªå¯¦ç¾")

        except ImportError as e:
            logger.error(f"LangGraph ç³»çµ±ä¸å¯ç”¨: {e}")
            raise Exception("LangGraph ç³»çµ±ä¸å¯ç”¨ï¼Œè«‹å®‰è£ç›¸é—œä¾è³´æˆ–åˆ‡æ›åˆ° AutoGen ç³»çµ±")

    def get_performance_stats(self) -> Dict[str, Any]:
        """ç²å–æ•ˆèƒ½çµ±è¨ˆ"""
        stats = {}

        for system_type, data in self.performance_stats.items():
            count = data["count"]
            total_time = data["total_time"]
            errors = data["errors"]

            stats[system_type.value] = {
                "execution_count": count,
                "total_execution_time": total_time,
                "average_execution_time": total_time / count if count > 0 else 0,
                "error_count": errors,
                "success_rate": (count - errors) / count * 100 if count > 0 else 0,
            }

        return {
            "current_system": self.current_system.value,
            "statistics": stats,
            "timestamp": datetime.now().isoformat(),
        }

    def recommend_system(self) -> SystemType:
        """æ ¹æ“šæ•ˆèƒ½çµ±è¨ˆæ¨è–¦ç³»çµ±"""
        autogen_stats = self.performance_stats[SystemType.AUTOGEN]
        langgraph_stats = self.performance_stats[SystemType.LANGGRAPH]

        # å¦‚æœä»»ä¸€ç³»çµ±åŸ·è¡Œæ¬¡æ•¸å¤ªå°‘ï¼Œæ¨è–¦é è¨­ç³»çµ±
        if autogen_stats["count"] < 5 and langgraph_stats["count"] < 5:
            return self.default_system

        # è¨ˆç®—æ•ˆèƒ½æŒ‡æ¨™
        autogen_success_rate = (
            (autogen_stats["count"] - autogen_stats["errors"]) / autogen_stats["count"]
            if autogen_stats["count"] > 0
            else 0
        )
        langgraph_success_rate = (
            (langgraph_stats["count"] - langgraph_stats["errors"]) / langgraph_stats["count"]
            if langgraph_stats["count"] > 0
            else 0
        )

        autogen_avg_time = (
            autogen_stats["total_time"] / autogen_stats["count"]
            if autogen_stats["count"] > 0
            else float("inf")
        )
        langgraph_avg_time = (
            langgraph_stats["total_time"] / langgraph_stats["count"]
            if langgraph_stats["count"] > 0
            else float("inf")
        )

        # å„ªå…ˆè€ƒæ…®æˆåŠŸç‡ï¼Œå…¶æ¬¡è€ƒæ…®åŸ·è¡Œæ™‚é–“
        if autogen_success_rate > langgraph_success_rate:
            return SystemType.AUTOGEN
        elif langgraph_success_rate > autogen_success_rate:
            return SystemType.LANGGRAPH
        else:
            # æˆåŠŸç‡ç›¸åŒæ™‚ï¼Œé¸æ“‡æ›´å¿«çš„ç³»çµ±
            return (
                SystemType.AUTOGEN
                if autogen_avg_time < langgraph_avg_time
                else SystemType.LANGGRAPH
            )

    def set_environment_system(self, system_type: SystemType):
        """è¨­ç½®ç’°å¢ƒè®Šæ•¸ä¾†æ§åˆ¶ç³»çµ±é¸æ“‡"""
        os.environ["USE_AUTOGEN_SYSTEM"] = "true" if system_type == SystemType.AUTOGEN else "false"
        self.current_system = system_type
        logger.info(f"ç’°å¢ƒç³»çµ±è¨­å®šç‚º: {system_type.value}")

    async def health_check(self) -> Dict[str, Any]:
        """ç³»çµ±å¥åº·æª¢æŸ¥"""
        health_status = {
            "timestamp": datetime.now().isoformat(),
            "current_system": self.current_system.value,
            "systems": {},
        }

        # æª¢æŸ¥ AutoGen ç³»çµ±
        try:
            from src.autogen_system.compatibility.api_adapter import run_agent_workflow_async

            # ç°¡å–®æ¸¬è©¦
            test_result = await asyncio.wait_for(
                run_agent_workflow_async(
                    user_input="å¥åº·æª¢æŸ¥æ¸¬è©¦", auto_accepted_plan=True, max_step_num=1
                ),
                timeout=30,
            )
            health_status["systems"]["autogen"] = {
                "available": True,
                "status": "healthy",
                "test_success": test_result.get("success", False),
            }
        except Exception as e:
            health_status["systems"]["autogen"] = {
                "available": False,
                "status": "error",
                "error": str(e),
            }

        # æª¢æŸ¥ LangGraph ç³»çµ±
        try:
            from src.workflow import run_agent_workflow_async as langgraph_workflow

            # ç°¡å–®æ¸¬è©¦
            test_result = await asyncio.wait_for(
                langgraph_workflow(
                    user_input="å¥åº·æª¢æŸ¥æ¸¬è©¦", auto_accepted_plan=True, max_step_num=1
                ),
                timeout=30,
            )
            health_status["systems"]["langgraph"] = {
                "available": True,
                "status": "healthy",
                "test_success": test_result.get("success", False),
            }
        except Exception as e:
            health_status["systems"]["langgraph"] = {
                "available": False,
                "status": "error",
                "error": str(e),
            }

        return health_status


# å…¨åŸŸåˆ‡æ›å™¨å¯¦ä¾‹
global_system_switcher = SystemSwitcher()


# ä¾¿åˆ©å‡½æ•¸
async def run_workflow_with_auto_switch(
    user_input: str,
    workflow_type: str = "research",
    model_client: Optional[ChatCompletionClient] = None,
    **kwargs,
) -> Dict[str, Any]:
    """
    ä½¿ç”¨è‡ªå‹•ç³»çµ±åˆ‡æ›åŸ·è¡Œå·¥ä½œæµ

    Args:
        user_input: ç”¨æˆ¶è¼¸å…¥
        workflow_type: å·¥ä½œæµé¡å‹
        model_client: æ¨¡å‹å®¢æˆ¶ç«¯
        **kwargs: å…¶ä»–åƒæ•¸

    Returns:
        Dict[str, Any]: åŸ·è¡Œçµæœ
    """
    return await global_system_switcher.run_workflow(
        user_input, workflow_type, model_client, **kwargs
    )


def get_current_system() -> str:
    """ç²å–ç•¶å‰ä½¿ç”¨çš„ç³»çµ±"""
    return global_system_switcher.get_current_system().value


def switch_to_autogen():
    """åˆ‡æ›åˆ° AutoGen ç³»çµ±"""
    global_system_switcher.switch_system(SystemType.AUTOGEN)
    global_system_switcher.set_environment_system(SystemType.AUTOGEN)


def switch_to_langgraph():
    """åˆ‡æ›åˆ° LangGraph ç³»çµ±"""
    global_system_switcher.switch_system(SystemType.LANGGRAPH)
    global_system_switcher.set_environment_system(SystemType.LANGGRAPH)


async def system_health_check() -> Dict[str, Any]:
    """åŸ·è¡Œç³»çµ±å¥åº·æª¢æŸ¥"""
    return await global_system_switcher.health_check()


def get_system_performance_stats() -> Dict[str, Any]:
    """ç²å–ç³»çµ±æ•ˆèƒ½çµ±è¨ˆ"""
    return global_system_switcher.get_performance_stats()

```

## 4. çµ±ä¸€å°å…¥è·¯å¾‘

ç¾åœ¨çµ±ä¸€æ‰€æœ‰æª”æ¡ˆçš„å°å…¥è·¯å¾‘ï¼Œä½¿å…¶æ›´åŠ ä¸€è‡´ï¼š

```python
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
éŸ¿æ‡‰æ˜ å°„å™¨

è² è²¬å°‡ AutoGen ç³»çµ±çš„éŸ¿æ‡‰è½‰æ›ç‚ºå‰ç«¯æœŸæœ›çš„æ ¼å¼ã€‚
"""

import json
from typing import Dict, List, Any, Optional, AsyncGenerator
from datetime import datetime

from src.deerflow_logging import get_simple_logger as get_logger

logger = get_logger(__name__)


class ResponseMapper:
    """
    éŸ¿æ‡‰æ˜ å°„å™¨

    å°‡ AutoGen ç³»çµ±éŸ¿æ‡‰è½‰æ›ç‚ºå‰ç«¯å…¼å®¹æ ¼å¼ã€‚
    """

    @staticmethod
    def map_execution_result(autogen_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ˜ å°„åŸ·è¡Œçµæœ

        Args:
            autogen_result: AutoGen åŸ·è¡Œçµæœ

        Returns:
            Dict[str, Any]: æ˜ å°„å¾Œçš„çµæœ
        """
        if not autogen_result:
            return ResponseMapper._create_empty_result()

        success = autogen_result.get("success", False)

        if success:
            return {
                "success": True,
                "research_topic": autogen_result.get("research_topic", ""),
                "final_report": autogen_result.get("final_report", ""),
                "execution_time": autogen_result.get("execution_time", 0),
                "plan": autogen_result.get("workflow_plan", {}),
                "execution_metadata": {
                    "session_id": autogen_result.get("session_id"),
                    "interaction_enabled": autogen_result.get("interaction_enabled", False),
                    "timestamp": autogen_result.get("timestamp"),
                    "execution_result": autogen_result.get("execution_result", {}),
                },
            }
        else:
            return {
                "success": False,
                "error": autogen_result.get("error", "æœªçŸ¥éŒ¯èª¤"),
                "timestamp": autogen_result.get("timestamp"),
                "session_id": autogen_result.get("session_id"),
                "execution_metadata": {
                    "error_details": autogen_result.get("error", ""),
                    "failed_at": autogen_result.get("timestamp"),
                },
            }

    @staticmethod
    def map_plan_data(autogen_plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ˜ å°„è¨ˆåŠƒæ•¸æ“š

        Args:
            autogen_plan: AutoGen è¨ˆåŠƒæ•¸æ“š

        Returns:
            Dict[str, Any]: æ˜ å°„å¾Œçš„è¨ˆåŠƒ
        """
        if not autogen_plan:
            return {}

        return {
            "plan_id": autogen_plan.get("plan_id", autogen_plan.get("id", "")),
            "name": autogen_plan.get("name", "ç ”ç©¶è¨ˆåŠƒ"),
            "description": autogen_plan.get("description", ""),
            "steps": ResponseMapper._map_plan_steps(autogen_plan.get("steps", [])),
            "metadata": autogen_plan.get("metadata", {}),
            "estimated_time": ResponseMapper._calculate_estimated_time(
                autogen_plan.get("steps", [])
            ),
            "created_at": autogen_plan.get("created_at"),
            "status": autogen_plan.get("status", "pending"),
        }

    @staticmethod
    def _map_plan_steps(steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ˜ å°„è¨ˆåŠƒæ­¥é©Ÿ"""
        mapped_steps = []

        for i, step in enumerate(steps):
            mapped_step = {
                "step_id": step.get("step_id", step.get("id", f"step_{i}")),
                "step_type": step.get("step_type", "research"),
                "description": step.get("description", f"æ­¥é©Ÿ {i + 1}"),
                "expected_output": step.get("expected_output", ""),
                "dependencies": step.get("dependencies", []),
                "status": step.get("status", "pending"),
                "agent_type": step.get("agent_type", "researcher"),
                "inputs": step.get("inputs", {}),
                "estimated_time": step.get("timeout_seconds", 60),
            }
            mapped_steps.append(mapped_step)

        return mapped_steps

    @staticmethod
    def _calculate_estimated_time(steps: List[Dict[str, Any]]) -> int:
        """è¨ˆç®—é ä¼°åŸ·è¡Œæ™‚é–“ï¼ˆåˆ†é˜ï¼‰"""
        total_seconds = sum(step.get("timeout_seconds", 60) for step in steps)
        return max(1, total_seconds // 60)  # è‡³å°‘ 1 åˆ†é˜

    @staticmethod
    def _create_empty_result() -> Dict[str, Any]:
        """å‰µå»ºç©ºçµæœ"""
        return {
            "success": False,
            "error": "ç„¡åŸ·è¡Œçµæœ",
            "timestamp": datetime.now().isoformat(),
            "execution_metadata": {
                "error_details": "çµæœç‚ºç©ºæˆ–ç„¡æ•ˆ",
                "failed_at": datetime.now().isoformat(),
            },
        }


class StreamResponseMapper:
    """
    æµå¼éŸ¿æ‡‰æ˜ å°„å™¨

    è™•ç†æµå¼éŸ¿æ‡‰çš„æ ¼å¼è½‰æ›ã€‚
    """

    @staticmethod
    async def map_stream_events(
        autogen_stream: AsyncGenerator[Dict[str, Any], None],
    ) -> AsyncGenerator[str, None]:
        """
        æ˜ å°„æµå¼äº‹ä»¶ç‚º SSE æ ¼å¼

        Args:
            autogen_stream: AutoGen æµå¼äº‹ä»¶

        Yields:
            str: SSE æ ¼å¼çš„äº‹ä»¶å­—ç¬¦ä¸²
        """
        try:
            async for event in autogen_stream:
                sse_event = StreamResponseMapper._convert_to_sse(event)
                if sse_event:
                    yield sse_event

        except Exception as e:
            logger.error(f"æµå¼éŸ¿æ‡‰æ˜ å°„å¤±æ•—: {e}")
            # ç™¼é€éŒ¯èª¤äº‹ä»¶
            error_event = StreamResponseMapper._create_error_sse(str(e))
            yield error_event

    @staticmethod
    def _convert_to_sse(event: Dict[str, Any]) -> str:
        """å°‡äº‹ä»¶è½‰æ›ç‚º SSE æ ¼å¼"""
        event_type = event.get("event", "message_chunk")
        data = event.get("data", {})

        # æ¸…ç†ç©ºå…§å®¹
        if data.get("content") == "":
            data.pop("content", None)

        # æ§‹å»º SSE äº‹ä»¶
        sse_lines = [f"event: {event_type}"]
        sse_lines.append(f"data: {json.dumps(data, ensure_ascii=False)}")
        sse_lines.append("")  # ç©ºè¡Œè¡¨ç¤ºäº‹ä»¶çµæŸ

        return "\n".join(sse_lines) + "\n"

    @staticmethod
    def _create_error_sse(error_message: str) -> str:
        """å‰µå»ºéŒ¯èª¤ SSE äº‹ä»¶"""
        error_data = {
            "thread_id": "error",
            "agent": "system",
            "id": f"error_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}",
            "role": "assistant",
            "content": f"âŒ éŒ¯èª¤: {error_message}",
            "finish_reason": "error",
        }

        sse_lines = ["event: error"]
        sse_lines.append(f"data: {json.dumps(error_data, ensure_ascii=False)}")
        sse_lines.append("")

        return "\n".join(sse_lines) + "\n"

    @staticmethod
    def map_message_chunk(
        content: str,
        thread_id: str,
        agent: str = "assistant",
        finish_reason: str = None,
        additional_data: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """
        æ˜ å°„è¨Šæ¯å¡Š

        Args:
            content: è¨Šæ¯å…§å®¹
            thread_id: åŸ·è¡Œç·’ ID
            agent: æ™ºèƒ½é«”åç¨±
            finish_reason: å®ŒæˆåŸå› 
            additional_data: é¡å¤–æ•¸æ“š

        Returns:
            Dict[str, Any]: æ˜ å°„å¾Œçš„äº‹ä»¶æ•¸æ“š
        """
        event_data = {
            "thread_id": thread_id,
            "agent": agent,
            "id": f"msg_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}",
            "role": "assistant",
            "content": content,
        }

        if finish_reason:
            event_data["finish_reason"] = finish_reason

        if additional_data:
            event_data.update(additional_data)

        return {"event": "message_chunk", "data": event_data}

    @staticmethod
    def map_tool_call(
        tool_call_data: Dict[str, Any], thread_id: str, agent: str = "assistant"
    ) -> Dict[str, Any]:
        """
        æ˜ å°„å·¥å…·èª¿ç”¨äº‹ä»¶

        Args:
            tool_call_data: å·¥å…·èª¿ç”¨æ•¸æ“š
            thread_id: åŸ·è¡Œç·’ ID
            agent: æ™ºèƒ½é«”åç¨±

        Returns:
            Dict[str, Any]: æ˜ å°„å¾Œçš„å·¥å…·èª¿ç”¨äº‹ä»¶
        """
        return {
            "event": "tool_calls",
            "data": {
                "thread_id": thread_id,
                "agent": agent,
                "id": f"tool_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}",
                "role": "assistant",
                "content": "",
                "tool_calls": tool_call_data.get("tool_calls", []),
                "tool_call_chunks": tool_call_data.get("tool_call_chunks", []),
            },
        }

    @staticmethod
    def map_tool_result(
        tool_result: Dict[str, Any], thread_id: str, tool_call_id: str
    ) -> Dict[str, Any]:
        """
        æ˜ å°„å·¥å…·åŸ·è¡Œçµæœ

        Args:
            tool_result: å·¥å…·åŸ·è¡Œçµæœ
            thread_id: åŸ·è¡Œç·’ ID
            tool_call_id: å·¥å…·èª¿ç”¨ ID

        Returns:
            Dict[str, Any]: æ˜ å°„å¾Œçš„å·¥å…·çµæœäº‹ä»¶
        """
        return {
            "event": "tool_call_result",
            "data": {
                "thread_id": thread_id,
                "agent": "tool",
                "id": f"result_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}",
                "role": "assistant",
                "content": str(tool_result.get("result", "")),
                "tool_call_id": tool_call_id,
            },
        }


# ä¾¿åˆ©å‡½æ•¸
def map_autogen_to_frontend(autogen_result: Dict[str, Any]) -> Dict[str, Any]:
    """å°‡ AutoGen çµæœæ˜ å°„ç‚ºå‰ç«¯æ ¼å¼"""
    return ResponseMapper.map_execution_result(autogen_result)


async def stream_autogen_to_frontend(
    autogen_stream: AsyncGenerator[Dict[str, Any], None],
) -> AsyncGenerator[str, None]:
    """å°‡ AutoGen æµå¼éŸ¿æ‡‰æ˜ å°„ç‚ºå‰ç«¯ SSE æ ¼å¼"""
    async for sse_event in StreamResponseMapper.map_stream_events(autogen_stream):
        yield sse_event

```

```python
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
AutoGen è¨Šæ¯æ¡†æ¶

å®šç¾© Agent é–“çš„è¨Šæ¯å‚³éæ ¼å¼ï¼Œå–ä»£åŸæœ‰çš„ State ç‹€æ…‹ç®¡ç†ç³»çµ±ã€‚
ä½¿ç”¨ AutoGen åŸç”Ÿçš„è¨Šæ¯æ©Ÿåˆ¶ä¾†å¯¦ç¾å·¥ä½œæµç¨‹ç‹€æ…‹çš„å‚³éå’Œç®¡ç†ã€‚
"""

import json
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum

from src.deerflow_logging import get_simple_logger

logger = get_simple_logger(__name__)


class MessageType(str, Enum):
    """è¨Šæ¯é¡å‹æšèˆ‰"""

    COORDINATION = "coordination"
    PLAN = "plan"
    RESEARCH_RESULT = "research_result"
    CODE_EXECUTION = "code_execution"
    REPORT = "report"
    ERROR = "error"
    STATUS_UPDATE = "status_update"


class StepType(str, Enum):
    """æ­¥é©Ÿé¡å‹æšèˆ‰ï¼ˆå°æ‡‰åŸæœ‰çš„ LangGraph ç¯€é»ï¼‰"""

    RESEARCH = "research"
    PROCESSING = "processing"
    CODING = "coding"
    ANALYSIS = "analysis"
    REPORTING = "reporting"


class StepStatus(str, Enum):
    """æ­¥é©Ÿç‹€æ…‹æšèˆ‰"""

    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"


@dataclass
class WorkflowStep:
    """å·¥ä½œæµç¨‹æ­¥é©Ÿ"""

    id: str
    step_type: StepType
    description: str
    status: StepStatus = StepStatus.PENDING
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    dependencies: List[str] = None

    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []

    def mark_completed(self, result: Dict[str, Any]):
        """æ¨™è¨˜æ­¥é©Ÿå®Œæˆ"""
        self.status = StepStatus.COMPLETED
        self.result = result

    def mark_failed(self, error: str):
        """æ¨™è¨˜æ­¥é©Ÿå¤±æ•—"""
        self.status = StepStatus.FAILED
        self.error_message = error


@dataclass
class ResearchWorkflowMessage:
    """ç ”ç©¶å·¥ä½œæµç¨‹è¨Šæ¯åŸºé¡"""

    message_type: MessageType
    timestamp: str
    agent_name: str
    data: Dict[str, Any]
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()

    def to_json(self) -> str:
        """è½‰æ›ç‚º JSON å­—ä¸²"""
        return json.dumps(asdict(self), ensure_ascii=False, indent=2)

    @classmethod
    def from_json(cls, json_str: str) -> "ResearchWorkflowMessage":
        """å¾ JSON å­—ä¸²å‰µå»ºå¯¦ä¾‹"""
        data = json.loads(json_str)
        return cls(**data)

    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸"""
        return asdict(self)


@dataclass
class PlanMessage(ResearchWorkflowMessage):
    """è¨ˆåŠƒè¨Šæ¯"""

    def __init__(
        self,
        agent_name: str,
        steps: List[WorkflowStep],
        original_task: str,
        analysis: str = "",
        completed_steps: List[str] = None,
        **kwargs,
    ):
        data = {
            "steps": [asdict(step) for step in steps],
            "original_task": original_task,
            "analysis": analysis,
            "total_steps": len(steps),
            "completed_steps": completed_steps or [],
        }
        super().__init__(
            message_type=MessageType.PLAN,
            timestamp=datetime.now().isoformat(),
            agent_name=agent_name,
            data=data,
            **kwargs,
        )

    def get_steps(self) -> List[WorkflowStep]:
        """ç²å–æ­¥é©Ÿåˆ—è¡¨"""
        return [WorkflowStep(**step_data) for step_data in self.data["steps"]]

    def get_next_step(self) -> Optional[WorkflowStep]:
        """ç²å–ä¸‹ä¸€å€‹å¾…åŸ·è¡Œçš„æ­¥é©Ÿ"""
        completed = set(self.data.get("completed_steps", []))
        for step_data in self.data["steps"]:
            if step_data["id"] not in completed and step_data["status"] == StepStatus.PENDING:
                return WorkflowStep(**step_data)
        return None

    def mark_step_completed(self, step_id: str, result: Dict[str, Any]):
        """æ¨™è¨˜æ­¥é©Ÿå®Œæˆ"""
        completed_steps = self.data.get("completed_steps", [])
        if step_id not in completed_steps:
            completed_steps.append(step_id)
            self.data["completed_steps"] = completed_steps

        # æ›´æ–°æ­¥é©Ÿç‹€æ…‹
        for step_data in self.data["steps"]:
            if step_data["id"] == step_id:
                step_data["status"] = StepStatus.COMPLETED
                step_data["result"] = result
                break


@dataclass
class ResearchResultMessage(ResearchWorkflowMessage):
    """ç ”ç©¶çµæœè¨Šæ¯"""

    def __init__(
        self,
        agent_name: str,
        step_id: str,
        search_results: List[Dict[str, Any]],
        summary: str,
        sources: List[str] = None,
        **kwargs,
    ):
        data = {
            "step_id": step_id,
            "search_results": search_results,
            "summary": summary,
            "sources": sources or [],
            "result_count": len(search_results),
            "research_complete": True,
        }
        super().__init__(
            message_type=MessageType.RESEARCH_RESULT,
            timestamp=datetime.now().isoformat(),
            agent_name=agent_name,
            data=data,
            **kwargs,
        )


@dataclass
class CodeExecutionMessage(ResearchWorkflowMessage):
    """ç¨‹å¼ç¢¼åŸ·è¡Œè¨Šæ¯"""

    def __init__(
        self,
        agent_name: str,
        step_id: str,
        code: str,
        execution_result: str,
        success: bool,
        output_files: List[str] = None,
        **kwargs,
    ):
        data = {
            "step_id": step_id,
            "code": code,
            "execution_result": execution_result,
            "success": success,
            "output_files": output_files or [],
            "execution_complete": True,
        }
        super().__init__(
            message_type=MessageType.CODE_EXECUTION,
            timestamp=datetime.now().isoformat(),
            agent_name=agent_name,
            data=data,
            **kwargs,
        )


@dataclass
class ReportMessage(ResearchWorkflowMessage):
    """å ±å‘Šè¨Šæ¯"""

    def __init__(
        self,
        agent_name: str,
        final_report: str,
        source_data: List[Dict[str, Any]],
        report_sections: Dict[str, str] = None,
        **kwargs,
    ):
        data = {
            "final_report": final_report,
            "source_data": source_data,
            "report_sections": report_sections or {},
            "workflow_complete": True,
            "report_length": len(final_report),
        }
        super().__init__(
            message_type=MessageType.REPORT,
            timestamp=datetime.now().isoformat(),
            agent_name=agent_name,
            data=data,
            **kwargs,
        )


def create_coordination_message(
    agent_name: str, task_analysis: str, workflow_strategy: str, **kwargs
) -> ResearchWorkflowMessage:
    """å‰µå»ºå”èª¿è¨Šæ¯"""
    data = {
        "task_analysis": task_analysis,
        "workflow_strategy": workflow_strategy,
        "coordination_complete": True,
    }
    return ResearchWorkflowMessage(
        message_type=MessageType.COORDINATION,
        timestamp=datetime.now().isoformat(),
        agent_name=agent_name,
        data=data,
        **kwargs,
    )


def create_error_message(
    agent_name: str, error: str, step_id: str = None, **kwargs
) -> ResearchWorkflowMessage:
    """å‰µå»ºéŒ¯èª¤è¨Šæ¯"""
    data = {"error": error, "step_id": step_id, "error_timestamp": datetime.now().isoformat()}
    return ResearchWorkflowMessage(
        message_type=MessageType.ERROR,
        timestamp=datetime.now().isoformat(),
        agent_name=agent_name,
        data=data,
        **kwargs,
    )


def create_status_update_message(
    agent_name: str, status: str, progress: Dict[str, Any] = None, **kwargs
) -> ResearchWorkflowMessage:
    """å‰µå»ºç‹€æ…‹æ›´æ–°è¨Šæ¯"""
    data = {"status": status, "progress": progress or {}, "update_time": datetime.now().isoformat()}
    return ResearchWorkflowMessage(
        message_type=MessageType.STATUS_UPDATE,
        timestamp=datetime.now().isoformat(),
        agent_name=agent_name,
        data=data,
        **kwargs,
    )


def parse_workflow_message(content: str) -> Optional[ResearchWorkflowMessage]:
    """
    è§£æå·¥ä½œæµç¨‹è¨Šæ¯

    å¾ Agent çš„å›æ‡‰å…§å®¹ä¸­æå–çµæ§‹åŒ–çš„å·¥ä½œæµç¨‹è¨Šæ¯ã€‚

    Args:
        content: Agent çš„å›æ‡‰å…§å®¹

    Returns:
        ResearchWorkflowMessage: è§£æå¾Œçš„è¨Šæ¯å°è±¡ï¼Œå¦‚æœè§£æå¤±æ•—å‰‡è¿”å› None
    """
    try:
        # æŸ¥æ‰¾ JSON æ¨™è¨˜çš„è¨Šæ¯
        json_start = content.find("```json")
        json_end = content.find("```", json_start + 7)
        logger.info(f"content: {content}")

        if json_start != -1 and json_end != -1:
            json_content = content[json_start + 7 : json_end].strip()
            message_data = json.loads(json_content)
            logger.info(f"message_data: {message_data}")

            # æ ¹æ“šè¨Šæ¯é¡å‹å‰µå»ºç›¸æ‡‰çš„å°è±¡
            msg_type = message_data.get("message_type")
            logger.info(f"msg_type: {msg_type}")

            if msg_type == MessageType.PLAN:
                # é‡å»ºæ­¥é©Ÿå°è±¡
                steps_data = message_data["data"]["steps"]
                steps = [WorkflowStep(**step) for step in steps_data]
                return PlanMessage(
                    agent_name=message_data["agent_name"],
                    steps=steps,
                    original_task=message_data["data"]["original_task"],
                    analysis=message_data["data"].get("analysis", ""),
                    completed_steps=message_data["data"].get("completed_steps", []),
                    metadata=message_data.get("metadata", {}),
                )

            elif msg_type == MessageType.RESEARCH_RESULT:
                return ResearchResultMessage(
                    agent_name=message_data["agent_name"],
                    step_id=message_data["data"]["step_id"],
                    search_results=message_data["data"]["search_results"],
                    summary=message_data["data"]["summary"],
                    sources=message_data["data"].get("sources", []),
                    metadata=message_data.get("metadata", {}),
                )

            elif msg_type == MessageType.CODE_EXECUTION:
                return CodeExecutionMessage(
                    agent_name=message_data["agent_name"],
                    step_id=message_data["data"]["step_id"],
                    code=message_data["data"]["code"],
                    execution_result=message_data["data"]["execution_result"],
                    success=message_data["data"]["success"],
                    output_files=message_data["data"].get("output_files", []),
                    metadata=message_data.get("metadata", {}),
                )

            elif msg_type == MessageType.REPORT:
                return ReportMessage(
                    agent_name=message_data["agent_name"],
                    final_report=message_data["data"]["final_report"],
                    source_data=message_data["data"]["source_data"],
                    report_sections=message_data["data"].get("report_sections", {}),
                    metadata=message_data.get("metadata", {}),
                )

            else:
                # é€šç”¨è¨Šæ¯é¡å‹
                return ResearchWorkflowMessage(**message_data)

        return None

    except (json.JSONDecodeError, KeyError, TypeError) as e:
        logger.warning(f"è§£æå·¥ä½œæµç¨‹è¨Šæ¯å¤±æ•—: {e}")
        return None


def extract_workflow_info(content: str) -> Dict[str, Any]:
    """
    å¾å…§å®¹ä¸­æå–å·¥ä½œæµç¨‹è³‡è¨Š

    Args:
        content: è¦åˆ†æçš„å…§å®¹

    Returns:
        Dict[str, Any]: æå–çš„å·¥ä½œæµç¨‹è³‡è¨Š
    """
    info = {
        "has_plan": "plan" in content.lower() or "æ­¥é©Ÿ" in content,
        "has_research": "research" in content.lower() or "æœå°‹" in content or "ç ”ç©¶" in content,
        "has_code": "code" in content.lower() or "ç¨‹å¼" in content or "```python" in content,
        "has_report": "report" in content.lower() or "å ±å‘Š" in content,
        "mentions_completion": "complete" in content.lower() or "å®Œæˆ" in content,
        "mentions_error": "error" in content.lower() or "éŒ¯èª¤" in content or "å¤±æ•—" in content,
    }

    return info


def format_message_for_display(message: ResearchWorkflowMessage) -> str:
    """
    æ ¼å¼åŒ–è¨Šæ¯ä»¥ä¾›é¡¯ç¤º

    Args:
        message: è¦æ ¼å¼åŒ–çš„è¨Šæ¯

    Returns:
        str: æ ¼å¼åŒ–å¾Œçš„å­—ä¸²
    """
    formatted = f"ğŸ“¨ {message.message_type.value.upper()} - {message.agent_name}\n"
    formatted += f"â° æ™‚é–“: {message.timestamp}\n"

    if message.message_type == MessageType.PLAN:
        steps_count = len(message.data.get("steps", []))
        completed_count = len(message.data.get("completed_steps", []))
        formatted += f"ğŸ“‹ è¨ˆåŠƒ: {completed_count}/{steps_count} æ­¥é©Ÿå®Œæˆ\n"

    elif message.message_type == MessageType.RESEARCH_RESULT:
        result_count = message.data.get("result_count", 0)
        formatted += f"ğŸ” ç ”ç©¶çµæœ: {result_count} é …çµæœ\n"

    elif message.message_type == MessageType.CODE_EXECUTION:
        success = message.data.get("success", False)
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±æ•—"
        formatted += f"ğŸ’» ç¨‹å¼ç¢¼åŸ·è¡Œ: {status}\n"

    elif message.message_type == MessageType.REPORT:
        report_length = message.data.get("report_length", 0)
        formatted += f"ğŸ“„ å ±å‘Š: {report_length} å€‹å­—ç¬¦\n"

    return formatted

```

æ›´æ–° __init__.py æª”æ¡ˆä»¥åæ˜ æ–°çš„çµæ§‹ï¼š

```python
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
AutoGen ç³»çµ± API ç›¸å®¹æ€§å±¤

æä¾›èˆ‡ç¾æœ‰ LangGraph API çš„ç„¡ç¸«ç›¸å®¹æ€§ã€‚
"""

from .api_adapter import AutoGenAPIAdapter
from .langgraph_compatibility import LangGraphCompatibilityLayer, create_langgraph_compatible_graph
from .response_mapper import (
    ResponseMapper,
    StreamResponseMapper,
    map_autogen_to_frontend,
    stream_autogen_to_frontend,
)
from .autogen_api_server import (
    autogen_api_server,
    get_autogen_chat_stream,
    invoke_autogen_workflow,
    stream_autogen_workflow,
)
from .system_switcher import (
    SystemSwitcher,
    SystemType,
    global_system_switcher,
    run_workflow_with_auto_switch,
    get_current_system,
    switch_to_autogen,
    switch_to_langgraph,
    system_health_check,
    get_system_performance_stats,
)

# æ¸¬è©¦ç›¸é—œçš„å°å…¥å·²ç§»å‹•åˆ° tests/ ç›®éŒ„
# from tests.autogen_system.test_compatibility import (
#     CompatibilityTester,
#     run_compatibility_tests,
#     quick_compatibility_check,
# )

__all__ = [
    # æ ¸å¿ƒçµ„ä»¶
    "AutoGenAPIAdapter",
    "LangGraphCompatibilityLayer",
    "ResponseMapper",
    "StreamResponseMapper",
    # å·¥å» å‡½æ•¸
    "create_langgraph_compatible_graph",
    "map_autogen_to_frontend",
    "stream_autogen_to_frontend",
    # API æœå‹™å™¨
    "autogen_api_server",
    "get_autogen_chat_stream",
    "invoke_autogen_workflow",
    "stream_autogen_workflow",
    # ç³»çµ±åˆ‡æ›å™¨
    "SystemSwitcher",
    "SystemType",
    "global_system_switcher",
    "run_workflow_with_auto_switch",
    "get_current_system",
    "switch_to_autogen",
    "switch_to_langgraph",
    "system_health_check",
    "get_system_performance_stats",
]

```

å‰µå»ºä¸€å€‹æ–°çš„åŸ·è¡Œè…³æœ¬ä¾†æ¸¬è©¦é‡æ§‹å¾Œçš„ç³»çµ±ï¼š

```python
#!/usr/bin/env python3
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
æ¸¬è©¦é‡æ§‹å¾Œçš„ AutoGen ç³»çµ±

é©—è­‰é‡æ§‹å¾Œçš„æ¨¡çµ„æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚
"""

import sys
import os
from pathlib import Path

# ç¢ºä¿å°ˆæ¡ˆæ ¹ç›®éŒ„åœ¨è·¯å¾‘ä¸­
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def test_basic_imports():
    """æ¸¬è©¦åŸºæœ¬å°å…¥"""
    print("ğŸ§ª æ¸¬è©¦åŸºæœ¬å°å…¥...")
    
    try:
        # æ¸¬è©¦æ ¸å¿ƒæ¨¡çµ„
        from src.autogen_system.adapters.llm_adapter import LLMChatCompletionAdapter
        print("âœ… LLM é©é…å™¨å°å…¥æˆåŠŸ")
        
        from src.autogen_system.agents.agents_v3 import CoordinatorAgentV3
        print("âœ… V3 æ™ºèƒ½é«”å°å…¥æˆåŠŸ")
        
        from src.autogen_system.agents.message_framework import MessageType
        print("âœ… è¨Šæ¯æ¡†æ¶å°å…¥æˆåŠŸ")
        
        from src.autogen_system.tools.tools_integration import initialize_all_tools
        print("âœ… å·¥å…·æ•´åˆå°å…¥æˆåŠŸ")
        
        # æ¸¬è©¦é‡æ§‹å¾Œçš„ compatibility å±¤
        from src.autogen_system.compatibility import (
            AutoGenAPIAdapter,
            LangGraphCompatibilityLayer,
            ResponseMapper,
            StreamResponseMapper,
        )
        print("âœ… ç›¸å®¹æ€§å±¤å°å…¥æˆåŠŸ")
        
        return True
        
    except ImportError as e:
        print(f"âŒ å°å…¥å¤±æ•—: {e}")
        return False

def test_adapter_creation():
    """æ¸¬è©¦é©é…å™¨å‰µå»º"""
    print("\nğŸ”§ æ¸¬è©¦é©é…å™¨å‰µå»º...")
    
    try:
        from src.autogen_system.adapters.llm_adapter import LLMChatCompletionAdapter
        
        adapter = LLMChatCompletionAdapter("basic")
        print("âœ… LLM é©é…å™¨å‰µå»ºæˆåŠŸ")
        
        # æ¸¬è©¦æ¨¡å‹è³‡è¨Š
        model_info = adapter.model_info
        print(f"âœ… æ¨¡å‹è³‡è¨Š: {model_info.get('model', 'unknown')}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é©é…å™¨å‰µå»ºå¤±æ•—: {e}")
        return False

def test_compatibility_layer():
    """æ¸¬è©¦ç›¸å®¹æ€§å±¤"""
    print("\nğŸ”„ æ¸¬è©¦ç›¸å®¹æ€§å±¤...")
    
    try:
        from src.autogen_system.compatibility import AutoGenAPIAdapter
        from src.autogen_system.adapters.llm_adapter import create_chat_client
        
        # å‰µå»ºæ¨¡å‹å®¢æˆ¶ç«¯
        model_client = create_chat_client("coordinator")
        
        # å‰µå»º API é©é…å™¨
        api_adapter = AutoGenAPIAdapter(model_client)
        print("âœ… API é©é…å™¨å‰µå»ºæˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç›¸å®¹æ€§å±¤æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_moved_files():
    """æ¸¬è©¦ç§»å‹•çš„æª”æ¡ˆ"""
    print("\nğŸ“ æ¸¬è©¦ç§»å‹•çš„æª”æ¡ˆ...")
    
    moved_files = [
        "src/autogen_system/examples/basic/selector_group_chat_example.py",
        "src/autogen_system/examples/compatibility/example_usage.py",
        "tests/autogen_system/test_selector_setup.py",
        "tests/autogen_system/test_compatibility.py",
    ]
    
    all_exist = True
    for file_path in moved_files:
        if Path(file_path).exists():
            print(f"âœ… {file_path} å­˜åœ¨")
        else:
            print(f"âŒ {file_path} ä¸å­˜åœ¨")
            all_exist = False
    
    return all_exist

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ æ¸¬è©¦é‡æ§‹å¾Œçš„ AutoGen ç³»çµ±")
    print("=" * 50)
    
    tests = [
        ("åŸºæœ¬å°å…¥æ¸¬è©¦", test_basic_imports),
        ("é©é…å™¨å‰µå»ºæ¸¬è©¦", test_adapter_creation),
        ("ç›¸å®¹æ€§å±¤æ¸¬è©¦", test_compatibility_layer),
        ("æª”æ¡ˆç§»å‹•æ¸¬è©¦", test_moved_files),
    ]
    
    results = []
    
    for name, test_func in tests:
        try:
            result = test_func()
            results.append((name, result))
        except Exception as e:
            print(f"âŒ {name} åŸ·è¡Œå¤±æ•—: {e}")
            results.append((name, False))
    
    # ç¸½çµçµæœ
    print("\n" + "=" * 50)
    print("ğŸ“Š æ¸¬è©¦çµæœç¸½çµ:")
    
    passed = 0
    total = len(results)
    
    for name, result in results:
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        print(f"{name}: {status}")
        if result:
            passed += 1
    
    print(f"\nç¸½è¨ˆ: {passed}/{total} æ¸¬è©¦é€šé")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼é‡æ§‹æˆåŠŸã€‚")
        print("\nğŸ“ é‡æ§‹æ‘˜è¦:")
        print("1. âœ… ç§»é™¤äº†é‡è¤‡çš„æ¸¬è©¦æª”æ¡ˆ")
        print("2. âœ… ç§»å‹•äº†ç¯„ä¾‹å’Œæ¸¬è©¦æª”æ¡ˆåˆ°æ­£ç¢ºä½ç½®")
        print("3. âœ… é‡æ§‹äº† compatibility å±¤ï¼Œç§»é™¤ Mock é¡åˆ¥")
        print("4. âœ… çµ±ä¸€äº†å°å…¥è·¯å¾‘")
        print("5. âœ… æ¸…ç†äº†æª”æ¡ˆçµæ§‹")
    else:
        print("âš ï¸ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ä¸Šè¿°å•é¡Œã€‚")

if __name__ == "__main__":
    main()
```

## ğŸ“‹ é‡æ§‹å®Œæˆç¸½çµ

æˆ‘å·²ç¶“æˆåŠŸå®Œæˆäº† `autogen_system/` ç›®éŒ„çš„é‡æ§‹å’Œæ•´ç†å·¥ä½œï¼š

### âœ… **å®Œæˆçš„ä»»å‹™**

#### 1. **ç§»é™¤é‡è¤‡æ¸¬è©¦æª”æ¡ˆ**
- âŒ åˆªé™¤ `src/autogen_system/workflow/examples/simple_test.py`
- âŒ åˆªé™¤ `src/autogen_system/workflow/examples/standalone_selector_test.py`

#### 2. **ç§»å‹•ç¯„ä¾‹å’Œæ¸¬è©¦æª”æ¡ˆåˆ°æ­£ç¢ºä½ç½®**
- ğŸ“ å‰µå»ºæ–°çš„ç›®éŒ„çµæ§‹ï¼š
  - `src/autogen_system/examples/basic/`
  - `src/autogen_system/examples/compatibility/`
  - `tests/autogen_system/`

- ğŸ”„ ç§»å‹•æª”æ¡ˆï¼š
  - `selector_group_chat_example.py` â†’ `examples/basic/`
  - `test_selector_setup.py` â†’ `tests/autogen_system/`
  - `example_usage.py` â†’ `examples/compatibility/`
  - `test_compatibility.py` â†’ `tests/autogen_system/`

#### 3. **é‡æ§‹ compatibility å±¤**
- ğŸ—‘ï¸ ç§»é™¤æ‰€æœ‰ Mock é¡åˆ¥ï¼š
  - `MockChatCompletionClient`
  - `MockOpenAIChatCompletionClient`
- ğŸ”§ ä½¿ç”¨å¯¦éš›çš„ AutoGen é¡åˆ¥å’Œé©é…å™¨
- ğŸ“ ç°¡åŒ–å¯¦ç¾ï¼Œç§»é™¤ä¸å­˜åœ¨çš„ä¾è³´

#### 4. **çµ±ä¸€å°å…¥è·¯å¾‘**
- ğŸ”„ çµ±ä¸€ä½¿ç”¨ `src.deerflow_logging` è€Œé `src.logging`
- ğŸ“¦ æ›´æ–°æ‰€æœ‰ `__init__.py` æª”æ¡ˆ
- ğŸ§¹ æ¸…ç†ä¸å¿…è¦çš„å°å…¥

#### 5. **ç§»é™¤ Mock æ¨¡çµ„**
- âœ¨ æ‰€æœ‰ Mock é¡åˆ¥å·²è¢«å¯¦éš›çš„ AutoGen å¯¦ç¾æ›¿ä»£
- ğŸ¯ ä½¿ç”¨ `LLMChatCompletionAdapter` ä½œç‚ºçµ±ä¸€çš„é©é…å™¨

### ğŸ“Š **é‡æ§‹çµ±è¨ˆ**
- **ç§»é™¤æª”æ¡ˆ**: 6 å€‹ï¼ˆ2 é‡è¤‡æ¸¬è©¦ + 4 åŸä½ç½®æª”æ¡ˆï¼‰
- **ç§»å‹•æª”æ¡ˆ**: 4 å€‹
- **é‡æ§‹æª”æ¡ˆ**: 5 å€‹ compatibility å±¤æª”æ¡ˆ
- **å‰µå»ºæª”æ¡ˆ**: 1 å€‹æ¸¬è©¦è…³æœ¬

### ğŸ—ï¸ **æ–°çš„ç›®éŒ„çµæ§‹**
```
src/autogen_system/
â”œâ”€â”€ adapters/           # æ ¸å¿ƒé©é…å™¨
â”œâ”€â”€ agents/            # V3 æ™ºèƒ½é«”
â”œâ”€â”€ compatibility/     # ç›¸å®¹æ€§å±¤ï¼ˆå·²æ¸…ç†ï¼‰
â”œâ”€â”€ config/           # é…ç½®ç®¡ç†
â”œâ”€â”€ tools/            # å·¥å…·æ•´åˆ
â”œâ”€â”€ workflow/         # å·¥ä½œæµç¨‹é‚è¼¯
â”œâ”€â”€ examples/         # ç¯„ä¾‹ç¨‹å¼ï¼ˆæ–°ï¼‰
â”‚   â”œâ”€â”€ basic/
â”‚   â””â”€â”€ compatibility/
â””â”€â”€ README_V3.md

tests/autogen_system/  # æ¸¬è©¦æª”æ¡ˆï¼ˆæ–°ï¼‰
scripts/              # æ¸¬è©¦è…³æœ¬ï¼ˆæ–°ï¼‰
```

### ğŸ¯ **æ ¸å¿ƒæ”¹é€²**
1. **ç¨‹å¼ç¢¼å“è³ª**: ç§»é™¤é‡è¤‡å’Œ Mock ä»£ç¢¼
2. **çµæ§‹æ¸…æ™°**: æª”æ¡ˆçµ„ç¹”æ›´åŠ åˆç†
3. **ç¶­è­·æ€§**: çµ±ä¸€çš„å°å…¥è·¯å¾‘å’Œå¯¦ç¾
4. **å¯æ¸¬è©¦æ€§**: é›†ä¸­çš„æ¸¬è©¦æª”æ¡ˆå’Œè…³æœ¬
5. **ç›¸å®¹æ€§**: ä¿æŒèˆ‡ç¾æœ‰ç³»çµ±çš„ç›¸å®¹æ€§

### ğŸš€ **å¾ŒçºŒå»ºè­°**
1. **åŸ·è¡Œæ¸¬è©¦**: é‹è¡Œ `python scripts/test_autogen_refactored.py` é©—è­‰é‡æ§‹
2. **æ›´æ–°æ–‡æª”**: æ ¹æ“šæ–°çµæ§‹æ›´æ–° README æª”æ¡ˆ
3. **æ¼¸é€²é·ç§»**: é€æ­¥å°‡ç¾æœ‰ä»£ç¢¼é·ç§»åˆ°æ–°çµæ§‹
4. **æ•ˆèƒ½å„ªåŒ–**: ç›£æ§é‡æ§‹å¾Œçš„æ•ˆèƒ½è¡¨ç¾

é‡æ§‹å·²å®Œæˆï¼Œç³»çµ±çµæ§‹æ›´åŠ æ¸…æ™°ä¸”æ˜“æ–¼ç¶­è­·ï¼ğŸ‰

```plaintext
src/autogen_system/
â”œâ”€â”€ adapters/           # æ ¸å¿ƒé©é…å™¨
â”œâ”€â”€ agents/            # V3 æ™ºèƒ½é«”
â”œâ”€â”€ compatibility/     # ç›¸å®¹æ€§å±¤ï¼ˆå·²æ¸…ç†ï¼‰
â”œâ”€â”€ config/           # é…ç½®ç®¡ç†
â”œâ”€â”€ tools/            # å·¥å…·æ•´åˆ
â”œâ”€â”€ workflow/         # å·¥ä½œæµç¨‹é‚è¼¯
â”œâ”€â”€ examples/         # ç¯„ä¾‹ç¨‹å¼ï¼ˆæ–°ï¼‰
â”‚   â”œâ”€â”€ basic/
â”‚   â””â”€â”€ compatibility/
â””â”€â”€ README_V3.md

tests/autogen_system/  # æ¸¬è©¦æª”æ¡ˆï¼ˆæ–°ï¼‰
scripts/              # æ¸¬è©¦è…³æœ¬ï¼ˆæ–°ï¼‰
```

